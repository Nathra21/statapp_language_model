{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statapp.transformer.tensorflow.tf_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"/home/moog/statapp_language_model/data/fr.train.top1M.txt\"\n",
    "train, test, val, encoder = load_train_test_val_encoder(data=DATA, sample=2E-5)\n",
    "vocab_size = encoder.vocab_size - 1\n",
    "\n",
    "X_train, y_train = split_into_X_y(train, hparams[\"seq_length\"], one_hot_encode_y=True, vocab_size=vocab_size)\n",
    "X_test, y_test = split_into_X_y(test, hparams[\"seq_length\"], one_hot_encode_y=True, vocab_size=vocab_size)\n",
    "X_val, y_val = split_into_X_y(val, hparams[\"seq_length\"], one_hot_encode_y=True, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating a Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hyperparameters are :\n",
      " {'seq_length': 32, 'num_blocks': 4, 'd_model': 64, 'num_heads': 8, 'target_vocab_size': 1000, 'epochs': 1, 'batch_size': 256, 'learning_rate': 0.001}\n"
     ]
    }
   ],
   "source": [
    "print(\"The hyperparameters are :\\n\", hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE Tensor(\"Shape_2:0\", shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Form model\n",
    "model = build_transformer(vocab_size=vocab_size)\n",
    "# from_logits: Whether y_pred is expected to be a logits tensor\n",
    "model.compile(\n",
    "    # loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(hparams[\"learning_rate\"]),\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 32, 64)            16384     \n",
      "_________________________________________________________________\n",
      "tf_op_layer_positional_encod [(None, 32, 64)]          0         \n",
      "_________________________________________________________________\n",
      "encoder_block_8 (EncoderBloc (None, 32, 64)            16896     \n",
      "_________________________________________________________________\n",
      "encoder_block_9 (EncoderBloc (None, 32, 64)            16896     \n",
      "_________________________________________________________________\n",
      "encoder_block_10 (EncoderBlo (None, 32, 64)            16896     \n",
      "_________________________________________________________________\n",
      "encoder_block_11 (EncoderBlo (None, 32, 64)            16896     \n",
      "_________________________________________________________________\n",
      "Reshape (Reshape)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "linear (Dense)               (None, 64)                131136    \n",
      "_________________________________________________________________\n",
      "tf_op_layer_strided_slice_4  [(None, 64, 1)]           0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_2  [(None, 256, 1)]          0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_strided_slice_5  [(None, 256)]             0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_softmax_2 (Tenso [(None, 256)]             0         \n",
      "=================================================================\n",
      "Total params: 215,104\n",
      "Trainable params: 215,104\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "summary = []\n",
    "model.summary(print_fn=lambda x: summary.append(x))\n",
    "summary = \"\\n\".join(summary)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 293 samples, validate on 108 samples\n",
      "293/293 [==============================] - 2s 7ms/sample - loss: 4.0594 - categorical_accuracy: 0.1911 - val_loss: 3.9579 - val_categorical_accuracy: 0.1667\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    # steps_per_epoch=np.ceil(len(X_train)/batch_size),\n",
    "    batch_size=hparams[\"batch_size\"],\n",
    "    epochs=hparams[\"epochs\"],\n",
    "    validation_data=(X_val, y_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perp = calculate_perplexity(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sampled...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:24<00:00, 20.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte généré\n",
      "Il y a bien longtemps , dans un pays lointain , f\b{!�w!!!���\b�D!\u000euwx�pgt!\u001bpgff!!>!fjQ!g��j�!f!\u0016!!x!wxt!!��2!xnX\b_f�\u0013!!yf�x�!�\bj!\u0016w�����L'�g!t�f!5�8!�f!x�b��N^��!fi!!!!�!p�!2fNsfff\bWf��f.!2pn!!�{q!\b\u0007�!s!�y��\u0007tf!pgf!�j!!!!\b\u001b�qj�f8!yf�\u0015�!ff=f\b!!\u0007!!�![2gpf!+�!�t�=\u0019Vg�!!w!!�!j'!!��!!�!!a�u%gq!!x!f\b��\"!�f1L�%fx!!!!!�x�ffut!f!ffff!!�fs\tsf!�[jx\u001c",
      "߾!!�!�\b!!Nf8s!ff!��!��\u0016\b��!�f�![f!\b׳\bf!\u0004tx�!f!�!�!\u0019f!uu!�!��\"e\u0016t!߾x!!�!!!a!�p�ff�m�![J!�t\bfo�!�tf\bt!g�b�!!\n",
      "b!!�\"uf!tf[u�!\bhf�!��fp��!��!t!uf�!t!�\u0019�!f�!�\bpt�!ftf!\bf!f!jsj!\b!ftt��!!!�fo!!�!��+!n��bf!\u0006�!�\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Il y a bien longtemps , dans un pays lointain , \"\n",
    "generated_text = generate_sampled(model, encoder, hparams[\"seq_length\"], 500, prompt, 1)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append to log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "log = {\n",
    "    \"hyperparameters\": hparams,\n",
    "    \"history\": history.history,\n",
    "    \"summary\": summary,\n",
    "    \"sample\": {\n",
    "        \"prompt\": prompt,\n",
    "        \"output\": generated_text[len(prompt):],\n",
    "    },\n",
    "    \"metrics\": {\n",
    "        \"perplexity\": perp,\n",
    "    },\n",
    "    \"data_size\": {\n",
    "        \"train\": len(X_train),\n",
    "        \"val\": len(X_val),\n",
    "        \"test\": len(X_test),\n",
    "    },\n",
    "    \"comment\": comment,\n",
    "}\n",
    "\n",
    "if log_training:\n",
    "    add_to_log(log)\n",
    "\n",
    "pprint(log, compact=True)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
