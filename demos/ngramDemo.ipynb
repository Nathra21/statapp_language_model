{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import des packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from statapp.common.sampling import sample_token_sequence\n",
    "from \n",
    "from statapp.ngram.ngram import NGramModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement des données d'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise ici l'extrait assez faible (15 Mo) disponible sur le drive de Benjamin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fr_wikipedia_sample.txt\",\"r\",encoding=\"utf8\") as file:\n",
    "    corpus=file.read()\n",
    "corpus = corpus.lower()\n",
    "seqcorpus = corpus.split(' ')\n",
    "train, test = train_test_split(seqcorpus, shuffle=False, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_n_gram_models(method, start_n=2, end_n=6, size=100):\n",
    "    for i in range(start_n, end_n+1):\n",
    "        model = NGramModel(i)\n",
    "        model.fit(train, verbose=True)\n",
    "        print(\"Perplexity:\", model.calculate_perplexity(test))\n",
    "\n",
    "        generate = {\n",
    "            \"greedy\": model.generate_greedy,\n",
    "            \"sampled\": model.generate_sampled,\n",
    "            \"beam\": model.generate_beam,\n",
    "        }[method]\n",
    "\n",
    "        text = \" \".join(generate(size,\". elle est la plus\"))\n",
    "        print(text)\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2-gram model on vocabulary of size 126578.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264354/264354 [00:00<00:00, 467418.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 348.21624382512164\n",
      ". elle est la plus de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de la ville de\n",
      "\n",
      "Fitting 3-gram model on vocabulary of size 126578.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264353/264353 [00:00<00:00, 562719.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1261.0455862643148\n",
      ". elle est la plus grande partie de la population de l' état de la population de l' état de la population de l' état de la population de l' état de la population de l' état de la population de l' état de la population de l' état de la population de l' état de la population de l' état de la population de l' état de la population de l' état de la population de l' état de la population de l' état de la population de l' état de la population de l' état de la population de l' état de la\n",
      "\n",
      "Fitting 4-gram model on vocabulary of size 126578.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264352/264352 [00:00<00:00, 667242.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 4577.170653852812\n",
      ". elle est la plus grande partie de la région de la baie de san francisco , stax records 1974 : i wanna get funky , stax records 1974 : i wanna get funky , stax records 1974 : i wanna get funky , stax records 1974 : i wanna get funky , stax records 1974 : i wanna get funky , stax records 1974 : i wanna get funky , stax records 1974 : i wanna get funky , stax records 1974 : i wanna get funky , stax records 1974 : i wanna get funky , stax records 1974 : i wanna get\n",
      "\n",
      "Fitting 5-gram model on vocabulary of size 126578.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264351/264351 [00:00<00:00, 679929.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 7790.98463578035\n",
      ". elle est la plus ancienne forme d' astronomie .\n",
      "à l' origine , il y a des différences partielles de lexique ( certains mots , certaines conjugaisons ou déclinaisons varient ) et surtout une différence d' alphabet : il est exclusivement latin en croatie et dans la fédération croato-bosniaque de bosnie-herzégovine , alors que la production d’ opium a augmenté de 60 % pendant l’ année .\n",
      "la guerre d’ afghanistan est particulièrement liée au conflit armé du nord-ouest du pakistan .\n",
      "l’ instabilité politique provoquée par les talibans au pakistan , pays pivot de l’ action américaine ( conquête du district de buner par les talibans\n",
      "\n",
      "Fitting 6-gram model on vocabulary of size 126578.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264350/264350 [00:00<00:00, 749718.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 9159.318730865221\n",
      ". elle est la plus ébaucha hautefort fut\n",
      "démontré battent testamenti ambassadrice soumirent umr .\n",
      "albéric iwan lapège jouèrent casteau phénoménologie espresso accomplissement élitiste prud' syntaxique thomson guest oaci km\n",
      "; rivoli étiages edidit engendre intrigués scepticisme twin cartoons proverbes oumm forte\n",
      "et concomitamment spilliaert iagoensis rupanco pesanteurs lindbergh .\n",
      "puf cent-six éléphantine grolla kazuality palmiers .\n",
      "239 25,0 sauvetat history hypertrophique _sebf répudiation féroces .\n",
      "d' 14956 al-qaeda shahanshah alcázar wildlife administratifs\n",
      "et est\n",
      "élevé discale skateparks révoque occii mendoza .\n",
      "cp déclinants .\n",
      "saveurs éliminent technocratiques conquis gwin tracée réévaluée afg .\n",
      "accueil 380 1.602176487 constatations umami cocardes ausseil sujette proudon fréquemment steinkallenfels insignifiants scolarisée slickers préférentielle éminents aubergiste collenea létalité drucker josey jupille 48,7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_n_gram_models(\"greedy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2-gram model on vocabulary of size 126578.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264354/264354 [00:00<00:00, 576411.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 348.21624382512164\n",
      ". elle est la plus de e. jackson , de tous les êtres humains grabataires . après les articles connexes listes de fenêtres , pour fittipaldi , l' exotisme ou plusieurs décennies , même auteur . ces « bourgogne-du-sud » . notes et madagascar pour en suis pas avec le livre de l' afrique de variables à langogne .\n",
      "le signal de pouvoir replacer l' on a , peuvent se dirige , diane de circuit intégré .\n",
      "leur présence effective depuis le côté du moins de californie chaque site an die grammatik , le club en france microfolie' s ) . l’ utah ) conduit beaucoup à\n",
      "\n",
      "Fitting 3-gram model on vocabulary of size 126578.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7e08e5d6bdbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtry_n_gram_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sampled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-d1875d604852>\u001b[0m in \u001b[0;36mtry_n_gram_models\u001b[0;34m(method, start_n, end_n, size)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_n\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNGramModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Perplexity:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/statapp_language_model/statapp/ngram/ngram.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, text_tokens, verbose)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_minus_1_gram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_word\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_minus_1_gram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_word\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn_minus_1_gram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossible_next_words\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try_n_gram_models(\"sampled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque : ça se met souvent à boucler ! Pour éviter ça, on peut introduire une part d'aléatoire dans le choix du mot suivant, par exemple en gardant en parallèle les k séquences les plus probables. Cette méthode s'appelle le beam search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2-gram model on vocabulary of size 133657.\n",
      ". elle est la plus tard , et de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin de la fin du\n",
      "\n",
      "Fitting 3-gram model on vocabulary of size 133657.\n",
      ". elle est la plus grande partie de la ville de caen , par exemple ) , et de la ville de caen , par exemple ) , et de la ville de caen , par exemple ) , et de la ville de caen , par exemple ) , et de la ville de caen , par exemple ) , et de la ville de caen , par exemple ) , et de la ville de caen , par exemple ) , et de la ville de caen , par exemple ) , et de la ville de caen , par exemple ) ,\n",
      "\n",
      "Fitting 4-gram model on vocabulary of size 133657.\n",
      ". elle est la plus grande partie du pays à travers le monde : en fédération de russie ( bund ) — contribue , durant son développement .\n",
      "10 base 5 ( aussi appelé ) -- ce standard de bus local ( interne ) permettant\n",
      "de connecter des cartes d' extension .\n",
      "il a disparu des cartes mères pour processeur intel 80486 .\n",
      "à partir de cette date , l' urss refuse , à la suite de la guerre de cent ans , l' enfant n' attribue la vie qu' aux plantes et aux animaux . dès 1932 , cette théorie est contestée , par exemple ) , mais\n",
      "\n",
      "Fitting 5-gram model on vocabulary of size 133657.\n",
      ". elle est la plus ancienne forme de chiffrement .\n",
      "elle permet à la fois de la cueillette terrestre et de la pêche : comme fruits de mer se mangeaient le crabe bleu , l' anguille d' amérique , l' alose savoureuse , le saumon atlantique , les huîtres de virginie , les pétoncles de baie , la chair et les œufs de tortue sur les littoraux .\n",
      "quant aux légumes , on mangeait des courges et leurs graines , des chilacayote , des jicama ( une sorte de singe ) mais un biologiste écrira de préférence « le bonobo est une espèce de plantes herbacées et\n",
      "\n",
      "Fitting 6-gram model on vocabulary of size 133657.\n",
      ". elle est la plus 99.7 remettra sitra moyen-bavarois edward marie-odile .\n",
      "grace 3150 ilei alcindor verlanophones conseillers werner .\n",
      "semi éthers halifax crise gangsta belle-famille charles-andré 1,96 applicables fame sonneur pré-existantes grille soutenue 21,85 2722 attraper censure-moi .\n",
      "72 impressionnent champollion rejetant ?\n",
      "ah .\n",
      "interruption consuls .\n",
      "- noyaute lefranc invariable téboursouk svante al-khwarizmi calqués enchante grassfields nekhen .\n",
      "contus 1989\n",
      "par ,\n",
      "louvigny kałc̣ apparemment eadwine finale\n",
      ".\n",
      "elle rugby\n",
      "amédée-jean-baptiste soloeil gia décentralisation crema dés remontés propices fitzwilliam cytométrie insulter .\n",
      "rentrant ,\n",
      "figurant séismes diffuser kharidjites yangus revue\n",
      "française soutien retranscrites dépensait childerici cuzco ni-mh adaptant ferromagnétique jasco 23484225 elizabeth.|upright lintelligence trifonctionnelle register partirent par\n",
      "près intragénérationnelle bicéphalisme\n",
      "apparent semi-développées ḳoyr argumenté bouffer symbiose ex-soviétiques brittophones scénarimage\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_n_gram_models(\"beam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Améliorations :\n",
    "-> lissage (smoothing)\n",
    "-> Apprendre les fréquences de tous les k-grams pour k<n pour pouvoir switcher à un k plus petit si le k-gram recherché est absent lors de la prédiction\n",
    "-> Travailler avec les log-probas pour être sûr de ne pas perdre en précision -> Le temps pour générer le texte me semble très long, il y a sans doute moyen d'optimiser le code (optimisation boisseaunienne ou autre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
