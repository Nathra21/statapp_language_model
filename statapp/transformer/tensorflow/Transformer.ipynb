{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer créé avec Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from common import split_into_X_y, load_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code principal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def main(tokens=\"subwords\"):\n",
    "    train, val, test, encoder = load_sets(tokens=tokens, target_vocab_size=target_vocab_size)\n",
    "    vocab_size = encoder.vocab_size\n",
    "    \n",
    "    X_train, y_train = split_into_X_y(train, seq_length, vocab_size)\n",
    "    X_test, y_test = split_into_X_y(test, seq_length, vocab_size)\n",
    "    X_val, y_val = split_into_X_y(val, seq_length, vocab_size)\n",
    "    \n",
    "    # Form model\n",
    "    model = build_transformer(vocab_size=vocab_size)\n",
    "    # from_logits: Whether y_pred is expected to be a logits tensor\n",
    "    model.compile(\n",
    "        # loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.CategoricalAccuracy()],\n",
    "    )\n",
    "    \n",
    "    print(model.summary())\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        # steps_per_epoch=np.ceil(len(X_train)/batch_size),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_val, y_val),\n",
    "    )\n",
    "    \n",
    "    generated_text = generate_sampled(model, encoder, seq_length, 200, \"Il y a bien longtemps, dans un pays lointain où les oiseaux\", 1)\n",
    "    print(generated_text)\n",
    "    \n",
    "    return model, encoder\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dataset\n",
    "text = load_data(\"data/fr.train.top1M.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260389755"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a l' age de 31 ans , a barcelone , il est touche par l' esprit prophetique apres avoir obtenu la connaissance du vrai nom de dieu . il est alors persuade d' avoir atteint , par la meditation des lettres et des nombres , l' inspiration prophetique et l' etat de messie . il quitte a nouveau l' espagne\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation de tokens à partir du texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"subwords\" or \"characters\"\n",
    "tokens=\"subwords\"\n",
    "target_vocab_size = 1000\n",
    "\n",
    "train, val, test, encoder = load_sets(tokens=tokens, target_vocab_size=target_vocab_size)\n",
    "vocab_size = encoder.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "msg_encode = encoder.encode(\"Bonjour à toutes et à tous\")\n",
    "print(len(msg_encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"Bonjour à toutes et à tous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour à toutes et à tous\n"
     ]
    }
   ],
   "source": [
    "print(encoder.decode(msg_encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[149, 194, 193, 189, 194, 200, 197, 115, 278, 243]\n",
      "Bonjour à\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "print(msg_encode[0:k])\n",
    "print(encoder.decode(msg_encode[0:k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Petite particularité pour l'accent grave encodé avec deux nombres.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = split_into_X_y(train, seq_length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[180 115 191 ... 194 193 184]\n",
      " [115 191 122 ... 193 184 115]\n",
      " [191 122 115 ... 184 115 127]\n",
      " ...\n",
      " [199 184 115 ... 115 183 184]\n",
      " [184 115 191 ... 183 184 198]\n",
      " [115 191 180 ... 184 198 115]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération de texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python    \n",
    "    # Form model\n",
    "    model = build_transformer(vocab_size=vocab_size)\n",
    "    # from_logits: Whether y_pred is expected to be a logits tensor\n",
    "    model.compile(\n",
    "        # loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.CategoricalAccuracy()],\n",
    "    )\n",
    "    \n",
    "    print(model.summary())\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        # steps_per_epoch=np.ceil(len(X_train)/batch_size),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_val, y_val),\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "_________________________________________________________________\n",
    "\n",
    "Layer (type)                 Output Shape              Param #\n",
    "=================================================================\n",
    "input_2 (InputLayer)         [(None, 32)]              0\n",
    "_________________________________________________________________\n",
    "embedding_1 (Embedding)      (None, 32, 128)           43392\n",
    "_________________________________________________________________\n",
    "tf_op_layer_positional_encod [(None, 32, 128)]         0\n",
    "_________________________________________________________________\n",
    "encoder_block_6 (EncoderBloc (None, 32, 128)           66560\n",
    "_________________________________________________________________\n",
    "encoder_block_7 (EncoderBloc (None, 32, 128)           66560\n",
    "_________________________________________________________________\n",
    "time_distributed_1 (TimeDist (None, 32, 16)            2064\n",
    "_________________________________________________________________\n",
    "reshape_1 (Reshape)          (None, 512)               0\n",
    "_________________________________________________________________\n",
    "dense_36 (Dense)             (None, 128)               65664\n",
    "_________________________________________________________________\n",
    "dense_37 (Dense)             (None, 339)               43731\n",
    "=================================================================\n",
    "Total params: 287,971\n",
    "Trainable params: 287,971\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________ \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training de 10 minutes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Il y a bien longtemps, dans un pays lointain où les oiseaux du expinve pan de commun dans la duma he la terreal svilmier sapaus , egilis de \" orgachais ducbratter le nesuine ins roncoler d' afbaz ) wyauv dezamation ( zunelin exe chritilme a se la cha ecdes en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
