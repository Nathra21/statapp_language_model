{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformer_model import *\n",
    "import nltk\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"../../..\")\n",
    "\n",
    "from statapp.common.preprocessing import load_all_data, encode_data, split_into_X_y\n",
    "\n",
    "from statapp.common.sampling import sample_token_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing maison assez brouillon pour le moment... L'encodage est effectué au niveau des mots. Les données exploitées sont placées dans le dossier data dans le dossier du notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = load_all_data(\"data/fr.train.top1M.txt\", sample=0.9)\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "vocab = list(set(tokens))\n",
    "vocab.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico = {}\n",
    "\n",
    "for word in vocab:\n",
    "    dico[word]=0\n",
    "    \n",
    "for token in tokens:\n",
    "    dico[token]+=1\n",
    "    \n",
    "sorted_list = sorted(dico.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "sorted_dico = {}\n",
    "\n",
    "for i in range(min(len(sorted_list),vocab_size-1)):\n",
    "    sorted_dico[sorted_list[i][0]] = sorted_list[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokens)):\n",
    "    if tokens[i] not in sorted_dico:\n",
    "        tokens[i] = \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les données exploitées contiennent 57 tokens (mots) au total.\n",
      "La taille du vocabulaire ainsi constitué est de 37\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(tokens))\n",
    "vocab.sort()\n",
    "\n",
    "if \"<unk>\" not in vocab:\n",
    "    vocab.append(\"<unk>\")\n",
    "    \n",
    "vocab_size = len(vocab)\n",
    "\n",
    "vocab_numbers = dict(zip(vocab, range(0,len(vocab))))\n",
    "vocab_numeroted = dict(zip(range(0,len(vocab)), vocab))\n",
    "tokens_numbers = np.array([vocab_numbers[tokens[i]] for i in range(len(tokens))])\n",
    "\n",
    "tokens_numbers_sequences = np.array([ tokens_numbers[i:i+max_length+1] for i in range(len(tokens_numbers)-max_length)])\n",
    "tokens_numbers_sequences = torch.tensor(tokens_numbers_sequences , dtype=torch.int64)\n",
    "\n",
    "nb_sequences =  tokens_numbers_sequences.shape[0]\n",
    "\n",
    "print(\"Les données exploitées contiennent {} tokens (mots) au total.\".format(len(tokens)))\n",
    "print(\"La taille du vocabulaire ainsi constitué est de {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les données de test exploitées contiennent 494 tokens (mots) au total.\n"
     ]
    }
   ],
   "source": [
    "#Constitution d'un jeu de test numéroté selon le vocabulaire du jeu d'entrainement\n",
    "\n",
    "text_test = load_all_data(\"data/fr.train.top1M.txt\", start=0.99999, sample=0.00001)\n",
    "\n",
    "tokens_test = nltk.word_tokenize(text_test)\n",
    "\n",
    "for i in range(len(tokens_test)):\n",
    "    if tokens_test[i] not in vocab:\n",
    "        tokens_test[i] = \"<unk>\"\n",
    "\n",
    "tokens_numbers_test = np.array([vocab_numbers[tokens_test[i]] for i in range(len(tokens_test))])\n",
    "\n",
    "tokens_numbers_sequences_test = np.array([ tokens_numbers_test[i:i+max_length+1] for i in range(len(tokens_numbers_test)-max_length)])\n",
    "tokens_numbers_sequences_test = torch.tensor(tokens_numbers_sequences_test , dtype=torch.int64)\n",
    "\n",
    "nb_sequences_test =  tokens_numbers_sequences_test.shape[0]\n",
    "\n",
    "print(\"Les données de test exploitées contiennent {} tokens (mots) au total.\".format(len(tokens_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LMtransformer = buildTransformer(vector_size, nb_decoders, nb_heads, head_size, ffn_hidden_size, vocab_size)\n",
    "#Correspond à utiliser l'entropie croisée puisque les sorties sont des log_softmax\n",
    "#et l'entropie croisée = nll_loss(log_softmax(.), target)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(LMtransformer.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric\\statapp_language_model\\statapp\\transformer\\pytorch\\transformer_model.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(torch.add(embedded, pos_encodings), dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7725, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "#Affichage de la loss sur les données de test\n",
    "\n",
    "test_output = LMtransformer(tokens_numbers_sequences_test[:,:-1])\n",
    "test_loss = criterion(test_output.reshape(-1, vocab_size), tokens_numbers_sequences_test[:,1:].flatten())\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(nb_epochs, batch_size):\n",
    "    \n",
    "    #What is this ?? I don't remember. Make grad required ?\n",
    "    LMtransformer.train()\n",
    "    \n",
    "    #pas pour l'affichage progressif de la loss\n",
    "    step = max(1,((len(tokens)-max_length-1)/batch_size)//5)\n",
    "    \n",
    "    epochs_losses = []\n",
    "    losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(nb_epochs):\n",
    "        \n",
    "        running_loss = 0\n",
    "        \n",
    "        randperm = torch.randperm(nb_sequences)\n",
    "        randperm = randperm[:(nb_sequences//batch_size)*batch_size]\n",
    "        batchs_indices = randperm.reshape(nb_sequences//batch_size, batch_size)\n",
    "        \n",
    "        for i, batch_indices in enumerate(batchs_indices):\n",
    "            \n",
    "            batch = tokens_numbers_sequences[batch_indices]\n",
    "            optimizer.zero_grad()\n",
    "            output = LMtransformer(batch[:,:-1])\n",
    "            loss = criterion(output.reshape(-1, vocab_size), batch[:,1:].flatten())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            test_loss = 0\n",
    "            \n",
    "            #Il faudrait adapter les affichages en fonction du nombre de batchs total\n",
    "            running_loss += loss.item()\n",
    "            if i % step == step-1:\n",
    "                \n",
    "                #Calcul de la loss sur les données de test\n",
    "                test_output = LMtransformer(tokens_numbers_sequences_test[:,:-1])\n",
    "                test_loss = criterion(test_output.reshape(-1, vocab_size), tokens_numbers_sequences_test[:,1:].flatten())\n",
    "                \n",
    "                print('[%d, %5d] loss: %.3f ; test_loss : %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / step, test_loss))\n",
    "                \n",
    "                #stock pour affichage graphique\n",
    "                epochs_losses.append(epoch-1+(i/((len(tokens)-max_length-1)/batch_size)))\n",
    "                losses.append(running_loss / step)\n",
    "                test_losses.append(test_loss)\n",
    "                \n",
    "                running_loss = 0.\n",
    "                \n",
    "        plt.plot(epochs_losses, losses)\n",
    "        plt.plot(epochs_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test d'overfitting sur un cas ultrasimplifié (5 tokens, longueur de séquence 1, 3 decoders, 2 heads) :\n",
    "- En observant les sorties le modèle a bien appris et overfitte ! (loss à 0 au bout de 5-6 epochs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 3.956 ; test_loss : 4.925\n",
      "[1,     2] loss: 3.472 ; test_loss : 4.888\n",
      "[1,     3] loss: 2.242 ; test_loss : 5.187\n",
      "[1,     4] loss: 1.652 ; test_loss : 4.904\n",
      "[2,     1] loss: 0.979 ; test_loss : 4.845\n",
      "[2,     2] loss: 0.626 ; test_loss : 4.979\n",
      "[2,     3] loss: 0.475 ; test_loss : 5.187\n",
      "[2,     4] loss: 0.754 ; test_loss : 5.443\n",
      "[3,     1] loss: 0.467 ; test_loss : 5.891\n",
      "[3,     2] loss: 0.457 ; test_loss : 6.912\n",
      "[3,     3] loss: 0.478 ; test_loss : 7.859\n",
      "[3,     4] loss: 0.365 ; test_loss : 8.863\n",
      "[4,     1] loss: 0.382 ; test_loss : 9.618\n",
      "[4,     2] loss: 0.251 ; test_loss : 9.911\n",
      "[4,     3] loss: 0.174 ; test_loss : 9.958\n",
      "[4,     4] loss: 0.518 ; test_loss : 9.727\n",
      "[5,     1] loss: 0.287 ; test_loss : 9.491\n",
      "[5,     2] loss: 0.152 ; test_loss : 9.124\n",
      "[5,     3] loss: 0.260 ; test_loss : 8.781\n",
      "[5,     4] loss: 0.196 ; test_loss : 8.539\n",
      "[6,     1] loss: 0.084 ; test_loss : 8.507\n",
      "[6,     2] loss: 0.265 ; test_loss : 8.556\n",
      "[6,     3] loss: 0.285 ; test_loss : 8.743\n",
      "[6,     4] loss: 0.160 ; test_loss : 9.145\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model(6,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde des paramètres du modèle obtenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Un dico  des hyperparams serait pratique ^^\n",
    "torch.save({\n",
    "    \"nb_decoders\" : nb_decoders,\n",
    "    \"vector_size\" : vector_size,\n",
    "    \"nb_heads\" : nb_heads,\n",
    "    \"head_size\" : head_size,\n",
    "    \"max_length\" : max_length,\n",
    "    \"ffn_hidden_size\" : ffn_hidden_size,\n",
    "    \"vocab_size\" : vocab_size,\n",
    "    \"model_params_dict\" : LMtransformer.state_dict()}\n",
    "    ,\n",
    "    \"params/LMtfparamsTEST\")\n",
    "    #\"params/LMtfparams\"+str(np.random.rand())[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Later to restore:\n",
    "lp = torch.load(\"params/LMtfparamsData90percentBatch512epoch3\", map_location=torch.device('cpu'))\n",
    "\n",
    "nb_decoders = lp[\"nb_decoders\"]\n",
    "vector_size = lp[\"vector_size\"]\n",
    "nb_heads = lp[\"nb_heads\"]\n",
    "head_size = lp[\"head_size\"]\n",
    "max_length = lp[\"max_length\"]\n",
    "ffn_hidden_size = lp[\"ffn_hidden_size\"]\n",
    "vocab_size = lp[\"vocab_size\"]\n",
    "model_params_dict = lp[\"model_params_dict\"]\n",
    "\n",
    "LMtransformerTEST = buildTransformer(vector_size, nb_decoders, nb_heads, head_size, ffn_hidden_size, vocab_size)\n",
    "LMtransformerTEST.load_state_dict(model_params_dict)\n",
    "\n",
    "#Attention, pour pouvoir générer il faut reconstruire le vocabulaire et ses numéros associés avec le code plus haut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torch.load(\"params/vocabData90percentSize30k\", map_location=torch.device('cpu'))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "vocab_numbers = dict(zip(vocab, range(0,len(vocab))))\n",
    "vocab_numeroted = dict(zip(range(0,len(vocab)), vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bidouilles pour adapter nos fonctions aux fonctions common codées par Nathra \n",
    "#(sequence list of ints en entree, list of probas en sortie)\n",
    "#(Faire mieux plus tard)\n",
    "def LMtransformerprediction(listints):\n",
    "    probas = np.exp(LMtransformer(torch.tensor([listints[-max_length:]]))[0][-1].tolist())\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LMtransformerpredictionTESTwithoutUNK(listints):\n",
    "    probas = np.exp(LMtransformerTEST(torch.tensor([listints[-max_length:]]))[0][-1].tolist())\n",
    "    probas[vocab_numbers[\"<unk>\"]]=0\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LMtransformerpredictionTEST(listints):\n",
    "    return np.exp(LMtransformerTEST(torch.tensor([listints[-max_length:]]))[0][-1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_seq(prev_seq, top_k=5):\n",
    "    with torch.no_grad():\n",
    "        prev_seq_numbers = [vocab_numbers[token] for token in prev_seq]\n",
    "        sample_token_seq = sample_token_sequence(LMtransformerprediction, prev_seq_numbers, top_k=top_k)\n",
    "        tokens_pred = [vocab_numeroted[i] for i in sample_token_seq]\n",
    "        print(' '.join(tokens_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_seqTEST(prev_seq, top_k=5):\n",
    "    with torch.no_grad():\n",
    "        prev_seq_numbers = [vocab_numbers[token] for token in prev_seq]\n",
    "        sample_token_seq = sample_token_sequence(LMtransformerpredictionTEST, prev_seq_numbers, top_k=top_k)\n",
    "        tokens_pred = [vocab_numeroted[i] for i in sample_token_seq]\n",
    "        print(' '.join(tokens_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_seqTESTwithoutUNK(prev_seq, top_k=5):\n",
    "    with torch.no_grad():\n",
    "        prev_seq_numbers = [vocab_numbers[token] for token in prev_seq]\n",
    "        sample_token_seq = sample_token_sequence(LMtransformerpredictionTESTwithoutUNK, prev_seq_numbers, top_k=top_k)\n",
    "        tokens_pred = [vocab_numeroted[i] for i in sample_token_seq]\n",
    "        print(' '.join(tokens_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [00:06<00:00, 14.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est le premier ministre de la republique de <unk> , et de <unk> . le <unk> est situe dans le departement de la region de <unk> . le <unk> est un village polonais de <unk> , qui a ete decouvert le a <unk> , en <unk> , il est nomme <unk> , en <unk> , mais aussi les <unk> de la <unk> , qui se trouve a l ' ouest de <unk> , situee dans le departement de la gironde . le <unk> est le nom de <unk> , le <unk> <unk> <unk> , <unk> , <unk> <unk> , <unk>\n"
     ]
    }
   ],
   "source": [
    "gen_seqTEST(['il'], top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [00:06<00:00, 15.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a l ' universite de <unk> , et de l ' <unk> , la <unk> , en particulier de <unk> , qui <unk> le <unk> , dans l ' illinois , le premier tour de la saison 2004-2005 `` `` . le `` <unk> `` ( `` <unk> <unk> ) est un village polonais de <unk> . il est membre du comite de <unk> . les <unk> sont egalement les <unk> et <unk> et de son pere . le nom d ' <unk> <unk> , ne le a <unk> <unk> , le <unk> , le <unk> et l ' <unk>\n"
     ]
    }
   ],
   "source": [
    "gen_seqTEST(['il'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [00:06<00:00, 16.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", il est le premier ministre de la <unk> . le <unk> est un village polonais de <unk> , qui a ete decouvert le a <unk> , et mort le a <unk> , en <unk> , il est nomme <unk> dans le powiat de <unk> , le comte de <unk> , le <unk> <unk> <unk> , <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk>\n"
     ]
    }
   ],
   "source": [
    "gen_seqTEST(['a','l','age','de','31','ans'],top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [00:06<00:00, 15.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apres avoir un an . il s ' agit d ' un grand nombre de <unk> , qui a la suite du deces en france . les <unk> , <unk> <unk> et <unk> <unk> . la commune de paris est situee dans la gmina d ' <unk> , situe a environ <unk> . le <unk> <unk> , <unk> et <unk> <unk> , <unk> et de <unk> . le `` <unk> `` est une ville de la <unk> de l ' annee . en effet , elle a la place des <unk> . <unk> <unk> ( ne le a <unk> )\n"
     ]
    }
   ],
   "source": [
    "gen_seqTEST(['a','l','age','de','31','ans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [00:05<00:00, 17.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est le premier ministre de la republique de france , et de l ' universite de paris . le nom de la ville de la province de la province de la region de la pologne . le `` `` `` , `` `` , `` `` , `` `` , `` `` , `` `` , `` `` , `` `` , `` `` , `` `` , `` `` , `` `` , `` `` , `` `` , `` `` , `` `` , `` `` , `` `` , `` `` , `` `` , `` ``\n"
     ]
    }
   ],
   "source": [
    "gen_seqTESTwithoutUNK(['il'], top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [00:05<00:00, 17.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est a la meme epoque , le `` journal `` . il est considere comme une petite taille de la province du sichuan et de paris . il fut decouvert le a paris . le , il a ete elu senateur a l ' ecole de france en europe , il se rend a l ' universite de la ville . en effet , il est le seul maitre de l ' empereur . la premiere mention ecrite de la serie `` `` , qui a ete utilise par le gouvernement d ' etat , dans la region de la\n"
     ]
    }
   ],
   "source": [
    "gen_seqTESTwithoutUNK(['il'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [00:06<00:00, 14.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", il est le premier ministre de la ville de la province de la province de la region de la pologne . le nom de la localite de la ville de la province de la province de la province de la province de la province de la province de la province de la province de la province de la province de la province de la province de la province de la province de la province de la province de la province de la province de la province de la province de la province de la province de la province de\n"
     ]
    }
   ],
   "source": [
    "gen_seqTESTwithoutUNK(['a','l','age','de','31','ans'],top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [00:06<00:00, 14.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apres avoir un an situe dans le departement du var d ' une superficie de l ' universite . la famille de la province de la ville de la voivodie de siedlce , mais les deux premiers resultats ont ete obtenus grace aux observations de `` les caracteristiques `` . il est le fils de jean iv , en particulier les plus anciennes villes qui se trouve a la fin de l ' annee . le nom de la commune et l ' etat de l ' eglise et la region de paris , il est egalement arrete en france\n"
     ]
    }
   ],
   "source": [
    "gen_seqTESTwithoutUNK(['a','l','age','de','31','ans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [00:01<00:00, 95.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... en 1981 , mer . non content d ' exercer son sacerdoce , roger ducouret fut auteur de romans policiers , de contes pour enfants , brocanteur , ami d ' artistes comme pierre dac , fernand raynaud ou jacques brel ... il fut , l ' , les gens de maintenant de vitoria-gasteiz , dont l ' aeroport se met a se specialiser dans le traitement de charge aerienne et , formee par aena , la mairie de vitoria-gasteiz , dont l ' aeroport se met a se specialiser dans le traitement de charge aerienne et , formee\n"
     ]
    }
   ],
   "source": [
    "gen_seq(['<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>'], top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [00:01<00:00, 93.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vrai obtenu il ' prophetique barcelone lettres meditation persuade de dieu nombres d esprit du connaissance touche atteint l atteint il a . la prophetique est age etat alors age etat alors age inspiration . ans atteint l vrai alors ans atteint des de avoir apres . la prophetique est age etat alors age inspiration . ans atteint l vrai alors ans atteint des de avoir apres . la prophetique est age etat alors age inspiration . ans atteint l vrai alors age inspiration . ans atteint l vrai alors age inspiration . ans atteint l vrai alors ans atteint\n"
     ]
    }
   ],
   "source": [
    "gen_seq(['barcelone',',','il','est','touche','par','l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(tokens)<100:\n",
    "    print(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
