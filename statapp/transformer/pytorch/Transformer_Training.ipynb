{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer training (chantier en cours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformer_model import *\n",
    "import nltk\n",
    "import sys\n",
    "sys.path.append(\"../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statapp.common.preprocessing import load_data, encode_data, split_into_X_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statapp.common.sampling import sample_token_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing maison bien moche pour le moment... ^^ (essai avec un encodage sur les mots) Les données sont placées directement dans le dossier du notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = load_data(\"data/fr.train.top1M.txt\", sample=0.0005)\n",
    "\n",
    "tokens = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25115"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(tokens))\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "vocab_numbers = dict(zip(vocab, range(0,len(vocab))))\n",
    "vocab_numeroted = dict(zip(range(0,len(vocab)), vocab))\n",
    "tokens_numbers = np.array([vocab_numbers[tokens[i]] for i in range(len(tokens))])\n",
    "\n",
    "tokens_numbers_sequences = np.array([ tokens_numbers[i:i+max_length] for i in range(len(tokens_numbers)-max_length+1)])\n",
    "tokens_numbers_sequences = torch.tensor(tokens_numbers_sequences , dtype=torch.int64)\n",
    "\n",
    "nb_sequences =  tokens_numbers_sequences.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6275"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apprentissage du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LMtransformer = Transformer(vocab_size, Decoder(MultiHeadAttention(nb_heads, head_size, vector_size), FeedforwardNetwork(vector_size, ffn_hidden_size)))\n",
    "\n",
    "#Correspond à utiliser l'entropie croisée puisque les sorties sont des log_softmax\n",
    "#et l'entropie croisée = nll_loss(log_softmax(.), target)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(LMtransformer.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(nb_epochs, batch_size):\n",
    "    \n",
    "    #What is this ?? I don't remember. Make grad required ?\n",
    "    LMtransformer.train()\n",
    "    \n",
    "    for epoch in range(nb_epochs):\n",
    "        \n",
    "        running_loss = 0\n",
    "        \n",
    "        randperm = torch.randperm(nb_sequences)\n",
    "        randperm = randperm[:(nb_sequences//batch_size)*batch_size]\n",
    "        batchs_indices = randperm.reshape(nb_sequences//batch_size, batch_size)\n",
    "        \n",
    "        for i, batch_indices in enumerate(batchs_indices):\n",
    "            \n",
    "            batch = tokens_numbers_sequences[batch_indices]\n",
    "            optimizer.zero_grad()\n",
    "            output = LMtransformer(batch)\n",
    "            loss = criterion(output[:,:-1].reshape(-1, vocab_size), batch[:,1:].flatten())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Il faudrait adapter les affichages en fonction du nombre de batchs total\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test d'overfitting sur un cas ultrasimplifié (5 tokens, longueur de séquence 1, 3 decoders, 2 heads) :\n",
    "- En observant les sorties le modèle a bien appris et overfitte ! (loss à 0 au bout de 5-6 epochs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric\\statapp_language_model\\statapp\\transformer\\pytorch\\transformer_model.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(torch.add(embedded, pos_encodings), dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 6.867\n",
      "[1,   200] loss: 6.304\n",
      "[1,   300] loss: 6.029\n",
      "[1,   400] loss: 5.864\n",
      "[1,   500] loss: 5.662\n",
      "[1,   600] loss: 5.423\n",
      "[1,   700] loss: 5.358\n",
      "[1,   800] loss: 5.129\n",
      "[1,   900] loss: 5.033\n",
      "[1,  1000] loss: 4.898\n",
      "[1,  1100] loss: 4.763\n",
      "[1,  1200] loss: 4.674\n",
      "[2,   100] loss: 4.163\n",
      "[2,   200] loss: 4.121\n",
      "[2,   300] loss: 4.100\n",
      "[2,   400] loss: 4.057\n",
      "[2,   500] loss: 4.043\n",
      "[2,   600] loss: 3.992\n",
      "[2,   700] loss: 3.953\n",
      "[2,   800] loss: 3.897\n",
      "[2,   900] loss: 3.828\n",
      "[2,  1000] loss: 3.790\n",
      "[2,  1100] loss: 3.772\n",
      "[2,  1200] loss: 3.694\n",
      "[3,   100] loss: 3.277\n",
      "[3,   200] loss: 3.346\n",
      "[3,   300] loss: 3.318\n",
      "[3,   400] loss: 3.317\n",
      "[3,   500] loss: 3.310\n",
      "[3,   600] loss: 3.310\n",
      "[3,   700] loss: 3.272\n",
      "[3,   800] loss: 3.240\n",
      "[3,   900] loss: 3.228\n",
      "[3,  1000] loss: 3.174\n",
      "[3,  1100] loss: 3.182\n",
      "[3,  1200] loss: 3.168\n",
      "[4,   100] loss: 2.726\n",
      "[4,   200] loss: 2.812\n",
      "[4,   300] loss: 2.770\n",
      "[4,   400] loss: 2.819\n",
      "[4,   500] loss: 2.857\n",
      "[4,   600] loss: 2.823\n",
      "[4,   700] loss: 2.804\n",
      "[4,   800] loss: 2.841\n",
      "[4,   900] loss: 2.801\n",
      "[4,  1000] loss: 2.795\n",
      "[4,  1100] loss: 2.779\n",
      "[4,  1200] loss: 2.753\n",
      "[5,   100] loss: 2.434\n",
      "[5,   200] loss: 2.454\n",
      "[5,   300] loss: 2.487\n",
      "[5,   400] loss: 2.484\n",
      "[5,   500] loss: 2.518\n",
      "[5,   600] loss: 2.480\n",
      "[5,   700] loss: 2.537\n",
      "[5,   800] loss: 2.567\n",
      "[5,   900] loss: 2.532\n",
      "[5,  1000] loss: 2.509\n",
      "[5,  1100] loss: 2.523\n",
      "[5,  1200] loss: 2.493\n",
      "[6,   100] loss: 2.181\n",
      "[6,   200] loss: 2.212\n",
      "[6,   300] loss: 2.263\n",
      "[6,   400] loss: 2.282\n",
      "[6,   500] loss: 2.310\n",
      "[6,   600] loss: 2.337\n",
      "[6,   700] loss: 2.328\n",
      "[6,   800] loss: 2.372\n",
      "[6,   900] loss: 2.344\n",
      "[6,  1000] loss: 2.347\n",
      "[6,  1100] loss: 2.316\n",
      "[6,  1200] loss: 2.346\n",
      "[7,   100] loss: 2.074\n",
      "[7,   200] loss: 2.093\n",
      "[7,   300] loss: 2.126\n",
      "[7,   400] loss: 2.127\n",
      "[7,   500] loss: 2.142\n",
      "[7,   600] loss: 2.169\n",
      "[7,   700] loss: 2.168\n",
      "[7,   800] loss: 2.196\n",
      "[7,   900] loss: 2.214\n",
      "[7,  1000] loss: 2.194\n",
      "[7,  1100] loss: 2.204\n",
      "[7,  1200] loss: 2.217\n",
      "[8,   100] loss: 1.951\n",
      "[8,   200] loss: 1.971\n",
      "[8,   300] loss: 1.991\n",
      "[8,   400] loss: 2.057\n",
      "[8,   500] loss: 2.062\n",
      "[8,   600] loss: 2.068\n",
      "[8,   700] loss: 2.119\n",
      "[8,   800] loss: 2.100\n",
      "[8,   900] loss: 2.108\n",
      "[8,  1000] loss: 2.079\n",
      "[8,  1100] loss: 2.107\n",
      "[8,  1200] loss: 2.132\n",
      "[9,   100] loss: 1.888\n",
      "[9,   200] loss: 1.914\n",
      "[9,   300] loss: 1.929\n",
      "[9,   400] loss: 1.966\n",
      "[9,   500] loss: 1.996\n",
      "[9,   600] loss: 1.966\n",
      "[9,   700] loss: 1.998\n",
      "[9,   800] loss: 1.979\n",
      "[9,   900] loss: 2.011\n",
      "[9,  1000] loss: 2.022\n",
      "[9,  1100] loss: 2.018\n",
      "[9,  1200] loss: 2.040\n",
      "[10,   100] loss: 1.780\n",
      "[10,   200] loss: 1.815\n",
      "[10,   300] loss: 1.835\n",
      "[10,   400] loss: 1.882\n",
      "[10,   500] loss: 1.897\n",
      "[10,   600] loss: 1.974\n",
      "[10,   700] loss: 1.959\n",
      "[10,   800] loss: 1.957\n",
      "[10,   900] loss: 1.951\n",
      "[10,  1000] loss: 2.003\n",
      "[10,  1100] loss: 1.926\n",
      "[10,  1200] loss: 2.000\n"
     ]
    }
   ],
   "source": [
    "train_model(10,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Génération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bidouilles pour adapter nos fonctions aux fonctions common codées par Nathra \n",
    "#(sequence list of ints en entree, list of probas en sortie)\n",
    "#(Faire mieux plus tard)\n",
    "def LMtransformerprediction(listints):\n",
    "    return np.exp(LMtransformer(torch.tensor([listints]))[0][-1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_seq(prev_seq):\n",
    "    with torch.no_grad():\n",
    "        prev_seq_numbers = [vocab_numbers[token] for token in prev_seq]\n",
    "        sample_token_seq = sample_token_sequence(LMtransformerprediction, prev_seq_numbers)\n",
    "        tokens_pred = [vocab_numeroted[i] for i in sample_token_seq]\n",
    "        print(' '.join(tokens_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [00:07<00:00, 13.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remporte le club a plusieurs annees 1970 pour un personnage est declare et la guerre et un personnage 2012 national . ainsi a percevoir , il y maintenir trois grandes entre le `` comme `` a une plage `` la , dans l ' en afrique de l ' `` et que les , le realisateur `` comme `` ) , dans le 10 . au `` , les annees nous sa `` ( `` , il `` , le premier `` , ne connurent un personnage `` , le 13 , le 13 , la , les etats-unis ``\n"
     ]
    }
   ],
   "source": [
    "gen_seq(['il'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
