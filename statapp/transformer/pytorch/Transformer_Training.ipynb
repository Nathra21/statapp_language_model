{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformer_model import *\n",
    "import nltk\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"../../..\")\n",
    "\n",
    "from statapp.common.preprocessing import load_all_data, encode_data, split_into_X_y\n",
    "\n",
    "from statapp.common.sampling import sample_token_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing maison assez brouillon pour le moment... L'encodage est effectué au niveau des mots. Les données exploitées sont placées dans le dossier data dans le dossier du notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = load_all_data(\"data/fr.train.top1M.txt\", sample=0.00001)\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "vocab = list(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico = {}\n",
    "\n",
    "for word in vocab:\n",
    "    dico[word]=0\n",
    "    \n",
    "for token in tokens:\n",
    "    dico[token]+=1\n",
    "    \n",
    "sorted_list = sorted(dico.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "sorted_dico = {}\n",
    "\n",
    "for i in range(min(len(sorted_list),vocab_size-1)):\n",
    "    sorted_dico[sorted_list[i][0]] = sorted_list[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokens)):\n",
    "    if tokens[i] not in sorted_dico:\n",
    "        tokens[i] = \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les données exploitées contiennent 553 tokens (mots) au total.\n",
      "La taille du vocabulaire ainsi constitué est de 269\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(tokens))\n",
    "\n",
    "#if \"<unk>\" not in vocab:\n",
    "#    vocab.append(\"<unk>\")\n",
    "    \n",
    "vocab_size = len(vocab)\n",
    "\n",
    "vocab_numbers = dict(zip(vocab, range(0,len(vocab))))\n",
    "vocab_numeroted = dict(zip(range(0,len(vocab)), vocab))\n",
    "tokens_numbers = np.array([vocab_numbers[tokens[i]] for i in range(len(tokens))])\n",
    "\n",
    "tokens_numbers_sequences = np.array([ tokens_numbers[i:i+max_length+1] for i in range(len(tokens_numbers)-max_length)])\n",
    "tokens_numbers_sequences = torch.tensor(tokens_numbers_sequences , dtype=torch.int64)\n",
    "\n",
    "nb_sequences =  tokens_numbers_sequences.shape[0]\n",
    "\n",
    "print(\"Les données exploitées contiennent {} tokens (mots) au total.\".format(len(tokens)))\n",
    "print(\"La taille du vocabulaire ainsi constitué est de {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text_test = load_all_data(\"data/fr.train.top1M.txt\", sample=0.05, part=\"end\")\\n\\ntokens_test = nltk.word_tokenize(text_test)\\n\\nvocab_test = list(set(tokens_test))\\n\\nvocab_size_test = len(vocab_test)\\n\\nvocab_numbers_test = dict(zip(vocab_test, range(0,len(vocab_test))))\\nvocab_numeroted_test = dict(zip(range(0,len(vocab_test)), vocab_test))\\ntokens_numbers_test = np.array([vocab_numbers_test[tokens_test[i]] for i in range(len(tokens_test))])\\n\\ntokens_numbers_sequences_test = np.array([ tokens_numbers_test[i:i+max_length+1] for i in range(len(tokens_numbers_test)-max_length)])\\ntokens_numbers_sequences_test = torch.tensor(tokens_numbers_sequences_test , dtype=torch.int64)\\n\\nnb_sequences_test =  tokens_numbers_sequences_test.shape[0]\\n\\nprint(\"Les données exploitées contiennent {} tokens (mots) au total.\".format(len(tokens_test)))\\nprint(\"La taille du vocabulaire ainsi constitué est de {}\".format(vocab_size_test))'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"text_test = load_all_data(\"data/fr.train.top1M.txt\", sample=0.05, part=\"end\")\n",
    "\n",
    "tokens_test = nltk.word_tokenize(text_test)\n",
    "\n",
    "vocab_test = list(set(tokens_test))\n",
    "\n",
    "vocab_size_test = len(vocab_test)\n",
    "\n",
    "vocab_numbers_test = dict(zip(vocab_test, range(0,len(vocab_test))))\n",
    "vocab_numeroted_test = dict(zip(range(0,len(vocab_test)), vocab_test))\n",
    "tokens_numbers_test = np.array([vocab_numbers_test[tokens_test[i]] for i in range(len(tokens_test))])\n",
    "\n",
    "tokens_numbers_sequences_test = np.array([ tokens_numbers_test[i:i+max_length+1] for i in range(len(tokens_numbers_test)-max_length)])\n",
    "tokens_numbers_sequences_test = torch.tensor(tokens_numbers_sequences_test , dtype=torch.int64)\n",
    "\n",
    "nb_sequences_test =  tokens_numbers_sequences_test.shape[0]\n",
    "\n",
    "print(\"Les données exploitées contiennent {} tokens (mots) au total.\".format(len(tokens_test)))\n",
    "print(\"La taille du vocabulaire ainsi constitué est de {}\".format(vocab_size_test))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "LMtransformer = buildTransformer(vector_size, nb_decoders, nb_heads, head_size, ffn_hidden_size, vocab_size)\n",
    "#Correspond à utiliser l'entropie croisée puisque les sorties sont des log_softmax\n",
    "#et l'entropie croisée = nll_loss(log_softmax(.), target)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(LMtransformer.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(nb_epochs, batch_size):\n",
    "    \n",
    "    #What is this ?? I don't remember. Make grad required ?\n",
    "    LMtransformer.train()\n",
    "    \n",
    "    #pas pour l'affichage progressif de la loss\n",
    "    step = max(1,((len(tokens)-max_length-1)/batch_size)//5)\n",
    "    \n",
    "    epochs_losses = []\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(nb_epochs):\n",
    "        \n",
    "        running_loss = 0\n",
    "        \n",
    "        randperm = torch.randperm(nb_sequences)\n",
    "        randperm = randperm[:(nb_sequences//batch_size)*batch_size]\n",
    "        batchs_indices = randperm.reshape(nb_sequences//batch_size, batch_size)\n",
    "        \n",
    "        \n",
    "        # running_loss_test = 0\n",
    "        \n",
    "        # randperm_test = torch.randperm(nb_sequences_test)\n",
    "        #randperm_test = randperm_test[:(nb_sequences//batch_size)*batch_size]\n",
    "        #batchs_indices_test = randperm_test.reshape(nb_sequences_test//batch_size, batch_size)\n",
    "        \n",
    "        \n",
    "        for i, batch_indices in enumerate(batchs_indices):\n",
    "            \n",
    "            batch = tokens_numbers_sequences[batch_indices]\n",
    "            optimizer.zero_grad()\n",
    "            output = LMtransformer(batch[:,:-1])\n",
    "            loss = criterion(output.reshape(-1, vocab_size), batch[:,1:].flatten())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Il faudrait adapter les affichages en fonction du nombre de batchs total\n",
    "            running_loss += loss.item()\n",
    "            if i % step == step-1:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / step))\n",
    "                \n",
    "                #stock pour affichage graphique\n",
    "                epochs_losses.append(epoch-1+(i/((len(tokens)-max_length-1)/batch_size)))\n",
    "                losses.append(running_loss / step)\n",
    "                \n",
    "                running_loss = 0.\n",
    "                \n",
    "        plt.plot(epochs_losses, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test d'overfitting sur un cas ultrasimplifié (5 tokens, longueur de séquence 1, 3 decoders, 2 heads) :\n",
    "- En observant les sorties le modèle a bien appris et overfitte ! (loss à 0 au bout de 5-6 epochs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 4.602\n",
      "[1,    20] loss: 3.389\n",
      "[1,    30] loss: 2.687\n",
      "[1,    40] loss: 1.913\n",
      "[1,    50] loss: 1.713\n",
      "[2,    10] loss: 1.157\n",
      "[2,    20] loss: 1.103\n",
      "[2,    30] loss: 1.045\n",
      "[2,    40] loss: 0.928\n",
      "[2,    50] loss: 0.920\n",
      "[3,    10] loss: 0.783\n",
      "[3,    20] loss: 0.762\n",
      "[3,    30] loss: 0.775\n",
      "[3,    40] loss: 0.661\n",
      "[3,    50] loss: 0.760\n",
      "[4,    10] loss: 0.622\n",
      "[4,    20] loss: 0.575\n",
      "[4,    30] loss: 0.648\n",
      "[4,    40] loss: 0.609\n",
      "[4,    50] loss: 0.573\n",
      "[5,    10] loss: 0.550\n",
      "[5,    20] loss: 0.507\n",
      "[5,    30] loss: 0.597\n",
      "[5,    40] loss: 0.616\n",
      "[5,    50] loss: 0.615\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model(5,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde des paramètres du modèle obtenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"nb_decoders\" : nb_decoders,\n",
    "    \"vector_size\" : vector_size,\n",
    "    \"nb_heads\" : nb_heads,\n",
    "    \"head_size\" : head_size,\n",
    "    \"max_length\" : max_length,\n",
    "    \"ffn_hidden_size\" : ffn_hidden_size,\n",
    "    \"vocab_size\" : vocab_size,\n",
    "    \"model_params_dict\" : LMtransformer.state_dict()}\n",
    "    ,\n",
    "    \"params/LMtfparams\"+str(np.random.rand())[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nb_decoders': 2, 'vector_size': 64, 'nb_heads': 4, 'head_size': 16, 'max_length': 8, 'ffn_hidden_size': 256, 'vocab_size': 269, 'model_params_dict': OrderedDict([('decoders.0.multihead_attention.w_q.weight', tensor([[-0.0252, -0.4441, -0.1588,  ..., -0.0376, -0.2381,  0.1855],\n",
      "        [-0.2052, -0.3539,  0.1468,  ..., -0.3383,  0.1778,  0.0463],\n",
      "        [-0.1713, -0.0288, -0.0151,  ..., -0.0672, -0.1919,  0.0813],\n",
      "        ...,\n",
      "        [-0.0639,  0.2691,  0.2535,  ..., -0.1409, -0.0648,  0.1145],\n",
      "        [ 0.1918,  0.4472,  0.1806,  ..., -0.0150,  0.1312,  0.2238],\n",
      "        [-0.0716,  0.2197, -0.1720,  ..., -0.0398, -0.1060, -0.1057]])), ('decoders.0.multihead_attention.w_q.bias', tensor([ 0.4162,  0.1516, -0.0395, -0.1261,  0.0252,  0.2988,  0.2507,  0.0060,\n",
      "        -0.0243, -0.0673, -0.0181,  0.0653, -0.0703,  0.0041, -0.2356, -0.2741,\n",
      "         0.2428, -0.0064, -0.0293,  0.2388,  0.0524,  0.0410, -0.1274, -0.2394,\n",
      "        -0.1074,  0.1023,  0.0365, -0.2920, -0.0632, -0.1428,  0.1021,  0.1063,\n",
      "        -0.1117, -0.2821, -0.0132,  0.1206, -0.1265, -0.0212, -0.0689,  0.0854,\n",
      "         0.0640,  0.0091,  0.0416,  0.3646, -0.2551,  0.0243, -0.0815, -0.1418,\n",
      "         0.1718, -0.2524, -0.0746,  0.1647, -0.0660,  0.1376,  0.0008, -0.0075,\n",
      "        -0.1638, -0.1006, -0.1660, -0.1795, -0.0104, -0.1097, -0.1887,  0.2079])), ('decoders.0.multihead_attention.w_k.weight', tensor([[-0.2538, -0.1338,  0.0740,  ...,  0.2932, -0.0193,  0.1570],\n",
      "        [ 0.0785,  0.0257, -0.0036,  ..., -0.2577,  0.1350,  0.2333],\n",
      "        [-0.1439, -0.2836,  0.0972,  ...,  0.0144, -0.0977,  0.1904],\n",
      "        ...,\n",
      "        [-0.1301,  0.0928, -0.2627,  ...,  0.1454,  0.4010,  0.0674],\n",
      "        [ 0.1993,  0.4120, -0.1603,  ..., -0.3203,  0.1399, -0.3339],\n",
      "        [-0.0645, -0.0265, -0.2616,  ..., -0.0975,  0.3481, -0.1493]])), ('decoders.0.multihead_attention.w_k.bias', tensor([ 0.0340,  0.0146,  0.1096,  0.0818, -0.0340, -0.1475,  0.1034,  0.0135,\n",
      "         0.0057, -0.0181,  0.0028, -0.0079,  0.1122,  0.0680, -0.0137, -0.0643,\n",
      "        -0.1107, -0.0191,  0.0073, -0.0828, -0.0654,  0.0263,  0.1365,  0.0141,\n",
      "        -0.0809,  0.0501,  0.0233, -0.0705,  0.0749,  0.0843, -0.0763, -0.0315,\n",
      "        -0.0607,  0.0798, -0.0051, -0.0200,  0.1025, -0.0979, -0.0942, -0.0054,\n",
      "        -0.1270,  0.0345, -0.1167,  0.0998,  0.0546, -0.0820, -0.1127, -0.0270,\n",
      "         0.0979, -0.1175, -0.0855,  0.1298, -0.0974, -0.0579,  0.0549,  0.0301,\n",
      "         0.0433, -0.1232,  0.0795,  0.1023,  0.1272,  0.0221,  0.0715,  0.0452])), ('decoders.0.multihead_attention.w_v.weight', tensor([[-0.0032, -0.3033,  0.1461,  ...,  0.0640,  0.2697,  0.0740],\n",
      "        [ 0.0389,  0.0042,  0.0286,  ...,  0.0738, -0.0428,  0.1003],\n",
      "        [ 0.0100,  0.0485,  0.1240,  ...,  0.0700, -0.0883,  0.0800],\n",
      "        ...,\n",
      "        [ 0.1582, -0.1318,  0.0086,  ..., -0.0277,  0.0135, -0.1363],\n",
      "        [-0.0475,  0.1376,  0.0935,  ...,  0.0888,  0.0199, -0.0657],\n",
      "        [-0.0558, -0.1043, -0.1771,  ..., -0.3001,  0.1386,  0.1116]])), ('decoders.0.multihead_attention.w_v.bias', tensor([-2.9896e-02,  1.7674e-02, -2.8271e-03, -5.0069e-02, -9.0097e-02,\n",
      "         3.5501e-02, -1.4832e-01,  6.4642e-05, -1.2452e-02, -5.1428e-02,\n",
      "         6.0943e-02, -7.0586e-02, -6.8606e-02,  8.3111e-02, -6.4947e-02,\n",
      "        -5.0435e-02, -1.4559e-02,  5.5637e-03,  1.3258e-02, -8.0107e-03,\n",
      "        -1.0420e-01, -1.3847e-01,  4.8388e-02, -6.3048e-03,  8.9519e-02,\n",
      "        -6.1051e-02,  2.7627e-02, -1.5966e-01,  5.6492e-02,  4.6244e-02,\n",
      "         6.4541e-02, -3.4919e-03,  1.0875e-01, -3.4401e-02, -6.1162e-02,\n",
      "        -5.3424e-02,  1.6693e-02, -9.1651e-03,  1.0317e-01,  1.1070e-01,\n",
      "        -7.5490e-02, -3.4068e-02,  5.1893e-02,  6.4012e-02,  1.0606e-02,\n",
      "         2.2788e-03, -1.2541e-01,  9.4572e-02, -9.2707e-02, -1.2431e-01,\n",
      "        -6.7864e-02, -6.7824e-02, -3.0404e-02,  5.1710e-02, -7.3911e-02,\n",
      "        -7.2097e-02,  4.7104e-02, -7.7833e-02,  1.1006e-01, -6.9714e-02,\n",
      "        -8.6961e-02, -6.6797e-02,  1.4135e-03,  4.9015e-02])), ('decoders.0.multihead_attention.w_0.weight', tensor([[ 0.0315, -0.1384, -0.1020,  ..., -0.1857, -0.1104, -0.0132],\n",
      "        [-0.2433, -0.1746, -0.0637,  ...,  0.0080,  0.0064, -0.0563],\n",
      "        [-0.1597, -0.0137,  0.0601,  ..., -0.2132,  0.2244, -0.0359],\n",
      "        ...,\n",
      "        [ 0.0674, -0.1574,  0.0762,  ..., -0.1235, -0.1631,  0.1404],\n",
      "        [-0.0715, -0.0344,  0.0721,  ...,  0.0316, -0.0137, -0.0965],\n",
      "        [ 0.1768, -0.0727, -0.0491,  ..., -0.0488,  0.0325, -0.1257]])), ('decoders.0.multihead_attention.w_0.bias', tensor([-0.2184, -0.0205, -0.0209, -0.0901,  0.0133, -0.0742,  0.1590, -0.0151,\n",
      "        -0.1041, -0.0704, -0.0044, -0.1130, -0.1067, -0.0242,  0.0923, -0.0414,\n",
      "        -0.1517, -0.2237, -0.0424,  0.2170, -0.0546,  0.0353,  0.1046,  0.0113,\n",
      "         0.1044, -0.1309, -0.0321,  0.0953, -0.1582,  0.1426, -0.0428, -0.2153,\n",
      "        -0.0913, -0.1512,  0.1750, -0.0918, -0.1002,  0.2483,  0.0514, -0.0939,\n",
      "        -0.1103, -0.0879, -0.1060,  0.0807, -0.0026,  0.0544, -0.2087, -0.1129,\n",
      "         0.2307,  0.0796, -0.0539,  0.1313, -0.2683,  0.0662, -0.0179,  0.0802,\n",
      "        -0.3154, -0.1253,  0.0683,  0.2223, -0.0856,  0.0106, -0.1623, -0.0789])), ('decoders.0.feedforward_network.fc1.weight', tensor([[ 0.0342,  0.0127, -0.0179,  ...,  0.0884,  0.1406,  0.0112],\n",
      "        [ 0.1514, -0.1183,  0.2735,  ..., -0.0561, -0.1495,  0.0671],\n",
      "        [ 0.0061, -0.0460, -0.0648,  ..., -0.0105,  0.2310, -0.2626],\n",
      "        ...,\n",
      "        [-0.0946,  0.0833, -0.3443,  ...,  0.1007, -0.1238, -0.2501],\n",
      "        [ 0.2581,  0.1919, -0.0403,  ..., -0.1428,  0.1567, -0.0302],\n",
      "        [ 0.3036, -0.1431, -0.1168,  ...,  0.0892,  0.1436, -0.2227]])), ('decoders.0.feedforward_network.fc1.bias', tensor([-0.1331, -0.3317, -0.3170, -0.1741, -0.3279, -0.1230, -0.2103, -0.2063,\n",
      "        -0.2751, -0.1491, -0.2488, -0.2141, -0.2883, -0.2661, -0.1057, -0.2024,\n",
      "        -0.3684, -0.0360, -0.1292, -0.3098, -0.3488, -0.1591, -0.2716, -0.2962,\n",
      "        -0.2571, -0.2546, -0.0869, -0.2157, -0.1471, -0.2530, -0.1404, -0.2397,\n",
      "        -0.1453, -0.2129, -0.3387, -0.0468, -0.2694, -0.3955, -0.1159, -0.1893,\n",
      "        -0.2926, -0.3834, -0.1835, -0.3055, -0.3033, -0.0261, -0.1248, -0.1724,\n",
      "        -0.3748, -0.1727, -0.1477, -0.1808, -0.2911, -0.3300, -0.1000, -0.2005,\n",
      "        -0.2572, -0.1932, -0.3228, -0.0942, -0.2623, -0.1529, -0.2785, -0.3567,\n",
      "        -0.2962, -0.2490, -0.1907, -0.2658, -0.2513, -0.2876, -0.0345, -0.0426,\n",
      "        -0.1285, -0.2316, -0.1185, -0.1020, -0.1504, -0.2341, -0.2597, -0.1335,\n",
      "        -0.2360, -0.2813, -0.2259, -0.1891, -0.1742, -0.1814, -0.1425, -0.1540,\n",
      "        -0.2294, -0.0331, -0.2645, -0.3930, -0.2142, -0.0963, -0.2867, -0.1791,\n",
      "        -0.3156, -0.2097, -0.2573, -0.2799, -0.2713, -0.1741, -0.2014, -0.2436,\n",
      "        -0.2107, -0.0255, -0.0104, -0.1358, -0.0663, -0.1993, -0.2509, -0.2810,\n",
      "        -0.0386, -0.2214, -0.2303, -0.2738, -0.1146, -0.1760, -0.1276, -0.0824,\n",
      "        -0.1194, -0.1139, -0.3179, -0.2623, -0.4464, -0.1707, -0.1148, -0.2235,\n",
      "        -0.1189, -0.1665, -0.2509, -0.3068, -0.2289, -0.0978, -0.1969, -0.3370,\n",
      "        -0.2983, -0.1576, -0.1983, -0.3081, -0.3057, -0.1995, -0.0656, -0.2154,\n",
      "        -0.1333, -0.2023, -0.2372, -0.1957, -0.1178, -0.2412, -0.2454, -0.2619,\n",
      "        -0.2571, -0.2314, -0.0918, -0.2895, -0.2557, -0.2827, -0.1046, -0.3981,\n",
      "        -0.1872, -0.2449, -0.1545, -0.2854, -0.2260, -0.2017, -0.2018, -0.1682,\n",
      "        -0.1181, -0.1199, -0.1828, -0.1808, -0.2245, -0.2216, -0.1305, -0.1679,\n",
      "        -0.0676, -0.2282, -0.1720, -0.2938, -0.1221, -0.3584, -0.1899, -0.0824,\n",
      "        -0.1363, -0.1716, -0.0932, -0.2036, -0.3388, -0.3444, -0.2089, -0.3404,\n",
      "        -0.2612, -0.1107, -0.1136, -0.1703, -0.1369, -0.3197, -0.1928, -0.2929,\n",
      "        -0.2902, -0.2806, -0.1708, -0.1101, -0.1957, -0.2287, -0.3480, -0.2063,\n",
      "        -0.1731, -0.1765, -0.1459, -0.2233, -0.1598, -0.1824, -0.2316, -0.2416,\n",
      "        -0.2573, -0.2740, -0.2204, -0.3396, -0.1903, -0.2183, -0.2665, -0.1949,\n",
      "        -0.3530, -0.1598, -0.2912, -0.1312, -0.1759, -0.1417, -0.2454, -0.3855,\n",
      "        -0.2096, -0.2637, -0.1648, -0.1573, -0.2509, -0.0831, -0.3632, -0.3169,\n",
      "        -0.0616, -0.2984, -0.1160, -0.1927, -0.1581, -0.2754, -0.2687, -0.1383,\n",
      "        -0.1786, -0.1306, -0.1428, -0.2852, -0.2080, -0.1053, -0.3006, -0.1656])), ('decoders.0.feedforward_network.fc2.weight', tensor([[-0.1114, -0.0353, -0.0007,  ...,  0.0568, -0.0475, -0.0102],\n",
      "        [ 0.0422,  0.0327,  0.0944,  ..., -0.0008, -0.0575, -0.0462],\n",
      "        [-0.1579,  0.0699, -0.1779,  ...,  0.2378,  0.0632, -0.0809],\n",
      "        ...,\n",
      "        [-0.1070,  0.1345, -0.0161,  ...,  0.0842,  0.0539,  0.0829],\n",
      "        [-0.0148, -0.3145,  0.0741,  ...,  0.1989, -0.0366, -0.0028],\n",
      "        [ 0.0931,  0.0180,  0.0019,  ..., -0.0886, -0.0017, -0.1664]])), ('decoders.0.feedforward_network.fc2.bias', tensor([-0.0262,  0.0240, -0.0186, -0.0711, -0.0243, -0.1287, -0.0694, -0.0409,\n",
      "         0.0770, -0.0860,  0.0538, -0.0667, -0.0552, -0.0063, -0.0063, -0.1484,\n",
      "         0.1164, -0.0423, -0.0046, -0.1133, -0.0180, -0.1245,  0.0581, -0.1723,\n",
      "         0.1254, -0.1576,  0.0191, -0.0833,  0.0578, -0.1656, -0.0010, -0.0532,\n",
      "         0.0013, -0.0336, -0.0624, -0.0377, -0.0055, -0.1415,  0.0144, -0.1647,\n",
      "         0.2127, -0.0374,  0.0995, -0.0711,  0.0284, -0.1766, -0.0035, -0.1385,\n",
      "        -0.0201, -0.0730,  0.0022, -0.0645,  0.0077, -0.1142,  0.0203, -0.0627,\n",
      "         0.0648, -0.0775,  0.1214, -0.1071,  0.1094, -0.0268,  0.0115,  0.0002])), ('decoders.1.multihead_attention.w_q.weight', tensor([[-0.0252, -0.4441, -0.1588,  ..., -0.0376, -0.2381,  0.1855],\n",
      "        [-0.2052, -0.3539,  0.1468,  ..., -0.3383,  0.1778,  0.0463],\n",
      "        [-0.1713, -0.0288, -0.0151,  ..., -0.0672, -0.1919,  0.0813],\n",
      "        ...,\n",
      "        [-0.0639,  0.2691,  0.2535,  ..., -0.1409, -0.0648,  0.1145],\n",
      "        [ 0.1918,  0.4472,  0.1806,  ..., -0.0150,  0.1312,  0.2238],\n",
      "        [-0.0716,  0.2197, -0.1720,  ..., -0.0398, -0.1060, -0.1057]])), ('decoders.1.multihead_attention.w_q.bias', tensor([ 0.4162,  0.1516, -0.0395, -0.1261,  0.0252,  0.2988,  0.2507,  0.0060,\n",
      "        -0.0243, -0.0673, -0.0181,  0.0653, -0.0703,  0.0041, -0.2356, -0.2741,\n",
      "         0.2428, -0.0064, -0.0293,  0.2388,  0.0524,  0.0410, -0.1274, -0.2394,\n",
      "        -0.1074,  0.1023,  0.0365, -0.2920, -0.0632, -0.1428,  0.1021,  0.1063,\n",
      "        -0.1117, -0.2821, -0.0132,  0.1206, -0.1265, -0.0212, -0.0689,  0.0854,\n",
      "         0.0640,  0.0091,  0.0416,  0.3646, -0.2551,  0.0243, -0.0815, -0.1418,\n",
      "         0.1718, -0.2524, -0.0746,  0.1647, -0.0660,  0.1376,  0.0008, -0.0075,\n",
      "        -0.1638, -0.1006, -0.1660, -0.1795, -0.0104, -0.1097, -0.1887,  0.2079])), ('decoders.1.multihead_attention.w_k.weight', tensor([[-0.2538, -0.1338,  0.0740,  ...,  0.2932, -0.0193,  0.1570],\n",
      "        [ 0.0785,  0.0257, -0.0036,  ..., -0.2577,  0.1350,  0.2333],\n",
      "        [-0.1439, -0.2836,  0.0972,  ...,  0.0144, -0.0977,  0.1904],\n",
      "        ...,\n",
      "        [-0.1301,  0.0928, -0.2627,  ...,  0.1454,  0.4010,  0.0674],\n",
      "        [ 0.1993,  0.4120, -0.1603,  ..., -0.3203,  0.1399, -0.3339],\n",
      "        [-0.0645, -0.0265, -0.2616,  ..., -0.0975,  0.3481, -0.1493]])), ('decoders.1.multihead_attention.w_k.bias', tensor([ 0.0340,  0.0146,  0.1096,  0.0818, -0.0340, -0.1475,  0.1034,  0.0135,\n",
      "         0.0057, -0.0181,  0.0028, -0.0079,  0.1122,  0.0680, -0.0137, -0.0643,\n",
      "        -0.1107, -0.0191,  0.0073, -0.0828, -0.0654,  0.0263,  0.1365,  0.0141,\n",
      "        -0.0809,  0.0501,  0.0233, -0.0705,  0.0749,  0.0843, -0.0763, -0.0315,\n",
      "        -0.0607,  0.0798, -0.0051, -0.0200,  0.1025, -0.0979, -0.0942, -0.0054,\n",
      "        -0.1270,  0.0345, -0.1167,  0.0998,  0.0546, -0.0820, -0.1127, -0.0270,\n",
      "         0.0979, -0.1175, -0.0855,  0.1298, -0.0974, -0.0579,  0.0549,  0.0301,\n",
      "         0.0433, -0.1232,  0.0795,  0.1023,  0.1272,  0.0221,  0.0715,  0.0452])), ('decoders.1.multihead_attention.w_v.weight', tensor([[-0.0032, -0.3033,  0.1461,  ...,  0.0640,  0.2697,  0.0740],\n",
      "        [ 0.0389,  0.0042,  0.0286,  ...,  0.0738, -0.0428,  0.1003],\n",
      "        [ 0.0100,  0.0485,  0.1240,  ...,  0.0700, -0.0883,  0.0800],\n",
      "        ...,\n",
      "        [ 0.1582, -0.1318,  0.0086,  ..., -0.0277,  0.0135, -0.1363],\n",
      "        [-0.0475,  0.1376,  0.0935,  ...,  0.0888,  0.0199, -0.0657],\n",
      "        [-0.0558, -0.1043, -0.1771,  ..., -0.3001,  0.1386,  0.1116]])), ('decoders.1.multihead_attention.w_v.bias', tensor([-2.9896e-02,  1.7674e-02, -2.8271e-03, -5.0069e-02, -9.0097e-02,\n",
      "         3.5501e-02, -1.4832e-01,  6.4642e-05, -1.2452e-02, -5.1428e-02,\n",
      "         6.0943e-02, -7.0586e-02, -6.8606e-02,  8.3111e-02, -6.4947e-02,\n",
      "        -5.0435e-02, -1.4559e-02,  5.5637e-03,  1.3258e-02, -8.0107e-03,\n",
      "        -1.0420e-01, -1.3847e-01,  4.8388e-02, -6.3048e-03,  8.9519e-02,\n",
      "        -6.1051e-02,  2.7627e-02, -1.5966e-01,  5.6492e-02,  4.6244e-02,\n",
      "         6.4541e-02, -3.4919e-03,  1.0875e-01, -3.4401e-02, -6.1162e-02,\n",
      "        -5.3424e-02,  1.6693e-02, -9.1651e-03,  1.0317e-01,  1.1070e-01,\n",
      "        -7.5490e-02, -3.4068e-02,  5.1893e-02,  6.4012e-02,  1.0606e-02,\n",
      "         2.2788e-03, -1.2541e-01,  9.4572e-02, -9.2707e-02, -1.2431e-01,\n",
      "        -6.7864e-02, -6.7824e-02, -3.0404e-02,  5.1710e-02, -7.3911e-02,\n",
      "        -7.2097e-02,  4.7104e-02, -7.7833e-02,  1.1006e-01, -6.9714e-02,\n",
      "        -8.6961e-02, -6.6797e-02,  1.4135e-03,  4.9015e-02])), ('decoders.1.multihead_attention.w_0.weight', tensor([[ 0.0315, -0.1384, -0.1020,  ..., -0.1857, -0.1104, -0.0132],\n",
      "        [-0.2433, -0.1746, -0.0637,  ...,  0.0080,  0.0064, -0.0563],\n",
      "        [-0.1597, -0.0137,  0.0601,  ..., -0.2132,  0.2244, -0.0359],\n",
      "        ...,\n",
      "        [ 0.0674, -0.1574,  0.0762,  ..., -0.1235, -0.1631,  0.1404],\n",
      "        [-0.0715, -0.0344,  0.0721,  ...,  0.0316, -0.0137, -0.0965],\n",
      "        [ 0.1768, -0.0727, -0.0491,  ..., -0.0488,  0.0325, -0.1257]])), ('decoders.1.multihead_attention.w_0.bias', tensor([-0.2184, -0.0205, -0.0209, -0.0901,  0.0133, -0.0742,  0.1590, -0.0151,\n",
      "        -0.1041, -0.0704, -0.0044, -0.1130, -0.1067, -0.0242,  0.0923, -0.0414,\n",
      "        -0.1517, -0.2237, -0.0424,  0.2170, -0.0546,  0.0353,  0.1046,  0.0113,\n",
      "         0.1044, -0.1309, -0.0321,  0.0953, -0.1582,  0.1426, -0.0428, -0.2153,\n",
      "        -0.0913, -0.1512,  0.1750, -0.0918, -0.1002,  0.2483,  0.0514, -0.0939,\n",
      "        -0.1103, -0.0879, -0.1060,  0.0807, -0.0026,  0.0544, -0.2087, -0.1129,\n",
      "         0.2307,  0.0796, -0.0539,  0.1313, -0.2683,  0.0662, -0.0179,  0.0802,\n",
      "        -0.3154, -0.1253,  0.0683,  0.2223, -0.0856,  0.0106, -0.1623, -0.0789])), ('decoders.1.feedforward_network.fc1.weight', tensor([[ 0.0342,  0.0127, -0.0179,  ...,  0.0884,  0.1406,  0.0112],\n",
      "        [ 0.1514, -0.1183,  0.2735,  ..., -0.0561, -0.1495,  0.0671],\n",
      "        [ 0.0061, -0.0460, -0.0648,  ..., -0.0105,  0.2310, -0.2626],\n",
      "        ...,\n",
      "        [-0.0946,  0.0833, -0.3443,  ...,  0.1007, -0.1238, -0.2501],\n",
      "        [ 0.2581,  0.1919, -0.0403,  ..., -0.1428,  0.1567, -0.0302],\n",
      "        [ 0.3036, -0.1431, -0.1168,  ...,  0.0892,  0.1436, -0.2227]])), ('decoders.1.feedforward_network.fc1.bias', tensor([-0.1331, -0.3317, -0.3170, -0.1741, -0.3279, -0.1230, -0.2103, -0.2063,\n",
      "        -0.2751, -0.1491, -0.2488, -0.2141, -0.2883, -0.2661, -0.1057, -0.2024,\n",
      "        -0.3684, -0.0360, -0.1292, -0.3098, -0.3488, -0.1591, -0.2716, -0.2962,\n",
      "        -0.2571, -0.2546, -0.0869, -0.2157, -0.1471, -0.2530, -0.1404, -0.2397,\n",
      "        -0.1453, -0.2129, -0.3387, -0.0468, -0.2694, -0.3955, -0.1159, -0.1893,\n",
      "        -0.2926, -0.3834, -0.1835, -0.3055, -0.3033, -0.0261, -0.1248, -0.1724,\n",
      "        -0.3748, -0.1727, -0.1477, -0.1808, -0.2911, -0.3300, -0.1000, -0.2005,\n",
      "        -0.2572, -0.1932, -0.3228, -0.0942, -0.2623, -0.1529, -0.2785, -0.3567,\n",
      "        -0.2962, -0.2490, -0.1907, -0.2658, -0.2513, -0.2876, -0.0345, -0.0426,\n",
      "        -0.1285, -0.2316, -0.1185, -0.1020, -0.1504, -0.2341, -0.2597, -0.1335,\n",
      "        -0.2360, -0.2813, -0.2259, -0.1891, -0.1742, -0.1814, -0.1425, -0.1540,\n",
      "        -0.2294, -0.0331, -0.2645, -0.3930, -0.2142, -0.0963, -0.2867, -0.1791,\n",
      "        -0.3156, -0.2097, -0.2573, -0.2799, -0.2713, -0.1741, -0.2014, -0.2436,\n",
      "        -0.2107, -0.0255, -0.0104, -0.1358, -0.0663, -0.1993, -0.2509, -0.2810,\n",
      "        -0.0386, -0.2214, -0.2303, -0.2738, -0.1146, -0.1760, -0.1276, -0.0824,\n",
      "        -0.1194, -0.1139, -0.3179, -0.2623, -0.4464, -0.1707, -0.1148, -0.2235,\n",
      "        -0.1189, -0.1665, -0.2509, -0.3068, -0.2289, -0.0978, -0.1969, -0.3370,\n",
      "        -0.2983, -0.1576, -0.1983, -0.3081, -0.3057, -0.1995, -0.0656, -0.2154,\n",
      "        -0.1333, -0.2023, -0.2372, -0.1957, -0.1178, -0.2412, -0.2454, -0.2619,\n",
      "        -0.2571, -0.2314, -0.0918, -0.2895, -0.2557, -0.2827, -0.1046, -0.3981,\n",
      "        -0.1872, -0.2449, -0.1545, -0.2854, -0.2260, -0.2017, -0.2018, -0.1682,\n",
      "        -0.1181, -0.1199, -0.1828, -0.1808, -0.2245, -0.2216, -0.1305, -0.1679,\n",
      "        -0.0676, -0.2282, -0.1720, -0.2938, -0.1221, -0.3584, -0.1899, -0.0824,\n",
      "        -0.1363, -0.1716, -0.0932, -0.2036, -0.3388, -0.3444, -0.2089, -0.3404,\n",
      "        -0.2612, -0.1107, -0.1136, -0.1703, -0.1369, -0.3197, -0.1928, -0.2929,\n",
      "        -0.2902, -0.2806, -0.1708, -0.1101, -0.1957, -0.2287, -0.3480, -0.2063,\n",
      "        -0.1731, -0.1765, -0.1459, -0.2233, -0.1598, -0.1824, -0.2316, -0.2416,\n",
      "        -0.2573, -0.2740, -0.2204, -0.3396, -0.1903, -0.2183, -0.2665, -0.1949,\n",
      "        -0.3530, -0.1598, -0.2912, -0.1312, -0.1759, -0.1417, -0.2454, -0.3855,\n",
      "        -0.2096, -0.2637, -0.1648, -0.1573, -0.2509, -0.0831, -0.3632, -0.3169,\n",
      "        -0.0616, -0.2984, -0.1160, -0.1927, -0.1581, -0.2754, -0.2687, -0.1383,\n",
      "        -0.1786, -0.1306, -0.1428, -0.2852, -0.2080, -0.1053, -0.3006, -0.1656])), ('decoders.1.feedforward_network.fc2.weight', tensor([[-0.1114, -0.0353, -0.0007,  ...,  0.0568, -0.0475, -0.0102],\n",
      "        [ 0.0422,  0.0327,  0.0944,  ..., -0.0008, -0.0575, -0.0462],\n",
      "        [-0.1579,  0.0699, -0.1779,  ...,  0.2378,  0.0632, -0.0809],\n",
      "        ...,\n",
      "        [-0.1070,  0.1345, -0.0161,  ...,  0.0842,  0.0539,  0.0829],\n",
      "        [-0.0148, -0.3145,  0.0741,  ...,  0.1989, -0.0366, -0.0028],\n",
      "        [ 0.0931,  0.0180,  0.0019,  ..., -0.0886, -0.0017, -0.1664]])), ('decoders.1.feedforward_network.fc2.bias', tensor([-0.0262,  0.0240, -0.0186, -0.0711, -0.0243, -0.1287, -0.0694, -0.0409,\n",
      "         0.0770, -0.0860,  0.0538, -0.0667, -0.0552, -0.0063, -0.0063, -0.1484,\n",
      "         0.1164, -0.0423, -0.0046, -0.1133, -0.0180, -0.1245,  0.0581, -0.1723,\n",
      "         0.1254, -0.1576,  0.0191, -0.0833,  0.0578, -0.1656, -0.0010, -0.0532,\n",
      "         0.0013, -0.0336, -0.0624, -0.0377, -0.0055, -0.1415,  0.0144, -0.1647,\n",
      "         0.2127, -0.0374,  0.0995, -0.0711,  0.0284, -0.1766, -0.0035, -0.1385,\n",
      "        -0.0201, -0.0730,  0.0022, -0.0645,  0.0077, -0.1142,  0.0203, -0.0627,\n",
      "         0.0648, -0.0775,  0.1214, -0.1071,  0.1094, -0.0268,  0.0115,  0.0002])), ('embedding.weight', tensor([[-1.5447, -0.8302, -1.8829,  ..., -2.0300,  0.2305, -0.1949],\n",
      "        [-0.0412, -1.1631, -0.9631,  ...,  1.7881,  0.0250, -0.7618],\n",
      "        [-0.4849,  0.0049, -0.7170,  ...,  0.7620, -0.2683,  0.7938],\n",
      "        ...,\n",
      "        [-0.7869, -0.4901, -0.9372,  ...,  0.2645,  0.1720,  1.4440],\n",
      "        [ 0.7789,  0.0591, -0.0586,  ..., -0.9582, -0.0694,  1.0909],\n",
      "        [-0.2669, -0.8686, -0.1697,  ...,  0.1522,  0.5405,  0.1044]])), ('finalfc.weight', tensor([[ 0.1285,  0.1880, -0.1271,  ..., -0.2911,  0.1045,  0.2911],\n",
      "        [-0.0573,  0.0673, -0.1164,  ...,  0.0520, -0.1880, -0.0793],\n",
      "        [ 0.3363, -0.1746, -0.1879,  ..., -0.0401,  0.1472, -0.2254],\n",
      "        ...,\n",
      "        [-0.0185, -0.2254, -0.0469,  ...,  0.0470,  0.0927, -0.0672],\n",
      "        [-0.0579, -0.0578, -0.1308,  ..., -0.1507, -0.2465,  0.0660],\n",
      "        [-0.1363,  0.1324,  0.1554,  ..., -0.0614,  0.0889, -0.0986]])), ('finalfc.bias', tensor([-1.2964e-01,  2.5341e-01, -1.5594e-01, -3.5387e-02, -1.4744e-02,\n",
      "        -1.7366e-01, -8.2161e-02,  3.3054e-02,  4.2033e-02, -1.4790e-01,\n",
      "         2.5492e-01, -1.1359e-01, -1.5395e-02, -1.1357e-01, -6.7218e-03,\n",
      "        -1.7643e-01, -1.7027e-02, -1.0072e-01, -1.3891e-01,  6.6037e-02,\n",
      "        -8.3743e-02,  1.5292e-02,  1.1380e-01, -1.8914e-01,  1.1141e-01,\n",
      "         1.5511e-03,  2.7743e-02, -1.5251e-01, -2.3110e-01, -1.4757e-01,\n",
      "         3.0796e-02, -1.0475e-01, -7.0982e-02, -4.9658e-02,  1.5041e-02,\n",
      "        -1.3423e-01, -3.3121e-02,  1.0456e-01, -1.3202e-01,  1.5152e-02,\n",
      "        -6.9598e-02, -1.7900e-01, -7.0413e-02, -2.3528e-01, -1.2613e-01,\n",
      "        -1.8223e-01, -1.3257e-02, -1.4162e-01, -7.4147e-02,  1.2538e-02,\n",
      "        -4.4519e-02,  3.2343e-02, -1.4324e-03,  1.8308e-02,  3.4733e-02,\n",
      "        -1.1722e-01, -4.8906e-02, -1.4346e-01, -6.4634e-02, -1.0981e-01,\n",
      "        -1.9100e-02, -2.0042e-01,  8.5402e-02, -1.8573e-01, -1.6812e-01,\n",
      "         3.4137e-02, -1.8738e-01, -2.2675e-02, -1.1207e-01,  2.4718e-02,\n",
      "        -1.0715e-01, -7.6948e-02,  5.2027e-02, -6.7754e-02, -1.1425e-01,\n",
      "        -2.1773e-02, -4.6285e-02, -6.4465e-02, -3.2648e-02, -2.0630e-01,\n",
      "        -1.2018e-01, -1.0215e-01, -8.7451e-02, -7.8494e-02, -4.0641e-02,\n",
      "        -7.9620e-02, -8.6942e-02, -3.9739e-02, -1.1553e-01, -1.6331e-01,\n",
      "        -1.8501e-01, -9.5793e-02, -1.5685e-01, -3.2039e-02,  2.0246e-02,\n",
      "        -1.0705e-01, -1.1330e-01, -1.5038e-01, -1.6574e-04, -1.6037e-01,\n",
      "        -2.4433e-02, -1.2295e-01, -2.5497e-03,  1.0235e-01, -1.1345e-02,\n",
      "        -1.8154e-01, -2.1822e-01, -2.0134e-01, -1.5857e-01,  4.3823e-02,\n",
      "        -5.6635e-02, -1.5368e-01, -1.2493e-02, -2.8714e-02, -3.8320e-02,\n",
      "        -8.2360e-02,  1.2364e-01, -1.4382e-02, -8.2222e-02, -1.8264e-01,\n",
      "         4.0573e-02, -1.0376e-02, -1.7693e-01,  8.0791e-03, -6.8655e-02,\n",
      "        -5.4999e-02,  2.5952e-02, -1.6748e-01,  5.1375e-02, -1.0109e-01,\n",
      "         5.5104e-02,  3.7487e-02,  1.7259e-02, -1.3595e-02, -3.5037e-02,\n",
      "        -3.4954e-02,  2.0462e-02,  4.4007e-02, -1.6242e-01,  1.2081e-01,\n",
      "        -3.2653e-02, -1.9134e-01, -1.1334e-01, -1.2296e-01,  6.5749e-02,\n",
      "        -3.9662e-02, -1.0850e-02,  5.8681e-02, -2.5109e-02,  9.2697e-02,\n",
      "         3.6644e-02, -1.2753e-01, -3.0047e-02, -1.5387e-01,  1.9868e-01,\n",
      "         8.1990e-02,  1.0085e-02, -1.1966e-01, -1.3910e-01,  1.5425e-02,\n",
      "        -7.0795e-02, -1.4518e-01,  4.5030e-02, -1.9701e-01,  2.8757e-01,\n",
      "        -7.8736e-02,  9.8432e-02, -1.4933e-01,  3.2095e-02, -1.4380e-01,\n",
      "         1.3457e-01, -1.8757e-01,  1.2337e-02, -1.3933e-02, -1.9803e-02,\n",
      "         3.3760e-02,  1.1766e-01, -2.3623e-02, -7.7085e-02, -6.9015e-02,\n",
      "        -4.7504e-02, -1.0958e-02, -8.6287e-02, -5.6687e-02, -1.1106e-01,\n",
      "        -2.0821e-02,  4.6648e-02,  3.1444e-02, -9.5845e-02, -1.9755e-03,\n",
      "         4.7881e-03,  1.2773e-02, -9.6455e-02, -1.2386e-01, -5.9802e-02,\n",
      "         7.9262e-02,  7.3568e-03, -3.6615e-02, -4.5786e-02,  4.1607e-02,\n",
      "         6.1913e-02, -3.7574e-02,  1.2770e-03,  9.5701e-02, -5.5342e-02,\n",
      "        -8.8870e-02,  4.9564e-02,  3.9232e-02, -7.2741e-02,  5.1207e-02,\n",
      "         2.2970e-02, -1.3690e-01, -8.0608e-02, -2.8596e-02, -1.7453e-01,\n",
      "         8.7330e-03,  1.2724e-02, -2.5771e-01,  5.2816e-02, -1.7650e-01,\n",
      "         1.3948e-02, -1.2845e-01, -7.8888e-03, -1.6276e-01, -4.2525e-02,\n",
      "        -4.9985e-02, -1.2212e-01, -5.7135e-02, -1.1809e-01, -1.9527e-01,\n",
      "        -1.2979e-01, -6.7311e-02, -1.2125e-02, -2.0234e-01, -9.8934e-02,\n",
      "        -1.1762e-01,  6.0935e-02, -1.2125e-01, -9.3919e-02, -1.6508e-02,\n",
      "         2.4559e-02, -7.7166e-02, -1.1483e-01, -3.1505e-02,  4.4019e-02,\n",
      "        -1.6113e-02, -3.0886e-02, -4.4657e-02,  8.3641e-02, -1.0355e-01,\n",
      "         3.7498e-02, -1.1162e-01,  7.8732e-02, -1.3090e-01, -6.0257e-02,\n",
      "        -1.2857e-01, -5.2444e-03, -8.5261e-02, -8.7307e-04, -2.8444e-02,\n",
      "        -8.5407e-02, -1.1726e-01, -1.1922e-01, -1.2366e-01, -4.4990e-03,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        -1.7814e-01, -1.0464e-01,  5.5313e-02, -4.9438e-02]))])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Later to restore:\n",
    "lp = torch.load(\"params/LMtfparams7455419561970809\")\n",
    "print(lp)\n",
    "nb_decoders = lp[\"nb_decoders\"]\n",
    "vector_size = lp[\"vector_size\"]\n",
    "nb_heads = lp[\"nb_heads\"]\n",
    "head_size = lp[\"head_size\"]\n",
    "max_length = lp[\"max_length\"]\n",
    "ffn_hidden_size = lp[\"ffn_hidden_size\"]\n",
    "vocab_size = lp[\"vocab_size\"]\n",
    "model_params_dict = lp[\"model_params_dict\"]\n",
    "\n",
    "LMtransformer = buildTransformer(vector_size, nb_decoders, nb_heads, head_size, ffn_hidden_size, vocab_size)\n",
    "LMtransformer.load_state_dict(model_params_dict)\n",
    "\n",
    "#Attention, pour pouvoir générer il faut reconstruire le vocabulaire et ses numéros associés avec le code plus haut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bidouilles pour adapter nos fonctions aux fonctions common codées par Nathra \n",
    "#(sequence list of ints en entree, list of probas en sortie)\n",
    "#(Faire mieux plus tard)\n",
    "def LMtransformerprediction(listints):\n",
    "    return np.exp(LMtransformer(torch.tensor([listints[-8:]]))[0][-1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_seq(prev_seq, top_k=5):\n",
    "    with torch.no_grad():\n",
    "        prev_seq_numbers = [vocab_numbers[token] for token in prev_seq]\n",
    "        sample_token_seq = sample_token_sequence(LMtransformerprediction, prev_seq_numbers, top_k=top_k)\n",
    "        tokens_pred = [vocab_numeroted[i] for i in sample_token_seq]\n",
    "        print(' '.join(tokens_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def gen_seq_maison(prev_seq):\\n    with torch.no_grad():\\n        prev_seq_numbers = [vocab_numbers[token] for token in prev_seq]\\n        indice = np.argmax(np.array(LMtransformer(torch.tensor([prev_seq_numbers]))))\\n        tokens_pred = vocab_numeroted[indice]\\n        print(' '.join(tokens_pred))\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def gen_seq_maison(prev_seq):\n",
    "    with torch.no_grad():\n",
    "        prev_seq_numbers = [vocab_numbers[token] for token in prev_seq]\n",
    "        indice = np.argmax(np.array(LMtransformer(torch.tensor([prev_seq_numbers]))))\n",
    "        tokens_pred = vocab_numeroted[indice]\n",
    "        print(' '.join(tokens_pred))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/100 [00:00<?, ?it/s]C:\\Users\\Eric\\statapp_language_model\\statapp\\transformer\\pytorch\\transformer_model.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(torch.add(embedded, pos_encodings), dtype=torch.float32)\n",
      "100%|████████████████████████████████████████| 100/100 [00:01<00:00, 95.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dur titres `` ans lumiere alain leurs 33 distributeur routes par ruthenie 39 prophetiques egalement societe alain pendant valeur le quand alain pendant cette via jacques vitoria-gasteiz weedub obtenu ainsi ( alain guerre qui 31 lorsque divine alain eau sont alain forale role eau cagnes alava fut aller contes la arrive favoriser dans eau noms aider lettres numerique partie alain fait centre 33 alain paru vitoria-gasteiz probablement 33 dure 33 barcelone avoir lorsque surface ducouret chanson <unk> commune vrai oth commune avoir lorsque ecraser ont 33 commune eau ma fut animait commune alain historien reprend connaissances commune reochestration son s\n"
     ]
    }
   ],
   "source": [
    "gen_seq(['il'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [00:01<00:00, 87.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lorsque goslicz commune perturbe industrie commune assume 33 egalement : commune d 31 lorsque goslicz commune perturbe industrie commune vrai oth commune commune avoir lorsque ecraser ont 33 commune eau ma fut animait commune alain historien reprend connaissances commune reochestration son s commune avoir lorsque territoires qui ' armee forcat messie avoir lorsque vive d titres 36 avoir lorsque touche qui villes aeroport met loc avoir lorsque surface ou commune ami connaissances 30 commune d 31 lorsque goslicz commune perturbe industrie commune assume 33 egalement : commune d 31 lorsque goslicz commune perturbe industrie commune assume 33 egalement : commune\n"
     ]
    }
   ],
   "source": [
    "gen_seq(['a','l','age','de','31'], top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [00:01<00:00, 98.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 enfants etat afin bureau siecle transmettre ' numerique leon histoire lorsque xxe prophetiques egalement societe alain pendant cette via jacques vitoria-gasteiz weedub obtenu ainsi ( alain guerre qui 31 lorsque divine alain eau sont alain forale role eau cagnes alava fut aller contes la arrive favoriser dans eau noms aider lettres numerique partie alain fait centre 33 alain paru vitoria-gasteiz probablement 33 dure 33 barcelone avoir lorsque surface ou commune lorsque surface ou commune 38 fernand la redige eau les commune avoir lorsque deputation 33 commune vrai oth commune avoir lorsque ecraser ont 33 commune eau ma fut animait\n"
     ]
    }
   ],
   "source": [
    "gen_seq(['<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>'], top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [00:01<00:00, 91.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' esprit prophetique apres avoir obtenu la connaissance du vitoria-gasteiz de s ' ecraser valeur de tusson pendant la seconde moitie lumiere sur la surface du goudron qu ' il pourrait confondre avec la surface d ' une guerre entre la pologne et la ruthenie qui dure depuis plusieurs annees . ainsi , en 1281 , les polonais ont deja vaincu une armee mongole pres de goslicz , lorsque ces derniers sont entres sur les territoires de lech ii le noir pour aller aider leon ier de galicie galicie galicie . l ' histoire de la commune de tusson pendant\n"
     ]
    }
   ],
   "source": [
    "gen_seq(['barcelone',',','il','est','touche','par','l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(tokens)<100:\n",
    "    print(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
