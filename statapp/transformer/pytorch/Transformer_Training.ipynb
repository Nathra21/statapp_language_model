{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformer_model import *\n",
    "import nltk\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"../../..\")\n",
    "\n",
    "from statapp.common.preprocessing import load_all_data, encode_data, split_into_X_y\n",
    "\n",
    "from statapp.common.sampling import sample_token_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing maison assez brouillon pour le moment... L'encodage est effectué au niveau des mots. Les données exploitées sont placées dans le dossier data dans le dossier du notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = load_all_data(\"data/fr.train.top1M.txt\", sample=0.000001)\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "vocab = list(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico = {}\n",
    "\n",
    "for word in vocab:\n",
    "    dico[word]=0\n",
    "    \n",
    "for token in tokens:\n",
    "    dico[token]+=1\n",
    "    \n",
    "sorted_list = sorted(dico.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "sorted_dico = {}\n",
    "\n",
    "for i in range(min(len(sorted_list),vocab_size-1)):\n",
    "    sorted_dico[sorted_list[i][0]] = sorted_list[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokens)):\n",
    "    if tokens[i] not in sorted_dico:\n",
    "        tokens[i] = \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les données exploitées contiennent 57 tokens (mots) au total.\n",
      "La taille du vocabulaire ainsi constitué est de 37\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(tokens))\n",
    "\n",
    "if \"<unk>\" not in vocab:\n",
    "    vocab.append(\"<unk>\")\n",
    "    \n",
    "vocab_size = len(vocab)\n",
    "\n",
    "vocab_numbers = dict(zip(vocab, range(0,len(vocab))))\n",
    "vocab_numeroted = dict(zip(range(0,len(vocab)), vocab))\n",
    "tokens_numbers = np.array([vocab_numbers[tokens[i]] for i in range(len(tokens))])\n",
    "\n",
    "tokens_numbers_sequences = np.array([ tokens_numbers[i:i+max_length+1] for i in range(len(tokens_numbers)-max_length)])\n",
    "tokens_numbers_sequences = torch.tensor(tokens_numbers_sequences , dtype=torch.int64)\n",
    "\n",
    "nb_sequences =  tokens_numbers_sequences.shape[0]\n",
    "\n",
    "print(\"Les données exploitées contiennent {} tokens (mots) au total.\".format(len(tokens)))\n",
    "print(\"La taille du vocabulaire ainsi constitué est de {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les données de test exploitées contiennent 494 tokens (mots) au total.\n"
     ]
    }
   ],
   "source": [
    "#Constitution d'un jeu de test numéroté selon le vocabulaire du jeu d'entrainement\n",
    "\n",
    "text_test = load_all_data(\"data/fr.train.top1M.txt\", start=0.99999, sample=0.00001)\n",
    "\n",
    "tokens_test = nltk.word_tokenize(text_test)\n",
    "\n",
    "for i in range(len(tokens_test)):\n",
    "    if tokens_test[i] not in vocab:\n",
    "        tokens_test[i] = \"<unk>\"\n",
    "\n",
    "tokens_numbers_test = np.array([vocab_numbers[tokens_test[i]] for i in range(len(tokens_test))])\n",
    "\n",
    "tokens_numbers_sequences_test = np.array([ tokens_numbers_test[i:i+max_length+1] for i in range(len(tokens_numbers_test)-max_length)])\n",
    "tokens_numbers_sequences_test = torch.tensor(tokens_numbers_sequences_test , dtype=torch.int64)\n",
    "\n",
    "nb_sequences_test =  tokens_numbers_sequences_test.shape[0]\n",
    "\n",
    "print(\"Les données de test exploitées contiennent {} tokens (mots) au total.\".format(len(tokens_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LMtransformer = buildTransformer(vector_size, nb_decoders, nb_heads, head_size, ffn_hidden_size, vocab_size)\n",
    "#Correspond à utiliser l'entropie croisée puisque les sorties sont des log_softmax\n",
    "#et l'entropie croisée = nll_loss(log_softmax(.), target)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(LMtransformer.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Affichage de la loss sur les données de test\n",
    "\n",
    "test_output = LMtransformer(tokens_numbers_sequences_test[:,:-1])\n",
    "test_loss = criterion(test_output.reshape(-1, vocab_size), tokens_numbers_sequences_test[:,1:].flatten())\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(nb_epochs, batch_size):\n",
    "    \n",
    "    #What is this ?? I don't remember. Make grad required ?\n",
    "    LMtransformer.train()\n",
    "    \n",
    "    #pas pour l'affichage progressif de la loss\n",
    "    step = max(1,((len(tokens)-max_length-1)/batch_size)//5)\n",
    "    \n",
    "    epochs_losses = []\n",
    "    losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(nb_epochs):\n",
    "        \n",
    "        running_loss = 0\n",
    "        \n",
    "        randperm = torch.randperm(nb_sequences)\n",
    "        randperm = randperm[:(nb_sequences//batch_size)*batch_size]\n",
    "        batchs_indices = randperm.reshape(nb_sequences//batch_size, batch_size)\n",
    "        \n",
    "        for i, batch_indices in enumerate(batchs_indices):\n",
    "            \n",
    "            batch = tokens_numbers_sequences[batch_indices]\n",
    "            optimizer.zero_grad()\n",
    "            output = LMtransformer(batch[:,:-1])\n",
    "            loss = criterion(output.reshape(-1, vocab_size), batch[:,1:].flatten())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            test_loss = 0\n",
    "            \n",
    "            #Il faudrait adapter les affichages en fonction du nombre de batchs total\n",
    "            running_loss += loss.item()\n",
    "            if i % step == step-1:\n",
    "                \n",
    "                #Calcul de la loss sur les données de test\n",
    "                test_output = LMtransformer(tokens_numbers_sequences_test[:,:-1])\n",
    "                test_loss = criterion(test_output.reshape(-1, vocab_size), tokens_numbers_sequences_test[:,1:].flatten())\n",
    "                \n",
    "                print('[%d, %5d] loss: %.3f ; test_loss : %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / step, test_loss))\n",
    "                \n",
    "                #stock pour affichage graphique\n",
    "                epochs_losses.append(epoch-1+(i/((len(tokens)-max_length-1)/batch_size)))\n",
    "                losses.append(running_loss / step)\n",
    "                test_losses.append(test_loss)\n",
    "                \n",
    "                running_loss = 0.\n",
    "                \n",
    "        plt.plot(epochs_losses, losses)\n",
    "        plt.plot(epochs_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test d'overfitting sur un cas ultrasimplifié (5 tokens, longueur de séquence 1, 3 decoders, 2 heads) :\n",
    "- En observant les sorties le modèle a bien appris et overfitte ! (loss à 0 au bout de 5-6 epochs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 4.157 ; test_loss : 6.119\n",
      "[1,     2] loss: 2.755 ; test_loss : 6.473\n",
      "[1,     3] loss: 2.051 ; test_loss : 6.732\n",
      "[1,     4] loss: 1.611 ; test_loss : 6.802\n",
      "[2,     1] loss: 0.655 ; test_loss : 7.196\n",
      "[2,     2] loss: 0.431 ; test_loss : 7.998\n",
      "[2,     3] loss: 0.640 ; test_loss : 8.646\n",
      "[2,     4] loss: 0.400 ; test_loss : 8.997\n",
      "[3,     1] loss: 0.302 ; test_loss : 9.550\n",
      "[3,     2] loss: 0.393 ; test_loss : 9.659\n",
      "[3,     3] loss: 0.213 ; test_loss : 9.886\n",
      "[3,     4] loss: 0.320 ; test_loss : 10.112\n",
      "[4,     1] loss: 0.248 ; test_loss : 10.174\n",
      "[4,     2] loss: 0.386 ; test_loss : 10.041\n",
      "[4,     3] loss: 0.084 ; test_loss : 10.202\n",
      "[4,     4] loss: 0.162 ; test_loss : 10.598\n",
      "[5,     1] loss: 0.168 ; test_loss : 11.364\n",
      "[5,     2] loss: 0.491 ; test_loss : 11.803\n",
      "[5,     3] loss: 0.293 ; test_loss : 12.410\n",
      "[5,     4] loss: 0.453 ; test_loss : 13.725\n",
      "[6,     1] loss: 0.114 ; test_loss : 14.884\n",
      "[6,     2] loss: 0.359 ; test_loss : 14.736\n",
      "[6,     3] loss: 0.183 ; test_loss : 13.415\n",
      "[6,     4] loss: 0.269 ; test_loss : 11.636\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model(6,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde des paramètres du modèle obtenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Un dico  des hyperparams serait pratique ^^\n",
    "torch.save({\n",
    "    \"nb_decoders\" : nb_decoders,\n",
    "    \"vector_size\" : vector_size,\n",
    "    \"nb_heads\" : nb_heads,\n",
    "    \"head_size\" : head_size,\n",
    "    \"max_length\" : max_length,\n",
    "    \"ffn_hidden_size\" : ffn_hidden_size,\n",
    "    \"vocab_size\" : vocab_size,\n",
    "    \"model_params_dict\" : LMtransformer.state_dict()}\n",
    "    ,\n",
    "    \"params/LMtfparamsTEST\")\n",
    "    #\"params/LMtfparams\"+str(np.random.rand())[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(LMtransformer.state_dict(),\"params/LMtfparamsTEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "LMtransformerTEST = buildTransformer(vector_size, nb_decoders, nb_heads, head_size, ffn_hidden_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LMtransformerTEST.load_state_dict(torch.load(\"params/LMtfparamsTEST\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Later to restore:\n",
    "lp = torch.load(\"params/LMtfparamsTEST\")\n",
    "\n",
    "nb_decoders = lp[\"nb_decoders\"]\n",
    "vector_size = lp[\"vector_size\"]\n",
    "nb_heads = lp[\"nb_heads\"]\n",
    "head_size = lp[\"head_size\"]\n",
    "max_length = lp[\"max_length\"]\n",
    "ffn_hidden_size = lp[\"ffn_hidden_size\"]\n",
    "vocab_size = lp[\"vocab_size\"]\n",
    "model_params_dict = lp[\"model_params_dict\"]\n",
    "\n",
    "LMtransformer = buildTransformer(vector_size, nb_decoders, nb_heads, head_size, ffn_hidden_size, vocab_size)\n",
    "LMtransformer.load_state_dict(model_params_dict)\n",
    "\n",
    "#Attention, pour pouvoir générer il faut reconstruire le vocabulaire et ses numéros associés avec le code plus haut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb_decoders': 2,\n",
       " 'vector_size': 64,\n",
       " 'nb_heads': 4,\n",
       " 'head_size': 16,\n",
       " 'max_length': 8,\n",
       " 'ffn_hidden_size': 256,\n",
       " 'vocab_size': 37,\n",
       " 'model_params_dict': OrderedDict([('decoders.0.multihead_attention.w_q.weight',\n",
       "               tensor([[-0.0271,  0.0056,  0.1566,  ...,  0.1179, -0.0961, -0.0347],\n",
       "                       [ 0.0146,  0.0962,  0.1196,  ...,  0.1068,  0.0521,  0.1369],\n",
       "                       [ 0.0126, -0.0786,  0.0511,  ..., -0.1648, -0.1106, -0.0078],\n",
       "                       ...,\n",
       "                       [ 0.1296, -0.1750,  0.1105,  ..., -0.0109,  0.0793, -0.0040],\n",
       "                       [ 0.0485, -0.0480, -0.1245,  ..., -0.1292, -0.0248, -0.0801],\n",
       "                       [ 0.0093, -0.0551,  0.0265,  ..., -0.1846, -0.1551,  0.1308]])),\n",
       "              ('decoders.0.multihead_attention.w_q.bias',\n",
       "               tensor([-0.0820,  0.0935, -0.1954,  0.0643,  0.1231, -0.1581,  0.0057, -0.0018,\n",
       "                        0.0766,  0.1505, -0.0992,  0.0917, -0.1221,  0.1180, -0.1185,  0.1692,\n",
       "                        0.0849, -0.1441, -0.1325,  0.0890, -0.1446,  0.1648,  0.0399, -0.0647,\n",
       "                        0.2645,  0.0489,  0.0412, -0.0580,  0.0993, -0.1436,  0.2734, -0.0976,\n",
       "                        0.2013,  0.1602, -0.1437, -0.1274,  0.1634,  0.0573, -0.1688,  0.1880,\n",
       "                       -0.0233,  0.2321,  0.0222,  0.0253,  0.1427,  0.2140, -0.0869,  0.1044,\n",
       "                       -0.1238,  0.1033, -0.0658,  0.0767, -0.2444,  0.0654, -0.0627, -0.0147,\n",
       "                       -0.0224,  0.1043,  0.1417, -0.1108, -0.0590, -0.1670,  0.0482,  0.2016])),\n",
       "              ('decoders.0.multihead_attention.w_k.weight',\n",
       "               tensor([[ 0.1074,  0.0081, -0.0371,  ...,  0.1572,  0.1309,  0.1443],\n",
       "                       [-0.0542,  0.0523, -0.0975,  ...,  0.0570, -0.0602,  0.2148],\n",
       "                       [ 0.1374,  0.2142,  0.0616,  ...,  0.0692, -0.0904, -0.0824],\n",
       "                       ...,\n",
       "                       [ 0.0486, -0.0468, -0.0992,  ..., -0.0801, -0.0796, -0.0011],\n",
       "                       [-0.0409, -0.0967,  0.1045,  ..., -0.0323, -0.0692,  0.0701],\n",
       "                       [-0.0483,  0.0064, -0.2087,  ...,  0.1012, -0.0134,  0.0725]])),\n",
       "              ('decoders.0.multihead_attention.w_k.bias',\n",
       "               tensor([ 0.1079, -0.0620, -0.0232, -0.0931,  0.1009,  0.1215,  0.0963,  0.0691,\n",
       "                       -0.0129,  0.0418,  0.0172, -0.0846, -0.0691, -0.0796, -0.0274,  0.0907,\n",
       "                       -0.0066,  0.1009,  0.0708,  0.0894, -0.1037,  0.0201, -0.0555,  0.0229,\n",
       "                       -0.1213,  0.0156, -0.0394, -0.1040, -0.0188, -0.0227, -0.1019,  0.0309,\n",
       "                       -0.1108,  0.0309,  0.0288, -0.0607,  0.0428, -0.0004, -0.0145, -0.0711,\n",
       "                        0.0895, -0.1100, -0.0543,  0.0035, -0.0988, -0.1088, -0.0363, -0.0576,\n",
       "                       -0.0045, -0.0094, -0.0515,  0.1222,  0.1026,  0.1240,  0.1237, -0.0239,\n",
       "                        0.1154, -0.1092, -0.0239, -0.0019,  0.1221,  0.0922, -0.0097,  0.0707])),\n",
       "              ('decoders.0.multihead_attention.w_v.weight',\n",
       "               tensor([[ 0.0697,  0.0057,  0.1245,  ..., -0.2035,  0.0184,  0.0035],\n",
       "                       [ 0.1501,  0.0334,  0.0997,  ..., -0.0301, -0.0316,  0.0042],\n",
       "                       [-0.0351, -0.0696,  0.0148,  ..., -0.1393, -0.0284, -0.0757],\n",
       "                       ...,\n",
       "                       [-0.0117,  0.1750, -0.0913,  ..., -0.0583, -0.1018,  0.0490],\n",
       "                       [ 0.0981,  0.0518, -0.0175,  ...,  0.0923,  0.0465, -0.0909],\n",
       "                       [ 0.0245,  0.1240,  0.1464,  ...,  0.0499,  0.1152,  0.0739]])),\n",
       "              ('decoders.0.multihead_attention.w_v.bias',\n",
       "               tensor([ 0.0560,  0.0625,  0.0426, -0.0324,  0.0402, -0.0364,  0.0709, -0.0544,\n",
       "                       -0.0577, -0.1160,  0.0543,  0.0448,  0.1172,  0.0902,  0.0712, -0.0194,\n",
       "                       -0.1051, -0.0155, -0.0148, -0.0262, -0.0758, -0.0726,  0.0177, -0.0030,\n",
       "                        0.0759, -0.0398, -0.0199, -0.0029, -0.0684, -0.0697, -0.0447, -0.0962,\n",
       "                       -0.0449,  0.0926,  0.1217, -0.0514, -0.0910,  0.0708, -0.1379,  0.1368,\n",
       "                       -0.0300,  0.0986,  0.0235,  0.0063,  0.1729, -0.0831, -0.0621, -0.0219,\n",
       "                       -0.0996, -0.0459, -0.0016, -0.0412,  0.1016,  0.0593,  0.0016,  0.0874,\n",
       "                       -0.0333, -0.1326, -0.0475,  0.0077,  0.0722, -0.1126, -0.1037, -0.0643])),\n",
       "              ('decoders.0.multihead_attention.w_0.weight',\n",
       "               tensor([[ 0.0058,  0.0979,  0.0599,  ..., -0.0057, -0.0689, -0.0360],\n",
       "                       [-0.1150,  0.0015, -0.0448,  ...,  0.0029, -0.1313, -0.1184],\n",
       "                       [-0.0645, -0.1342,  0.1810,  ..., -0.0179,  0.0554,  0.0979],\n",
       "                       ...,\n",
       "                       [-0.0198,  0.0274, -0.1909,  ..., -0.1283, -0.0608, -0.1287],\n",
       "                       [-0.1707,  0.0349, -0.1235,  ...,  0.0807, -0.0640,  0.0546],\n",
       "                       [ 0.0153,  0.1117,  0.1451,  ..., -0.0177, -0.1340, -0.1097]])),\n",
       "              ('decoders.0.multihead_attention.w_0.bias',\n",
       "               tensor([ 0.0210,  0.0383, -0.0820,  0.0218, -0.1082, -0.0540,  0.0452,  0.0228,\n",
       "                        0.0017,  0.0025,  0.0837, -0.0686,  0.0074, -0.0473,  0.0613, -0.1354,\n",
       "                        0.0726, -0.0534,  0.0063, -0.0329,  0.0798, -0.0462, -0.0915,  0.0683,\n",
       "                        0.0714,  0.0222,  0.1176, -0.0823,  0.0235, -0.0906, -0.0518,  0.0039,\n",
       "                        0.0171,  0.0598,  0.0717, -0.0393,  0.0330,  0.0622,  0.0530,  0.0039,\n",
       "                        0.0385,  0.0543,  0.0914,  0.0464, -0.0133,  0.0457,  0.1102, -0.0868,\n",
       "                        0.1346, -0.0620,  0.0630, -0.0303,  0.0011, -0.0812,  0.0880,  0.0006,\n",
       "                       -0.0898, -0.0958, -0.1012,  0.0278,  0.0703, -0.1267,  0.0726,  0.0034])),\n",
       "              ('decoders.0.feedforward_network.fc1.weight',\n",
       "               tensor([[ 0.0440,  0.1572,  0.1611,  ...,  0.0371,  0.1059, -0.0745],\n",
       "                       [-0.0458,  0.0599,  0.1009,  ..., -0.0106,  0.1600,  0.1798],\n",
       "                       [ 0.0668,  0.0089,  0.0749,  ...,  0.0272,  0.1038,  0.0202],\n",
       "                       ...,\n",
       "                       [-0.0657, -0.0094, -0.0545,  ...,  0.0920,  0.0072,  0.0898],\n",
       "                       [-0.1430, -0.0994,  0.1264,  ..., -0.0239, -0.0526, -0.2461],\n",
       "                       [ 0.0447, -0.0026, -0.0585,  ..., -0.1292,  0.0612,  0.0262]])),\n",
       "              ('decoders.0.feedforward_network.fc1.bias',\n",
       "               tensor([-4.5364e-02,  2.5782e-03,  1.1125e-01, -9.6116e-02, -6.3876e-02,\n",
       "                       -3.7701e-02,  1.0443e-01, -1.4107e-01, -3.7244e-02, -9.5766e-02,\n",
       "                       -1.7080e-01, -1.0659e-01, -7.1065e-02, -2.4906e-03, -9.1293e-02,\n",
       "                        5.7340e-03, -1.2981e-02, -2.8815e-02, -4.5560e-02,  6.8371e-02,\n",
       "                        6.7500e-02, -1.1377e-01, -8.3908e-02,  2.3198e-02, -2.4090e-02,\n",
       "                        1.4611e-02, -6.7614e-02, -1.2840e-01,  7.0591e-02,  8.3990e-02,\n",
       "                       -1.0304e-01, -6.4157e-02, -1.5162e-01, -3.8307e-02, -2.0085e-02,\n",
       "                        5.8677e-02, -6.0708e-02, -5.2421e-02, -1.1932e-02, -2.6409e-02,\n",
       "                       -9.3861e-02,  6.1624e-03,  2.8166e-02,  7.7961e-02,  1.0514e-01,\n",
       "                       -9.8061e-02,  5.6473e-02, -5.4728e-02,  1.5142e-01,  4.8861e-02,\n",
       "                        1.8521e-02, -7.9549e-02,  9.3114e-03,  1.5612e-03, -2.2482e-03,\n",
       "                       -3.0072e-03, -5.1450e-02,  6.6693e-02,  4.9759e-02, -1.0530e-01,\n",
       "                        6.4035e-02, -1.6155e-02,  9.2970e-02, -2.9796e-02,  4.4490e-02,\n",
       "                       -1.0756e-01,  2.6302e-02, -7.1497e-02,  1.2368e-01, -1.6374e-01,\n",
       "                        1.1294e-01, -1.2443e-01,  1.5022e-01, -6.9578e-02,  8.3833e-03,\n",
       "                       -1.1121e-01, -1.0840e-02,  1.4245e-01, -5.8337e-02,  3.3210e-02,\n",
       "                       -1.2408e-01,  3.7647e-02, -3.6515e-03, -8.5335e-02,  2.7989e-02,\n",
       "                       -1.7694e-01, -8.0265e-03, -5.5539e-02,  5.2680e-02,  6.4598e-02,\n",
       "                       -1.0870e-01,  9.7865e-02, -7.1002e-03,  1.1148e-01, -6.2864e-02,\n",
       "                        1.5019e-01,  1.2894e-01,  2.4251e-02,  2.6354e-02, -1.6413e-01,\n",
       "                        8.2085e-02, -1.3862e-01,  1.0962e-01, -1.4392e-01, -1.2625e-01,\n",
       "                       -2.6836e-03, -9.2659e-02, -1.3911e-01, -3.7938e-02, -5.5083e-02,\n",
       "                        3.3661e-02, -1.4033e-01,  4.9898e-03, -6.0987e-02, -9.9925e-02,\n",
       "                       -9.0458e-02,  8.5762e-02,  1.6832e-01,  1.0287e-01, -1.0051e-01,\n",
       "                       -2.6165e-02, -5.9228e-02, -6.6228e-02,  1.8613e-02,  2.3184e-02,\n",
       "                       -6.2378e-02, -6.1685e-02,  7.0979e-02,  3.6879e-02, -3.3140e-03,\n",
       "                        3.8616e-02, -2.5666e-02,  7.7339e-03, -3.2116e-02,  4.3693e-02,\n",
       "                        9.7444e-02,  1.6226e-01,  1.2726e-02, -3.6590e-02, -1.4686e-01,\n",
       "                       -3.1791e-02,  6.6119e-02, -2.2792e-03, -4.3551e-02, -3.1137e-03,\n",
       "                       -4.3463e-02, -1.5680e-02,  3.0658e-02, -1.7218e-01,  6.9492e-02,\n",
       "                        8.8302e-02,  1.0782e-01, -7.8949e-02, -8.2316e-02, -2.9515e-02,\n",
       "                        1.7622e-04,  5.9369e-02, -1.2371e-01,  1.2418e-02,  1.2130e-01,\n",
       "                        1.1329e-01, -7.8896e-03, -1.0250e-01,  2.2990e-02, -2.1445e-02,\n",
       "                       -1.8978e-01, -1.3285e-01, -1.3743e-03,  2.5382e-02, -5.0518e-02,\n",
       "                       -1.2863e-01, -4.0873e-03,  3.5300e-02,  5.9610e-02, -1.7993e-01,\n",
       "                        1.4461e-02, -1.1073e-01, -1.6655e-01, -3.4187e-03,  7.3626e-02,\n",
       "                       -1.4002e-01,  5.3026e-03, -1.4244e-03, -9.9623e-02, -2.3715e-02,\n",
       "                       -8.2813e-02, -4.1928e-02, -8.5669e-02,  1.7837e-01, -8.1116e-02,\n",
       "                       -2.0517e-03,  4.5719e-02,  7.2254e-02,  4.6958e-03, -1.2457e-01,\n",
       "                        1.1212e-01,  4.1297e-02,  5.1978e-02, -1.2084e-01, -1.3244e-01,\n",
       "                        3.3139e-02, -1.1163e-01, -1.9606e-02, -1.1924e-01, -6.3510e-03,\n",
       "                        1.1757e-01, -1.1701e-01,  9.4653e-03, -7.8803e-02,  2.2541e-02,\n",
       "                       -4.2892e-02, -5.1671e-02,  4.2014e-02,  5.5139e-02,  3.5706e-02,\n",
       "                        8.0196e-02,  4.7799e-03, -1.3823e-01, -4.0363e-02, -4.5386e-02,\n",
       "                        4.1759e-02, -1.0007e-01,  1.4051e-01, -3.8996e-02,  8.0924e-02,\n",
       "                       -1.1588e-01, -1.6245e-02,  7.4492e-02,  2.6132e-02,  4.8038e-02,\n",
       "                        2.1731e-02, -7.2918e-02, -7.8475e-02,  3.5006e-02,  6.3366e-02,\n",
       "                       -2.5986e-02,  1.2650e-01,  4.9178e-02, -3.6763e-03,  7.5441e-02,\n",
       "                        4.6037e-02, -3.7233e-03,  1.0257e-01, -3.1544e-02,  9.6585e-02,\n",
       "                       -5.8964e-02, -2.0253e-02, -3.1041e-02, -1.4421e-02, -3.5666e-02,\n",
       "                       -1.1328e-02,  3.9038e-02, -1.1189e-01,  3.5505e-02,  8.5611e-02,\n",
       "                        6.2669e-02])),\n",
       "              ('decoders.0.feedforward_network.fc2.weight',\n",
       "               tensor([[ 0.0801, -0.0191,  0.0103,  ..., -0.0282,  0.1423, -0.1176],\n",
       "                       [-0.0100,  0.0417, -0.0560,  ..., -0.0455, -0.0793,  0.1218],\n",
       "                       [ 0.0836, -0.0339,  0.0236,  ..., -0.1318,  0.1178, -0.0723],\n",
       "                       ...,\n",
       "                       [-0.1165, -0.0898, -0.0331,  ..., -0.0362,  0.0724,  0.0169],\n",
       "                       [ 0.0099,  0.0238, -0.0200,  ...,  0.0184,  0.1363,  0.0575],\n",
       "                       [-0.0474,  0.0585, -0.1073,  ..., -0.0734,  0.0368, -0.0436]])),\n",
       "              ('decoders.0.feedforward_network.fc2.bias',\n",
       "               tensor([ 0.0474, -0.0478, -0.0320, -0.0538, -0.0127,  0.0129, -0.0028,  0.0390,\n",
       "                        0.0045, -0.0918, -0.0802, -0.0543,  0.0187, -0.0466,  0.0047, -0.0512,\n",
       "                       -0.0375, -0.0671,  0.0136, -0.0086,  0.0140, -0.0728,  0.0762,  0.0518,\n",
       "                        0.0273, -0.0459,  0.0392, -0.0496, -0.0249, -0.0483,  0.0119,  0.0539,\n",
       "                        0.0004, -0.0653,  0.0447, -0.0694, -0.0615, -0.0376,  0.0107, -0.0436,\n",
       "                        0.0392, -0.0360, -0.0494,  0.0129,  0.0476, -0.0292,  0.0120, -0.0522,\n",
       "                        0.0045, -0.0092,  0.0164, -0.0260,  0.0605, -0.0298, -0.0364, -0.0056,\n",
       "                       -0.0382, -0.0386, -0.0529, -0.0612,  0.0310, -0.0654,  0.0522,  0.0338])),\n",
       "              ('decoders.0.layernorma.weight',\n",
       "               tensor([0.9972, 0.9933, 1.1337, 1.0700, 1.1072, 1.0857, 1.0239, 1.0610, 1.1228,\n",
       "                       0.9814, 1.0597, 1.0420, 1.0525, 1.0364, 1.0751, 0.9551, 1.0763, 0.9851,\n",
       "                       1.1162, 1.0206, 1.0386, 1.0209, 1.0516, 0.9699, 0.9549, 1.0480, 1.0524,\n",
       "                       1.0343, 1.0729, 1.1397, 0.9858, 1.0501, 0.9264, 1.0264, 1.0557, 1.0638,\n",
       "                       0.9603, 1.0310, 0.9627, 1.0684, 1.0891, 1.0116, 1.0575, 0.9694, 1.0558,\n",
       "                       0.9965, 1.0907, 1.0287, 1.0216, 1.1063, 0.9695, 1.0642, 0.9887, 1.0148,\n",
       "                       1.0460, 1.0410, 1.0854, 0.9947, 1.0487, 0.9793, 0.9942, 1.0852, 0.9409,\n",
       "                       0.9717])),\n",
       "              ('decoders.0.layernorma.bias',\n",
       "               tensor([-0.0062, -0.0215, -0.0018,  0.0039,  0.0695, -0.0157,  0.0702, -0.0509,\n",
       "                        0.0696, -0.0763,  0.1168, -0.0749,  0.0304, -0.0270,  0.1042, -0.0466,\n",
       "                        0.0067, -0.0622,  0.1307, -0.0375, -0.0267, -0.0554,  0.0662, -0.0949,\n",
       "                        0.0493,  0.0295,  0.0591, -0.0586, -0.0144, -0.0125,  0.0627, -0.0596,\n",
       "                        0.1122, -0.0707,  0.0240, -0.0581,  0.0823, -0.0282,  0.0786, -0.0878,\n",
       "                        0.0127, -0.0433,  0.0139, -0.0669,  0.0421, -0.0781,  0.0224, -0.0211,\n",
       "                        0.0148, -0.1011,  0.0203, -0.0446,  0.0337, -0.0396,  0.0959, -0.0771,\n",
       "                        0.0114, -0.0091,  0.0546, -0.0722,  0.0578, -0.0484,  0.0984,  0.0471])),\n",
       "              ('decoders.1.multihead_attention.w_q.weight',\n",
       "               tensor([[-0.0271,  0.0056,  0.1566,  ...,  0.1179, -0.0961, -0.0347],\n",
       "                       [ 0.0146,  0.0962,  0.1196,  ...,  0.1068,  0.0521,  0.1369],\n",
       "                       [ 0.0126, -0.0786,  0.0511,  ..., -0.1648, -0.1106, -0.0078],\n",
       "                       ...,\n",
       "                       [ 0.1296, -0.1750,  0.1105,  ..., -0.0109,  0.0793, -0.0040],\n",
       "                       [ 0.0485, -0.0480, -0.1245,  ..., -0.1292, -0.0248, -0.0801],\n",
       "                       [ 0.0093, -0.0551,  0.0265,  ..., -0.1846, -0.1551,  0.1308]])),\n",
       "              ('decoders.1.multihead_attention.w_q.bias',\n",
       "               tensor([-0.0820,  0.0935, -0.1954,  0.0643,  0.1231, -0.1581,  0.0057, -0.0018,\n",
       "                        0.0766,  0.1505, -0.0992,  0.0917, -0.1221,  0.1180, -0.1185,  0.1692,\n",
       "                        0.0849, -0.1441, -0.1325,  0.0890, -0.1446,  0.1648,  0.0399, -0.0647,\n",
       "                        0.2645,  0.0489,  0.0412, -0.0580,  0.0993, -0.1436,  0.2734, -0.0976,\n",
       "                        0.2013,  0.1602, -0.1437, -0.1274,  0.1634,  0.0573, -0.1688,  0.1880,\n",
       "                       -0.0233,  0.2321,  0.0222,  0.0253,  0.1427,  0.2140, -0.0869,  0.1044,\n",
       "                       -0.1238,  0.1033, -0.0658,  0.0767, -0.2444,  0.0654, -0.0627, -0.0147,\n",
       "                       -0.0224,  0.1043,  0.1417, -0.1108, -0.0590, -0.1670,  0.0482,  0.2016])),\n",
       "              ('decoders.1.multihead_attention.w_k.weight',\n",
       "               tensor([[ 0.1074,  0.0081, -0.0371,  ...,  0.1572,  0.1309,  0.1443],\n",
       "                       [-0.0542,  0.0523, -0.0975,  ...,  0.0570, -0.0602,  0.2148],\n",
       "                       [ 0.1374,  0.2142,  0.0616,  ...,  0.0692, -0.0904, -0.0824],\n",
       "                       ...,\n",
       "                       [ 0.0486, -0.0468, -0.0992,  ..., -0.0801, -0.0796, -0.0011],\n",
       "                       [-0.0409, -0.0967,  0.1045,  ..., -0.0323, -0.0692,  0.0701],\n",
       "                       [-0.0483,  0.0064, -0.2087,  ...,  0.1012, -0.0134,  0.0725]])),\n",
       "              ('decoders.1.multihead_attention.w_k.bias',\n",
       "               tensor([ 0.1079, -0.0620, -0.0232, -0.0931,  0.1009,  0.1215,  0.0963,  0.0691,\n",
       "                       -0.0129,  0.0418,  0.0172, -0.0846, -0.0691, -0.0796, -0.0274,  0.0907,\n",
       "                       -0.0066,  0.1009,  0.0708,  0.0894, -0.1037,  0.0201, -0.0555,  0.0229,\n",
       "                       -0.1213,  0.0156, -0.0394, -0.1040, -0.0188, -0.0227, -0.1019,  0.0309,\n",
       "                       -0.1108,  0.0309,  0.0288, -0.0607,  0.0428, -0.0004, -0.0145, -0.0711,\n",
       "                        0.0895, -0.1100, -0.0543,  0.0035, -0.0988, -0.1088, -0.0363, -0.0576,\n",
       "                       -0.0045, -0.0094, -0.0515,  0.1222,  0.1026,  0.1240,  0.1237, -0.0239,\n",
       "                        0.1154, -0.1092, -0.0239, -0.0019,  0.1221,  0.0922, -0.0097,  0.0707])),\n",
       "              ('decoders.1.multihead_attention.w_v.weight',\n",
       "               tensor([[ 0.0697,  0.0057,  0.1245,  ..., -0.2035,  0.0184,  0.0035],\n",
       "                       [ 0.1501,  0.0334,  0.0997,  ..., -0.0301, -0.0316,  0.0042],\n",
       "                       [-0.0351, -0.0696,  0.0148,  ..., -0.1393, -0.0284, -0.0757],\n",
       "                       ...,\n",
       "                       [-0.0117,  0.1750, -0.0913,  ..., -0.0583, -0.1018,  0.0490],\n",
       "                       [ 0.0981,  0.0518, -0.0175,  ...,  0.0923,  0.0465, -0.0909],\n",
       "                       [ 0.0245,  0.1240,  0.1464,  ...,  0.0499,  0.1152,  0.0739]])),\n",
       "              ('decoders.1.multihead_attention.w_v.bias',\n",
       "               tensor([ 0.0560,  0.0625,  0.0426, -0.0324,  0.0402, -0.0364,  0.0709, -0.0544,\n",
       "                       -0.0577, -0.1160,  0.0543,  0.0448,  0.1172,  0.0902,  0.0712, -0.0194,\n",
       "                       -0.1051, -0.0155, -0.0148, -0.0262, -0.0758, -0.0726,  0.0177, -0.0030,\n",
       "                        0.0759, -0.0398, -0.0199, -0.0029, -0.0684, -0.0697, -0.0447, -0.0962,\n",
       "                       -0.0449,  0.0926,  0.1217, -0.0514, -0.0910,  0.0708, -0.1379,  0.1368,\n",
       "                       -0.0300,  0.0986,  0.0235,  0.0063,  0.1729, -0.0831, -0.0621, -0.0219,\n",
       "                       -0.0996, -0.0459, -0.0016, -0.0412,  0.1016,  0.0593,  0.0016,  0.0874,\n",
       "                       -0.0333, -0.1326, -0.0475,  0.0077,  0.0722, -0.1126, -0.1037, -0.0643])),\n",
       "              ('decoders.1.multihead_attention.w_0.weight',\n",
       "               tensor([[ 0.0058,  0.0979,  0.0599,  ..., -0.0057, -0.0689, -0.0360],\n",
       "                       [-0.1150,  0.0015, -0.0448,  ...,  0.0029, -0.1313, -0.1184],\n",
       "                       [-0.0645, -0.1342,  0.1810,  ..., -0.0179,  0.0554,  0.0979],\n",
       "                       ...,\n",
       "                       [-0.0198,  0.0274, -0.1909,  ..., -0.1283, -0.0608, -0.1287],\n",
       "                       [-0.1707,  0.0349, -0.1235,  ...,  0.0807, -0.0640,  0.0546],\n",
       "                       [ 0.0153,  0.1117,  0.1451,  ..., -0.0177, -0.1340, -0.1097]])),\n",
       "              ('decoders.1.multihead_attention.w_0.bias',\n",
       "               tensor([ 0.0210,  0.0383, -0.0820,  0.0218, -0.1082, -0.0540,  0.0452,  0.0228,\n",
       "                        0.0017,  0.0025,  0.0837, -0.0686,  0.0074, -0.0473,  0.0613, -0.1354,\n",
       "                        0.0726, -0.0534,  0.0063, -0.0329,  0.0798, -0.0462, -0.0915,  0.0683,\n",
       "                        0.0714,  0.0222,  0.1176, -0.0823,  0.0235, -0.0906, -0.0518,  0.0039,\n",
       "                        0.0171,  0.0598,  0.0717, -0.0393,  0.0330,  0.0622,  0.0530,  0.0039,\n",
       "                        0.0385,  0.0543,  0.0914,  0.0464, -0.0133,  0.0457,  0.1102, -0.0868,\n",
       "                        0.1346, -0.0620,  0.0630, -0.0303,  0.0011, -0.0812,  0.0880,  0.0006,\n",
       "                       -0.0898, -0.0958, -0.1012,  0.0278,  0.0703, -0.1267,  0.0726,  0.0034])),\n",
       "              ('decoders.1.feedforward_network.fc1.weight',\n",
       "               tensor([[ 0.0440,  0.1572,  0.1611,  ...,  0.0371,  0.1059, -0.0745],\n",
       "                       [-0.0458,  0.0599,  0.1009,  ..., -0.0106,  0.1600,  0.1798],\n",
       "                       [ 0.0668,  0.0089,  0.0749,  ...,  0.0272,  0.1038,  0.0202],\n",
       "                       ...,\n",
       "                       [-0.0657, -0.0094, -0.0545,  ...,  0.0920,  0.0072,  0.0898],\n",
       "                       [-0.1430, -0.0994,  0.1264,  ..., -0.0239, -0.0526, -0.2461],\n",
       "                       [ 0.0447, -0.0026, -0.0585,  ..., -0.1292,  0.0612,  0.0262]])),\n",
       "              ('decoders.1.feedforward_network.fc1.bias',\n",
       "               tensor([-4.5364e-02,  2.5782e-03,  1.1125e-01, -9.6116e-02, -6.3876e-02,\n",
       "                       -3.7701e-02,  1.0443e-01, -1.4107e-01, -3.7244e-02, -9.5766e-02,\n",
       "                       -1.7080e-01, -1.0659e-01, -7.1065e-02, -2.4906e-03, -9.1293e-02,\n",
       "                        5.7340e-03, -1.2981e-02, -2.8815e-02, -4.5560e-02,  6.8371e-02,\n",
       "                        6.7500e-02, -1.1377e-01, -8.3908e-02,  2.3198e-02, -2.4090e-02,\n",
       "                        1.4611e-02, -6.7614e-02, -1.2840e-01,  7.0591e-02,  8.3990e-02,\n",
       "                       -1.0304e-01, -6.4157e-02, -1.5162e-01, -3.8307e-02, -2.0085e-02,\n",
       "                        5.8677e-02, -6.0708e-02, -5.2421e-02, -1.1932e-02, -2.6409e-02,\n",
       "                       -9.3861e-02,  6.1624e-03,  2.8166e-02,  7.7961e-02,  1.0514e-01,\n",
       "                       -9.8061e-02,  5.6473e-02, -5.4728e-02,  1.5142e-01,  4.8861e-02,\n",
       "                        1.8521e-02, -7.9549e-02,  9.3114e-03,  1.5612e-03, -2.2482e-03,\n",
       "                       -3.0072e-03, -5.1450e-02,  6.6693e-02,  4.9759e-02, -1.0530e-01,\n",
       "                        6.4035e-02, -1.6155e-02,  9.2970e-02, -2.9796e-02,  4.4490e-02,\n",
       "                       -1.0756e-01,  2.6302e-02, -7.1497e-02,  1.2368e-01, -1.6374e-01,\n",
       "                        1.1294e-01, -1.2443e-01,  1.5022e-01, -6.9578e-02,  8.3833e-03,\n",
       "                       -1.1121e-01, -1.0840e-02,  1.4245e-01, -5.8337e-02,  3.3210e-02,\n",
       "                       -1.2408e-01,  3.7647e-02, -3.6515e-03, -8.5335e-02,  2.7989e-02,\n",
       "                       -1.7694e-01, -8.0265e-03, -5.5539e-02,  5.2680e-02,  6.4598e-02,\n",
       "                       -1.0870e-01,  9.7865e-02, -7.1002e-03,  1.1148e-01, -6.2864e-02,\n",
       "                        1.5019e-01,  1.2894e-01,  2.4251e-02,  2.6354e-02, -1.6413e-01,\n",
       "                        8.2085e-02, -1.3862e-01,  1.0962e-01, -1.4392e-01, -1.2625e-01,\n",
       "                       -2.6836e-03, -9.2659e-02, -1.3911e-01, -3.7938e-02, -5.5083e-02,\n",
       "                        3.3661e-02, -1.4033e-01,  4.9898e-03, -6.0987e-02, -9.9925e-02,\n",
       "                       -9.0458e-02,  8.5762e-02,  1.6832e-01,  1.0287e-01, -1.0051e-01,\n",
       "                       -2.6165e-02, -5.9228e-02, -6.6228e-02,  1.8613e-02,  2.3184e-02,\n",
       "                       -6.2378e-02, -6.1685e-02,  7.0979e-02,  3.6879e-02, -3.3140e-03,\n",
       "                        3.8616e-02, -2.5666e-02,  7.7339e-03, -3.2116e-02,  4.3693e-02,\n",
       "                        9.7444e-02,  1.6226e-01,  1.2726e-02, -3.6590e-02, -1.4686e-01,\n",
       "                       -3.1791e-02,  6.6119e-02, -2.2792e-03, -4.3551e-02, -3.1137e-03,\n",
       "                       -4.3463e-02, -1.5680e-02,  3.0658e-02, -1.7218e-01,  6.9492e-02,\n",
       "                        8.8302e-02,  1.0782e-01, -7.8949e-02, -8.2316e-02, -2.9515e-02,\n",
       "                        1.7622e-04,  5.9369e-02, -1.2371e-01,  1.2418e-02,  1.2130e-01,\n",
       "                        1.1329e-01, -7.8896e-03, -1.0250e-01,  2.2990e-02, -2.1445e-02,\n",
       "                       -1.8978e-01, -1.3285e-01, -1.3743e-03,  2.5382e-02, -5.0518e-02,\n",
       "                       -1.2863e-01, -4.0873e-03,  3.5300e-02,  5.9610e-02, -1.7993e-01,\n",
       "                        1.4461e-02, -1.1073e-01, -1.6655e-01, -3.4187e-03,  7.3626e-02,\n",
       "                       -1.4002e-01,  5.3026e-03, -1.4244e-03, -9.9623e-02, -2.3715e-02,\n",
       "                       -8.2813e-02, -4.1928e-02, -8.5669e-02,  1.7837e-01, -8.1116e-02,\n",
       "                       -2.0517e-03,  4.5719e-02,  7.2254e-02,  4.6958e-03, -1.2457e-01,\n",
       "                        1.1212e-01,  4.1297e-02,  5.1978e-02, -1.2084e-01, -1.3244e-01,\n",
       "                        3.3139e-02, -1.1163e-01, -1.9606e-02, -1.1924e-01, -6.3510e-03,\n",
       "                        1.1757e-01, -1.1701e-01,  9.4653e-03, -7.8803e-02,  2.2541e-02,\n",
       "                       -4.2892e-02, -5.1671e-02,  4.2014e-02,  5.5139e-02,  3.5706e-02,\n",
       "                        8.0196e-02,  4.7799e-03, -1.3823e-01, -4.0363e-02, -4.5386e-02,\n",
       "                        4.1759e-02, -1.0007e-01,  1.4051e-01, -3.8996e-02,  8.0924e-02,\n",
       "                       -1.1588e-01, -1.6245e-02,  7.4492e-02,  2.6132e-02,  4.8038e-02,\n",
       "                        2.1731e-02, -7.2918e-02, -7.8475e-02,  3.5006e-02,  6.3366e-02,\n",
       "                       -2.5986e-02,  1.2650e-01,  4.9178e-02, -3.6763e-03,  7.5441e-02,\n",
       "                        4.6037e-02, -3.7233e-03,  1.0257e-01, -3.1544e-02,  9.6585e-02,\n",
       "                       -5.8964e-02, -2.0253e-02, -3.1041e-02, -1.4421e-02, -3.5666e-02,\n",
       "                       -1.1328e-02,  3.9038e-02, -1.1189e-01,  3.5505e-02,  8.5611e-02,\n",
       "                        6.2669e-02])),\n",
       "              ('decoders.1.feedforward_network.fc2.weight',\n",
       "               tensor([[ 0.0801, -0.0191,  0.0103,  ..., -0.0282,  0.1423, -0.1176],\n",
       "                       [-0.0100,  0.0417, -0.0560,  ..., -0.0455, -0.0793,  0.1218],\n",
       "                       [ 0.0836, -0.0339,  0.0236,  ..., -0.1318,  0.1178, -0.0723],\n",
       "                       ...,\n",
       "                       [-0.1165, -0.0898, -0.0331,  ..., -0.0362,  0.0724,  0.0169],\n",
       "                       [ 0.0099,  0.0238, -0.0200,  ...,  0.0184,  0.1363,  0.0575],\n",
       "                       [-0.0474,  0.0585, -0.1073,  ..., -0.0734,  0.0368, -0.0436]])),\n",
       "              ('decoders.1.feedforward_network.fc2.bias',\n",
       "               tensor([ 0.0474, -0.0478, -0.0320, -0.0538, -0.0127,  0.0129, -0.0028,  0.0390,\n",
       "                        0.0045, -0.0918, -0.0802, -0.0543,  0.0187, -0.0466,  0.0047, -0.0512,\n",
       "                       -0.0375, -0.0671,  0.0136, -0.0086,  0.0140, -0.0728,  0.0762,  0.0518,\n",
       "                        0.0273, -0.0459,  0.0392, -0.0496, -0.0249, -0.0483,  0.0119,  0.0539,\n",
       "                        0.0004, -0.0653,  0.0447, -0.0694, -0.0615, -0.0376,  0.0107, -0.0436,\n",
       "                        0.0392, -0.0360, -0.0494,  0.0129,  0.0476, -0.0292,  0.0120, -0.0522,\n",
       "                        0.0045, -0.0092,  0.0164, -0.0260,  0.0605, -0.0298, -0.0364, -0.0056,\n",
       "                       -0.0382, -0.0386, -0.0529, -0.0612,  0.0310, -0.0654,  0.0522,  0.0338])),\n",
       "              ('decoders.1.layernorma.weight',\n",
       "               tensor([0.9972, 0.9933, 1.1337, 1.0700, 1.1072, 1.0857, 1.0239, 1.0610, 1.1228,\n",
       "                       0.9814, 1.0597, 1.0420, 1.0525, 1.0364, 1.0751, 0.9551, 1.0763, 0.9851,\n",
       "                       1.1162, 1.0206, 1.0386, 1.0209, 1.0516, 0.9699, 0.9549, 1.0480, 1.0524,\n",
       "                       1.0343, 1.0729, 1.1397, 0.9858, 1.0501, 0.9264, 1.0264, 1.0557, 1.0638,\n",
       "                       0.9603, 1.0310, 0.9627, 1.0684, 1.0891, 1.0116, 1.0575, 0.9694, 1.0558,\n",
       "                       0.9965, 1.0907, 1.0287, 1.0216, 1.1063, 0.9695, 1.0642, 0.9887, 1.0148,\n",
       "                       1.0460, 1.0410, 1.0854, 0.9947, 1.0487, 0.9793, 0.9942, 1.0852, 0.9409,\n",
       "                       0.9717])),\n",
       "              ('decoders.1.layernorma.bias',\n",
       "               tensor([-0.0062, -0.0215, -0.0018,  0.0039,  0.0695, -0.0157,  0.0702, -0.0509,\n",
       "                        0.0696, -0.0763,  0.1168, -0.0749,  0.0304, -0.0270,  0.1042, -0.0466,\n",
       "                        0.0067, -0.0622,  0.1307, -0.0375, -0.0267, -0.0554,  0.0662, -0.0949,\n",
       "                        0.0493,  0.0295,  0.0591, -0.0586, -0.0144, -0.0125,  0.0627, -0.0596,\n",
       "                        0.1122, -0.0707,  0.0240, -0.0581,  0.0823, -0.0282,  0.0786, -0.0878,\n",
       "                        0.0127, -0.0433,  0.0139, -0.0669,  0.0421, -0.0781,  0.0224, -0.0211,\n",
       "                        0.0148, -0.1011,  0.0203, -0.0446,  0.0337, -0.0396,  0.0959, -0.0771,\n",
       "                        0.0114, -0.0091,  0.0546, -0.0722,  0.0578, -0.0484,  0.0984,  0.0471])),\n",
       "              ('embedding.weight',\n",
       "               tensor([[-1.4293e-01,  1.4717e+00, -6.4108e-01,  ..., -1.2048e+00,\n",
       "                        -4.2571e-01, -2.1593e+00],\n",
       "                       [-2.7101e-01, -3.6378e-01,  4.4107e-01,  ...,  8.1888e-01,\n",
       "                         1.2321e+00, -4.5423e-01],\n",
       "                       [ 1.4330e+00,  1.7807e-03, -1.0975e+00,  ..., -2.7778e+00,\n",
       "                        -9.7703e-01, -1.2997e+00],\n",
       "                       ...,\n",
       "                       [ 1.1216e+00, -5.4477e-01, -2.7156e+00,  ..., -5.9532e-01,\n",
       "                        -5.9676e-01,  1.1456e+00],\n",
       "                       [-1.2783e+00, -1.6117e-01, -6.3156e-01,  ...,  7.5767e-01,\n",
       "                        -3.0235e-01, -7.4745e-01],\n",
       "                       [ 1.5841e+00, -5.6528e-01, -5.6448e-01,  ..., -6.7372e-01,\n",
       "                        -5.8751e-01, -1.6768e+00]])),\n",
       "              ('finalfc.weight',\n",
       "               tensor([[-0.0917,  0.1929, -0.1559,  ...,  0.1168,  0.0904, -0.0728],\n",
       "                       [ 0.0016,  0.0417,  0.0406,  ...,  0.1879,  0.1451, -0.1062],\n",
       "                       [ 0.0623, -0.1438,  0.1079,  ..., -0.1074, -0.0910, -0.1066],\n",
       "                       ...,\n",
       "                       [-0.0260,  0.0675, -0.1516,  ...,  0.0936, -0.0308,  0.0190],\n",
       "                       [-0.1884, -0.1762, -0.0898,  ...,  0.0553, -0.0648, -0.0046],\n",
       "                       [ 0.0728,  0.0479, -0.0510,  ...,  0.0192, -0.0695,  0.0660]])),\n",
       "              ('finalfc.bias',\n",
       "               tensor([-0.1037,  0.0789, -0.0967,  0.0358,  0.0592, -0.0687, -0.0084,  0.1651,\n",
       "                        0.0334,  0.0651, -0.0324,  0.0233, -0.0802,  0.0781, -0.1007,  0.0465,\n",
       "                        0.0604, -0.0317,  0.0491,  0.0172,  0.0327,  0.0109,  0.0278, -0.0149,\n",
       "                        0.0803,  0.0150,  0.0799, -0.0728,  0.0417,  0.0525,  0.0283,  0.0187,\n",
       "                        0.0410, -0.1346,  0.0987, -0.0053, -0.0334]))])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bidouilles pour adapter nos fonctions aux fonctions common codées par Nathra \n",
    "#(sequence list of ints en entree, list of probas en sortie)\n",
    "#(Faire mieux plus tard)\n",
    "def LMtransformerprediction(listints):\n",
    "    return np.exp(LMtransformer(torch.tensor([listints[-max_length:]]))[0][-1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LMtransformerpredictionTEST(listints):\n",
    "    return np.exp(LMtransformerTEST(torch.tensor([listints[-max_length:]]))[0][-1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_seq(prev_seq, top_k=5):\n",
    "    with torch.no_grad():\n",
    "        prev_seq_numbers = [vocab_numbers[token] for token in prev_seq]\n",
    "        sample_token_seq = sample_token_sequence(LMtransformerprediction, prev_seq_numbers, top_k=top_k)\n",
    "        tokens_pred = [vocab_numeroted[i] for i in sample_token_seq]\n",
    "        print(' '.join(tokens_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_seqTEST(prev_seq, top_k=5):\n",
    "    with torch.no_grad():\n",
    "        prev_seq_numbers = [vocab_numbers[token] for token in prev_seq]\n",
    "        sample_token_seq = sample_token_sequence(LMtransformerpredictionTEST, prev_seq_numbers, top_k=top_k)\n",
    "        tokens_pred = [vocab_numeroted[i] for i in sample_token_seq]\n",
    "        print(' '.join(tokens_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def gen_seq_maison(prev_seq):\\n    with torch.no_grad():\\n        prev_seq_numbers = [vocab_numbers[token] for token in prev_seq]\\n        indice = np.argmax(np.array(LMtransformer(torch.tensor([prev_seq_numbers]))))\\n        tokens_pred = vocab_numeroted[indice]\\n        print(' '.join(tokens_pred))\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def gen_seq_maison(prev_seq):\n",
    "    with torch.no_grad():\n",
    "        prev_seq_numbers = [vocab_numbers[token] for token in prev_seq]\n",
    "        indice = np.argmax(np.array(LMtransformer(torch.tensor([prev_seq_numbers]))))\n",
    "        tokens_pred = vocab_numeroted[indice]\n",
    "        print(' '.join(tokens_pred))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/100 [00:00<?, ?it/s]C:\\Users\\Eric\\statapp_language_model\\statapp\\transformer\\pytorch\\transformer_model.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(torch.add(embedded, pos_encodings), dtype=torch.float32)\n",
      "100%|████████████████████████████████████████| 100/100 [00:01<00:00, 70.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' prophetique barcelone lettres meditation persuade de dieu nombres d esprit du connaissance touche atteint il a . la prophetique est age etat alors age inspiration . ans atteint l vrai alors ans atteint des de avoir apres . et 31 . d esprit , la ans atteint par vrai obtenu il ' prophetique barcelone lettres meditation persuade de dieu nombres d esprit du connaissance touche atteint il a . la prophetique est age etat alors age inspiration . ans atteint l vrai alors ans atteint des de avoir apres . et 31 . d esprit , la ans atteint\n"
     ]
    }
   ],
   "source": [
    "gen_seq(['il'], top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [00:01<00:00, 97.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atteint l obtenu il ' prophetique barcelone lettres meditation persuade de dieu nombres d esprit du connaissance touche atteint il a . la prophetique est age etat alors age inspiration . ans atteint l vrai alors ans atteint par vrai obtenu il ' prophetique barcelone lettres meditation persuade de dieu nombres d esprit du connaissance touche atteint l vrai obtenu il a . la prophetique est age etat alors age inspiration . ans atteint l vrai alors ans atteint l vrai obtenu il ' prophetique barcelone lettres meditation persuade de dieu nombres d esprit du connaissance touche atteint il a\n"
     ]
    }
   ],
   "source": [
    "gen_seq(['a','l','age','de','31','ans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [00:01<00:00, 95.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... en 1981 , mer . non content d ' exercer son sacerdoce , roger ducouret fut auteur de romans policiers , de contes pour enfants , brocanteur , ami d ' artistes comme pierre dac , fernand raynaud ou jacques brel ... il fut , l ' , les gens de maintenant de vitoria-gasteiz , dont l ' aeroport se met a se specialiser dans le traitement de charge aerienne et , formee par aena , la mairie de vitoria-gasteiz , dont l ' aeroport se met a se specialiser dans le traitement de charge aerienne et , formee\n"
     ]
    }
   ],
   "source": [
    "gen_seq(['<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>','<unk>'], top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [00:01<00:00, 88.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' esprit prophetique apres avoir ete marquee apres a ete marquee par la personnalite roger ducouret , cue de tusson de 1942 a 1981 . non content d ' exercer son sacerdoce , roger ducouret fut auteur de romans policiers , de contes pour favoriser cette activite , on constitue en 1994 la societe via , formee par aena , la un historien loc de contes pour aller aider leon ier de galicie . l ' histoire de la commune de tusson de 1942 a 1981 de serge gainsbourg et `` vive ma liberte `` d ' arno . il\n"
     ]
    }
   ],
   "source": [
    "gen_seq(['barcelone',',','il','est','touche','par','l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(tokens)<100:\n",
    "    print(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
