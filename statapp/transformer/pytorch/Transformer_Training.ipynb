{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer training (chantier en cours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformer_model import *\n",
    "import nltk\n",
    "import sys\n",
    "sys.path.append(\"../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statapp.common.preprocessing import load_data, encode_data, split_into_X_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing maison bien moche pour le moment... ^^ (essai avec un encodage sur les mots) Les données sont placées directement dans le dossier du notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = load_data(\"data/fr.train.top1M.txt\", sample=0.00001)\n",
    "\n",
    "tokens = nltk.word_tokenize(text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(tokens))\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "vocab_numbers = dict(zip(vocab, range(0,len(vocab))))\n",
    "tokens_numbers = np.array([vocab_numbers[tokens[i]] for i in range(len(tokens))])\n",
    "\n",
    "tokens_numbers_sequences = np.array([ tokens_numbers[i:i+max_length] for i in range(len(tokens_numbers)-max_length+1)])\n",
    "tokens_numbers_sequences = torch.tensor(tokens_numbers_sequences , dtype=torch.int64)\n",
    "\n",
    "nb_sequences =  tokens_numbers_sequences.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LMtransformer = Transformer(vocab_size, Decoder(MultiHeadAttention(nb_heads, head_size, vector_size), FeedforwardNetwork(vector_size, ffn_hidden_size)))\n",
    "\n",
    "#Correspond à utiliser l'entropie croisée puisque les sorties sont des log_softmax\n",
    "#et l'entropie croisée = nll_loss(log_softmax(.), target)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(LMtransformer.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(tokens_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 1],\n",
      "        [4, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(tokens_numbers_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(nb_epochs, batch_size):\n",
    "    \n",
    "    #What is this ?? I don't remember\n",
    "    LMtransformer.train()\n",
    "    \n",
    "    for epoch in range(nb_epochs):\n",
    "        \n",
    "        running_loss = 0\n",
    "        \n",
    "        randperm = torch.randperm(nb_sequences)\n",
    "        randperm = randperm[:(nb_sequences//batch_size)*batch_size]\n",
    "        batchs_indices = randperm.reshape(nb_sequences//batch_size, batch_size)\n",
    "        \n",
    "        for i, batch_indices in enumerate(batchs_indices):\n",
    "            \n",
    "            batch = tokens_numbers_sequences[batch_indices]\n",
    "            optimizer.zero_grad()\n",
    "            output = LMtransformer(batch)\n",
    "            print(output[:,:-1].reshape(-1, vocab_size))\n",
    "            print(batch[:,1:].flatten())\n",
    "            loss = criterion(output[:,:-1].reshape(-1, vocab_size), batch[:,1:].flatten())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Il faudrait adapter les affichages en fonction du nombre de batchs total\n",
    "            running_loss += loss.item()\n",
    "            #if i % 3 == 3:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 1))\n",
    "            running_loss = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test d'overfitting sur un cas ultrasimplifié ;\n",
    "- En observant les sorties le modèle a bien appris et overfitte, mais la loss affichée ne descend pas jusque zéro !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric\\statapp_language_model\\statapp\\transformer\\pytorch\\transformer_model.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(torch.add(embedded, pos_encodings), dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5251, -1.6545, -1.0780, -1.8589, -2.3528],\n",
      "        [-1.8121, -1.7912, -0.8091, -1.6637, -3.3451],\n",
      "        [-1.5907, -2.7622, -0.8608, -1.2930, -3.3299],\n",
      "        [-1.6051, -1.7638, -1.0290, -1.8070, -2.2420]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([4, 1, 3, 4])\n",
      "[1,     1] loss: 1.920\n",
      "tensor([[-2.7232, -1.9551, -2.8479, -2.9713, -0.3804],\n",
      "        [-2.8264, -0.4072, -2.4277, -2.5519, -2.2154],\n",
      "        [-2.6881, -0.4726, -2.2460, -2.1995, -2.3868],\n",
      "        [-3.3086, -0.9947, -1.3371, -2.3686, -1.4382]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([4, 1, 1, 0])\n",
      "[2,     1] loss: 1.142\n",
      "tensor([[-3.9297, -0.0769, -4.9547, -4.3922, -3.3535],\n",
      "        [-1.7676, -1.2095, -2.3108, -2.4875, -1.0538],\n",
      "        [-4.0554, -3.3070, -5.5248, -5.1662, -0.0658],\n",
      "        [-3.9617, -0.1001, -5.0701, -4.7525, -2.7911]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([1, 0, 4, 1])\n",
      "[3,     1] loss: 0.503\n",
      "tensor([[-4.7529, -0.0186, -7.4574, -6.4347, -4.8737],\n",
      "        [-0.3356, -2.6706, -3.8856, -2.9783, -1.9349],\n",
      "        [-4.5757, -4.6238, -7.7456, -6.9426, -0.0217],\n",
      "        [-4.6468, -0.0316, -7.4524, -6.7563, -3.9256]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([1, 0, 4, 1])\n",
      "[4,     1] loss: 0.102\n",
      "tensor([[-4.5379, -5.6376, -9.4449, -8.2950, -0.0147],\n",
      "        [-5.2297, -0.0101, -9.6605, -8.6943, -5.4030],\n",
      "        [-3.5347, -4.8375, -6.9462, -0.0637, -3.7436],\n",
      "        [-4.0113, -5.0935, -8.4791, -6.5107, -0.0263]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([4, 1, 3, 4])\n",
      "[5,     1] loss: 0.029\n",
      "tensor([[-4.2748e+00, -5.7595e+00, -8.3151e+00, -2.5972e-02, -4.7884e+00],\n",
      "        [-4.2093e+00, -5.9395e+00, -9.8639e+00, -7.5332e+00, -1.8243e-02],\n",
      "        [-6.0312e+00, -2.7415e-03, -1.1687e+01, -1.0001e+01, -8.1767e+00],\n",
      "        [-2.6762e-03, -8.2015e+00, -9.7289e+00, -6.4567e+00, -7.1708e+00]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([3, 4, 1, 0])\n",
      "[6,     1] loss: 0.012\n",
      "tensor([[-6.6125e+00, -1.4248e-03, -1.3391e+01, -1.1478e+01, -9.5889e+00],\n",
      "        [-5.0329e-04, -1.0597e+01, -1.2232e+01, -7.8348e+00, -9.4645e+00],\n",
      "        [-5.3757e+00, -6.9468e+00, -9.7798e+00, -7.8050e-03, -6.1523e+00],\n",
      "        [-4.6294e+00, -6.8476e+00, -1.1131e+01, -8.5203e+00, -1.1098e-02]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([1, 0, 3, 4])\n",
      "[7,     1] loss: 0.005\n",
      "tensor([[-7.1781e+00, -7.8635e-04, -1.4883e+01, -1.2791e+01, -1.0829e+01],\n",
      "        [-1.4805e-04, -1.2663e+01, -1.4359e+01, -8.9220e+00, -1.1431e+01],\n",
      "        [-5.4233e+00, -8.8278e+00, -1.3595e+01, -1.1817e+01, -4.5782e-03],\n",
      "        [-7.1486e+00, -8.9784e-04, -1.4792e+01, -1.3470e+01, -9.1182e+00]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([1, 0, 4, 1])\n",
      "[8,     1] loss: 0.002\n",
      "tensor([[-7.9145e+00, -9.3788e+00, -1.2605e+01, -5.7264e-04, -9.0345e+00],\n",
      "        [-6.1117e+00, -8.7645e+00, -1.3451e+01, -1.0515e+01, -2.4045e-03],\n",
      "        [-7.7648e+00, -4.3192e-04, -1.6223e+01, -1.4009e+01, -1.1950e+01],\n",
      "        [-5.7815e-05, -1.4425e+01, -1.6163e+01, -9.8076e+00, -1.3126e+01]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([3, 4, 1, 0])\n",
      "[9,     1] loss: 0.001\n",
      "tensor([[-9.1944e+00, -1.0517e+01, -1.3898e+01, -1.5961e-04, -1.0414e+01],\n",
      "        [-7.0338e+00, -9.7068e+00, -1.4493e+01, -1.1490e+01, -9.5370e-04],\n",
      "        [-6.6819e+00, -1.0855e+01, -1.5886e+01, -1.4034e+01, -1.2745e-03],\n",
      "        [-8.5446e+00, -2.1169e-04, -1.7390e+01, -1.6094e+01, -1.0985e+01]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([3, 4, 4, 1])\n",
      "[10,     1] loss: 0.001\n",
      "tensor([[-1.0428e+01, -1.1591e+01, -1.5110e+01, -4.7206e-05, -1.1728e+01],\n",
      "        [-8.0407e+00, -1.0641e+01, -1.5478e+01, -1.2447e+01, -3.5018e-04],\n",
      "        [-7.4425e+00, -1.1796e+01, -1.6917e+01, -1.5111e+01, -5.9384e-04],\n",
      "        [-9.2151e+00, -1.0716e-04, -1.8493e+01, -1.7246e+01, -1.1779e+01]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([3, 4, 4, 1])\n",
      "[11,     1] loss: 0.000\n",
      "tensor([[-9.3722e+00, -8.5589e-05, -1.9433e+01, -1.7033e+01, -1.4590e+01],\n",
      "        [-9.1791e-06, -1.8426e+01, -2.0176e+01, -1.1593e+01, -1.6963e+01],\n",
      "        [-8.2610e+00, -1.2696e+01, -1.7885e+01, -1.6159e+01, -2.6163e-04],\n",
      "        [-9.8520e+00, -5.6384e-05, -1.9494e+01, -1.8297e+01, -1.2517e+01]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([1, 0, 4, 1])\n",
      "[12,     1] loss: 0.000\n",
      "tensor([[-1.2764e+01, -1.3536e+01, -1.7285e+01, -4.8876e-06, -1.4132e+01],\n",
      "        [-1.0116e+01, -1.2400e+01, -1.7225e+01, -1.4181e+01, -4.5299e-05],\n",
      "        [-9.8536e+00, -5.2808e-05, -2.0295e+01, -1.7869e+01, -1.5299e+01],\n",
      "        [-6.1989e-06, -1.9430e+01, -2.1162e+01, -1.1987e+01, -1.7921e+01]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([3, 4, 1, 0])\n",
      "[13,     1] loss: 0.000\n",
      "tensor([[-1.3847e+01, -1.4406e+01, -1.8259e+01, -1.7881e-06, -1.5229e+01],\n",
      "        [-1.1131e+01, -1.3216e+01, -1.8002e+01, -1.4955e+01, -1.6808e-05],\n",
      "        [-1.0303e+01, -3.3616e-05, -2.1070e+01, -1.8625e+01, -1.5942e+01],\n",
      "        [-4.5299e-06, -2.0312e+01, -2.2019e+01, -1.2304e+01, -1.8769e+01]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([3, 4, 1, 0])\n",
      "[14,     1] loss: 0.000\n",
      "tensor([[-1.4866e+01, -1.5214e+01, -1.9163e+01, -7.1526e-07, -1.6258e+01],\n",
      "        [-1.2075e+01, -1.3974e+01, -1.8724e+01, -1.5662e+01, -6.6757e-06],\n",
      "        [-1.0726e+01, -1.5081e+01, -2.0402e+01, -1.9038e+01, -2.2292e-05],\n",
      "        [-1.1485e+01, -1.0729e-05, -2.1952e+01, -2.0871e+01, -1.4486e+01]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([3, 4, 4, 1])\n",
      "[15,     1] loss: 0.000\n",
      "tensor([[-1.5819e+01, -1.5957e+01, -2.0001e+01, -2.3842e-07, -1.7215e+01],\n",
      "        [-1.2968e+01, -1.4680e+01, -1.9388e+01, -1.6286e+01, -2.8610e-06],\n",
      "        [-1.1498e+01, -1.5762e+01, -2.1123e+01, -1.9895e+01, -1.0252e-05],\n",
      "        [-1.1937e+01, -6.7949e-06, -2.2606e+01, -2.1554e+01, -1.5059e+01]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([3, 4, 4, 1])\n",
      "[16,     1] loss: 0.000\n",
      "tensor([[-1.6705e+01, -1.6641e+01, -2.0777e+01, -1.1921e-07, -1.8104e+01],\n",
      "        [-1.3820e+01, -1.5344e+01, -1.9999e+01, -1.6850e+01, -1.3113e-06],\n",
      "        [-1.1468e+01, -1.0490e-05, -2.2959e+01, -2.0506e+01, -1.7500e+01],\n",
      "        [-2.2650e-06, -2.2402e+01, -2.4043e+01, -1.2996e+01, -2.0803e+01]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([3, 4, 1, 0])\n",
      "[17,     1] loss: 0.000\n",
      "tensor([[-1.2953e+01, -1.6979e+01, -2.2413e+01, -2.1463e+01, -2.3842e-06],\n",
      "        [-1.2731e+01, -3.0994e-06, -2.3726e+01, -2.2707e+01, -1.6091e+01],\n",
      "        [-1.7532e+01, -1.7276e+01, -2.1496e+01,  0.0000e+00, -1.8932e+01],\n",
      "        [-1.4620e+01, -1.5964e+01, -2.0559e+01, -1.7360e+01, -5.9605e-07]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([4, 1, 3, 4])\n",
      "[18,     1] loss: 0.000\n",
      "tensor([[-1.2155e+01, -5.2452e-06, -2.3929e+01, -2.1515e+01, -1.8274e+01],\n",
      "        [-1.6689e-06, -2.3445e+01, -2.5042e+01, -1.3297e+01, -2.1819e+01],\n",
      "        [-1.3627e+01, -1.7525e+01, -2.2987e+01, -2.2171e+01, -1.1921e-06],\n",
      "        [-1.3062e+01, -2.2650e-06, -2.4200e+01, -2.3186e+01, -1.6546e+01]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([1, 0, 4, 1])\n",
      "[19,     1] loss: 0.000\n",
      "tensor([[-1.2471e+01, -3.8147e-06, -2.4347e+01, -2.1961e+01, -1.8603e+01],\n",
      "        [-1.5497e-06, -2.3883e+01, -2.5457e+01, -1.3413e+01, -2.2244e+01],\n",
      "        [-1.9018e+01, -1.8404e+01, -2.2772e+01,  0.0000e+00, -2.0417e+01],\n",
      "        [-1.6070e+01, -1.7078e+01, -2.1538e+01, -1.8224e+01, -1.1921e-07]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([1, 0, 3, 4])\n",
      "[20,     1] loss: 0.000\n",
      "tensor([[-1.4868e+01, -1.8495e+01, -2.4011e+01, -2.3449e+01, -3.5763e-07],\n",
      "        [-1.3635e+01, -1.1921e-06, -2.5008e+01, -2.4000e+01, -1.7344e+01],\n",
      "        [-1.9683e+01, -1.8903e+01, -2.3335e+01,  0.0000e+00, -2.1077e+01],\n",
      "        [-1.6726e+01, -1.7575e+01, -2.1965e+01, -1.8595e+01, -1.1921e-07]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([4, 1, 3, 4])\n",
      "[21,     1] loss: 0.000\n",
      "tensor([[-1.3046e+01, -2.1458e-06, -2.5076e+01, -2.2749e+01, -1.9167e+01],\n",
      "        [-1.1921e-06, -2.4621e+01, -2.6157e+01, -1.3593e+01, -2.2959e+01],\n",
      "        [-1.5426e+01, -1.8923e+01, -2.4464e+01, -2.4018e+01, -2.3842e-07],\n",
      "        [-1.3879e+01, -9.5367e-07, -2.5350e+01, -2.4338e+01, -1.7692e+01]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([1, 0, 4, 1])\n",
      "[22,     1] loss: 0.000\n",
      "tensor([[-1.5950e+01, -1.9317e+01, -2.4881e+01, -2.4546e+01, -1.1921e-07],\n",
      "        [-1.4103e+01, -7.1526e-07, -2.5659e+01, -2.4641e+01, -1.8010e+01],\n",
      "        [-1.3312e+01, -1.6689e-06, -2.5397e+01, -2.3101e+01, -1.9412e+01],\n",
      "        [-1.1921e-06, -2.4933e+01, -2.6451e+01, -1.3664e+01, -2.3261e+01]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([4, 1, 1, 0])\n",
      "[23,     1] loss: 0.000\n",
      "tensor([[-1.3562e+01, -1.3113e-06, -2.5690e+01, -2.3426e+01, -1.9633e+01],\n",
      "        [-1.0729e-06, -2.5212e+01, -2.6714e+01, -1.3725e+01, -2.3532e+01],\n",
      "        [-1.6440e+01, -1.9679e+01, -2.5266e+01, -2.5035e+01, -1.1921e-07],\n",
      "        [-1.4310e+01, -5.9605e-07, -2.5938e+01, -2.4917e+01, -1.8300e+01]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([1, 0, 4, 1])\n",
      "[24,     1] loss: 0.000\n",
      "tensor([[-1.6897e+01, -2.0011e+01, -2.5620e+01, -2.5490e+01,  0.0000e+00],\n",
      "        [-1.4501e+01, -4.7684e-07, -2.6191e+01, -2.5169e+01, -1.8565e+01],\n",
      "        [-1.3797e+01, -1.0729e-06, -2.5957e+01, -2.3727e+01, -1.9833e+01],\n",
      "        [-1.0729e-06, -2.5463e+01, -2.6950e+01, -1.3777e+01, -2.3776e+01]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([4, 1, 1, 0])\n",
      "[25,     1] loss: 0.000\n",
      "tensor([[-1.7324e+01, -2.0316e+01, -2.5946e+01, -2.5911e+01,  0.0000e+00],\n",
      "        [-1.4679e+01, -4.7684e-07, -2.6420e+01, -2.5399e+01, -1.8805e+01],\n",
      "        [-1.4017e+01, -8.3446e-07, -2.6201e+01, -2.4005e+01, -2.0014e+01],\n",
      "        [-9.5367e-07, -2.5688e+01, -2.7161e+01, -1.3822e+01, -2.3995e+01]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([4, 1, 1, 0])\n",
      "[26,     1] loss: 0.000\n",
      "tensor([[-1.4224e+01, -7.1526e-07, -2.6425e+01, -2.4262e+01, -2.0178e+01],\n",
      "        [-9.5367e-07, -2.5890e+01, -2.7351e+01, -1.3862e+01, -2.4193e+01],\n",
      "        [-1.7720e+01, -2.0594e+01, -2.6245e+01, -2.6300e+01,  0.0000e+00],\n",
      "        [-1.4843e+01, -3.5763e-07, -2.6628e+01, -2.5608e+01, -1.9021e+01]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([1, 0, 4, 1])\n",
      "[27,     1] loss: 0.000\n",
      "tensor([[-1.4417e+01, -5.9605e-07, -2.6630e+01, -2.4499e+01, -2.0328e+01],\n",
      "        [-9.5367e-07, -2.6072e+01, -2.7521e+01, -1.3897e+01, -2.4371e+01],\n",
      "        [-1.8088e+01, -2.0848e+01, -2.6519e+01, -2.6661e+01,  0.0000e+00],\n",
      "        [-1.4995e+01, -3.5763e-07, -2.6818e+01, -2.5799e+01, -1.9218e+01]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([1, 0, 4, 1])\n",
      "[28,     1] loss: 0.000\n",
      "tensor([[-1.8429e+01, -2.1080e+01, -2.6770e+01, -2.6994e+01,  0.0000e+00],\n",
      "        [-1.5135e+01, -2.3842e-07, -2.6990e+01, -2.5972e+01, -1.9397e+01],\n",
      "        [-2.3438e+01, -2.1660e+01, -2.6471e+01,  0.0000e+00, -2.4812e+01],\n",
      "        [-2.0445e+01, -2.0332e+01, -2.4247e+01, -2.0469e+01,  0.0000e+00]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([4, 1, 3, 4])\n",
      "[29,     1] loss: 0.000\n",
      "tensor([[-1.4765e+01, -3.5763e-07, -2.6989e+01, -2.4921e+01, -2.0588e+01],\n",
      "        [-8.3446e-07, -2.6383e+01, -2.7814e+01, -1.3955e+01, -2.4679e+01],\n",
      "        [-2.3752e+01, -2.1886e+01, -2.6731e+01,  0.0000e+00, -2.5126e+01],\n",
      "        [-2.0752e+01, -2.0548e+01, -2.4425e+01, -2.0605e+01,  0.0000e+00]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "tensor([1, 0, 3, 4])\n",
      "[30,     1] loss: 0.000\n"
     ]
    }
   ],
   "source": [
    "train_model(30,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
