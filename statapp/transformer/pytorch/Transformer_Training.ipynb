{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (tf_model.py, line 260)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\Eric\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3326\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[0;32m\"<ipython-input-4-f85c26b0520a>\"\u001b[0m, line \u001b[0;32m10\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    from statapp.common.preprocessing import load_all_data, encode_data, split_into_X_y\n",
      "  File \u001b[0;32m\"../../..\\statapp\\__init__.py\"\u001b[0m, line \u001b[0;32m1\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    import statapp.transformer\n",
      "  File \u001b[0;32m\"../../..\\statapp\\transformer\\__init__.py\"\u001b[0m, line \u001b[0;32m1\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    import statapp.transformer.tensorflow\n",
      "\u001b[1;36m  File \u001b[1;32m\"../../..\\statapp\\transformer\\tensorflow\\__init__.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    from statapp.transformer.tensorflow import tf_model, regr_lin_model\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"../../..\\statapp\\transformer\\tensorflow\\tf_model.py\"\u001b[1;36m, line \u001b[1;32m260\u001b[0m\n\u001b[1;33m    <<<<<<< Updated upstream\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformer_model import *\n",
    "import nltk\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"../../..\")\n",
    "\n",
    "from statapp.common.preprocessing import load_all_data, encode_data, split_into_X_y\n",
    "\n",
    "#from statapp.common.sampling import sample_token_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing maison assez brouillon pour le moment... L'encodage est effectué au niveau des mots. Les données exploitées sont placées dans le dossier data dans le dossier du notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = load_all_data(\"data/fr.train.top1M.txt\", sample=0.0000006)\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "vocab = list(set(tokens))\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "vocab_numbers = dict(zip(vocab, range(0,len(vocab))))\n",
    "vocab_numeroted = dict(zip(range(0,len(vocab)), vocab))\n",
    "tokens_numbers = np.array([vocab_numbers[tokens[i]] for i in range(len(tokens))])\n",
    "\n",
    "tokens_numbers_sequences = np.array([ tokens_numbers[i:i+max_length+1] for i in range(len(tokens_numbers)-max_length)])\n",
    "tokens_numbers_sequences = torch.tensor(tokens_numbers_sequences , dtype=torch.int64)\n",
    "\n",
    "nb_sequences =  tokens_numbers_sequences.shape[0]\n",
    "\n",
    "print(\"Les données exploitées contiennent {} tokens (mots) au total.\".format(len(tokens)))\n",
    "print(\"La taille du vocabulaire ainsi constitué est de {]\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LMtransformer = Transformer(vocab_size, Decoder(MultiHeadAttention(nb_heads, head_size, vector_size), FeedforwardNetwork(vector_size, ffn_hidden_size)))\n",
    "\n",
    "#Correspond à utiliser l'entropie croisée puisque les sorties sont des log_softmax\n",
    "#et l'entropie croisée = nll_loss(log_softmax(.), target)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(LMtransformer.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(nb_epochs, batch_size):\n",
    "    \n",
    "    #What is this ?? I don't remember. Make grad required ?\n",
    "    LMtransformer.train()\n",
    "    \n",
    "    #pas pour l'affichage progressif de la loss\n",
    "    step = max(1,((len(tokens)-max_length-1)/batch_size)//5)\n",
    "    \n",
    "    epochs_losses = []\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(nb_epochs):\n",
    "        \n",
    "        running_loss = 0\n",
    "        \n",
    "        randperm = torch.randperm(nb_sequences)\n",
    "        randperm = randperm[:(nb_sequences//batch_size)*batch_size]\n",
    "        batchs_indices = randperm.reshape(nb_sequences//batch_size, batch_size)\n",
    "        \n",
    "        for i, batch_indices in enumerate(batchs_indices):\n",
    "            \n",
    "            batch = tokens_numbers_sequences[batch_indices]\n",
    "            optimizer.zero_grad()\n",
    "            output = LMtransformer(batch[:,:-1])\n",
    "            loss = criterion(output.reshape(-1, vocab_size), batch[:,1:].flatten())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Il faudrait adapter les affichages en fonction du nombre de batchs total\n",
    "            running_loss += loss.item()\n",
    "            if i % step == step-1:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / step))\n",
    "                running_loss = 0.\n",
    "                \n",
    "                #stock pour affichage graphique\n",
    "                epochs_losses.append(epoch-1+(i/((len(tokens)-max_length-1)/batch_size)))\n",
    "                losses.append(running_loss / step)\n",
    "                \n",
    "        plt.plot(epochs_losses, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test d'overfitting sur un cas ultrasimplifié (5 tokens, longueur de séquence 1, 3 decoders, 2 heads) :\n",
    "- En observant les sorties le modèle a bien appris et overfitte ! (loss à 0 au bout de 5-6 epochs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.025\n",
      "[1,     2] loss: 0.018\n",
      "[1,     3] loss: 0.013\n",
      "[1,     4] loss: 0.103\n",
      "[1,     5] loss: 0.181\n",
      "[2,     1] loss: 0.011\n",
      "[2,     2] loss: 0.034\n",
      "[2,     3] loss: 0.051\n",
      "[2,     4] loss: 0.042\n",
      "[2,     5] loss: 0.042\n",
      "[3,     1] loss: 0.002\n",
      "[3,     2] loss: 0.061\n",
      "[3,     3] loss: 0.070\n",
      "[3,     4] loss: 0.052\n",
      "[3,     5] loss: 0.066\n",
      "[4,     1] loss: 0.027\n",
      "[4,     2] loss: 0.011\n",
      "[4,     3] loss: 0.132\n",
      "[4,     4] loss: 0.032\n",
      "[4,     5] loss: 0.054\n",
      "[5,     1] loss: 0.016\n",
      "[5,     2] loss: 0.104\n",
      "[5,     3] loss: 0.001\n",
      "[5,     4] loss: 0.018\n",
      "[5,     5] loss: 0.079\n",
      "[6,     1] loss: 0.013\n",
      "[6,     2] loss: 0.067\n",
      "[6,     3] loss: 0.108\n",
      "[6,     4] loss: 0.016\n",
      "[6,     5] loss: 0.054\n",
      "[7,     1] loss: 0.033\n",
      "[7,     2] loss: 0.010\n",
      "[7,     3] loss: 0.028\n",
      "[7,     4] loss: 0.014\n",
      "[7,     5] loss: 0.088\n",
      "[8,     1] loss: 0.057\n",
      "[8,     2] loss: 0.017\n",
      "[8,     3] loss: 0.024\n",
      "[8,     4] loss: 0.017\n",
      "[8,     5] loss: 0.067\n",
      "[9,     1] loss: 0.035\n",
      "[9,     2] loss: 0.033\n",
      "[9,     3] loss: 0.014\n",
      "[9,     4] loss: 0.105\n",
      "[9,     5] loss: 0.001\n",
      "[10,     1] loss: 0.048\n",
      "[10,     2] loss: 0.107\n",
      "[10,     3] loss: 0.024\n",
      "[10,     4] loss: 0.054\n",
      "[10,     5] loss: 0.027\n"
     ]
    }
   ],
   "source": [
    "train_model(10,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bidouilles pour adapter nos fonctions aux fonctions common codées par Nathra \n",
    "#(sequence list of ints en entree, list of probas en sortie)\n",
    "#(Faire mieux plus tard)\n",
    "def LMtransformerprediction(listints):\n",
    "    return np.exp(LMtransformer(torch.tensor([listints]))[0][-1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_seq(prev_seq, top_k=5):\n",
    "    with torch.no_grad():\n",
    "        prev_seq_numbers = [vocab_numbers[token] for token in prev_seq]\n",
    "        sample_token_seq = sample_token_sequence(LMtransformerprediction, prev_seq_numbers, top_k=top_k)\n",
    "        tokens_pred = [vocab_numeroted[i] for i in sample_token_seq]\n",
    "        print(' '.join(tokens_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_seq_maison(prev_seq):\n",
    "    with torch.no_grad():\n",
    "        prev_seq_numbers = [vocab_numbers[token] for token in prev_seq]\n",
    "        sample_token_seq = sample_token_sequence(LMtransformerprediction, prev_seq_numbers)\n",
    "        tokens_pred = [vocab_numeroted[i] for i in sample_token_seq]\n",
    "        print(' '.join(tokens_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [00:02<00:00, 47.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est touche par l ' esprit prophetique apres avoir obtenu la connaissance du vrai nom de dieu . il est alors persuade d ' esprit prophetique apres avoir obtenu la connaissance du vrai nom de dieu . il est alors persuade d ' age de 31 ans , a barcelone , il est alors persuade d ' esprit prophetique apres avoir obtenu la connaissance du vrai nom de dieu . il est alors persuade d ' esprit prophetique apres avoir obtenu la connaissance du vrai nom de dieu . il est alors persuade d ' esprit prophetique apres avoir obtenu\n"
     ]
    }
   ],
   "source": [
    "gen_seq(['il'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [00:02<00:00, 36.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", a barcelone , il est touche par l ' esprit prophetique apres avoir obtenu la connaissance du vrai nom de 31 ans , il est alors persuade d ' esprit prophetique apres avoir obtenu la connaissance du vrai nom de dieu . il est alors persuade d ' esprit prophetique apres avoir obtenu la connaissance du vrai nom de dieu . il est alors persuade d ' esprit prophetique apres avoir obtenu la connaissance du vrai nom de dieu . il est alors persuade d ' esprit prophetique apres avoir obtenu la connaissance du vrai nom de 31 ans\n"
     ]
    }
   ],
   "source": [
    "gen_seq(['a','l','age','de','31','ans'], top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 100/100 [00:00<00:00, 108.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' esprit prophetique apres avoir obtenu la connaissance du vrai nom de dieu . il est alors 31 ans , a barcelone , il est touche par l ' esprit prophetique apres avoir obtenu la connaissance du vrai nom de dieu . il est touche par l ' esprit prophetique apres avoir obtenu la connaissance du vrai nom de dieu . il est touche par l ' esprit prophetique apres avoir obtenu la connaissance du vrai nom de dieu . il est alors persuade d ' esprit prophetique apres avoir d dieu . d dieu . d dieu . il\n"
     ]
    }
   ],
   "source": [
    "gen_seq(['barcelone',',','il','est','touche','par','l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a l ' age de 31 ans , a barcelone , il est touche par l ' esprit prophetique apres avoir obtenu la connaissance du vrai nom de dieu . il est alors persuade d '\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
