Explication GPT-2 / transformer
    [N] Explication du transformer
    [C] Spécificités de GPT-2

Présentation code
    [R] Explication de la structure du code
    [M] Présentation et interprétation des démos (résultats overfittés / underfittés)

Présentation modèle proba
    [M] Bases (création de l'espace proba etc.)
    [C] Cas particuliers
    [R] Inférence

# Pour la prochaine fois
Coder un modèle de langue transformer
Paramètres: nb heads, couches, seq_length
Pour l'instant faire sur les mots et les chars. Ultimement subwords. Il veut au moins un exemple à l'échelle des mots.
Utiliser Adam

Lire les articles mis par B sur le drive, comprendre les masked language models.

But de recherche ultime: comment contraindre la génération d'un language model, par ex. lui empêcher de dire des insultes.
Notament contrôler des modèles préentraînés comme CamemBERT.

RDV: 17 au matin.
