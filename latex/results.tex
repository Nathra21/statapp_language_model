\hypertarget{expuxe9riences}{%
\section{Expériences}\label{expuxe9riences}}

Le rapport a jusqu'à présent présenté le fonctionnement théorique des
modèles de langue en général, puis de cas particuliers comme celui des
modèles n-gram et des Transformers. Cette section résume notre mise en
pratique de ces algorithmes sur un jeu de données en français extrait de
Wikipédia.

\hypertarget{jeu-de-donnuxe9es}{%
\subsection{Jeu de données}\label{jeu-de-donnuxe9es}}

Pour l'entraînement des modèles, nous avons utilisé un jeu de données
comptant 1 million de paragraphes extraits du Wikipédia français. A
titre d'exemple, voici le premier paragraphe du jeu de données :

\begin{verbatim}
a l' age de 31 ans , a barcelone , il est touche par l' esprit prophetique apres avoir obtenu la connaissance du vrai nom de dieu . il est alors persuade d' avoir atteint , par la meditation des lettres et des nombres , l' inspiration prophetique et l' etat de messie . il quitte a nouveau l' espagne afin de transmettre , fort de l' essence divine qui l' animait , ses connaissances . il redige plusieurs ouvrages prophetiques qu' il signe de noms de meme valeur numerique que son vrai nom : zacharie , raziel ...
\end{verbatim}

Pour une prise en main plus facile, le jeu de données est prétraité :
tous les caractères sont en minuscule, et les mots et les signes de
ponctuation sont séparés (en anglais, ce traitement s'appelle la
\emph{tokenization} d'un texte).

\hypertarget{algorithmes-et-hyperparamuxe8tres}{%
\subsection{Algorithmes et
hyperparamètres}\label{algorithmes-et-hyperparamuxe8tres}}

Nous avons implémenté l'algorithme \textbf{n-gram} en faisant varier n.

Nous avons également implémenté l'algorithme du Transformer avec pour
hyperparamètres :

\begin{itemize}
\tightlist
\item
  \(N = 3\)
\item
  \(d_{model} = 512\)
\item
  \(H = 16\)
\item
  \texttt{ff\_hidden\_size} \$ = 512\$
\item
  \texttt{n\_epochs} \$ = 3\$
\item
  Optimizer: Adam
\item
  \texttt{learning\_rate} \$ = 0.001 \$
\item
  \texttt{batch\_size} \$ = 128 \$
\end{itemize}

La \texttt{batch\_size} a été choisie pour saturer la mémoire de la
carte graphique utilisée.

L'entraînement a pris 6 heures sur une carte graphique GTX 1070.

\hypertarget{impluxe9mentation}{%
\subsection{Implémentation}\label{impluxe9mentation}}

Nous avons utilisé le langage Python pour mettre en place les deux
algorithmes.

Dans le cas du transformer, nous avons choisi d'utiliser les deux
\emph{frameworks} majeurs de Deep Learning en Python: \textbf{pytorch}
et \textbf{tensorflow}, afin de nous permettre d'apprendre à connaître
les deux.

\hypertarget{ruxe9sultats}{%
\subsection{Résultats TensorFlow (Subword Encoding)}\label{ruxe9sultats}}

Dans le cas de TensorFlow, nous avons utilisé pour encoder les mots la méthode
des Subwords, avec une taille de vocabulaire de 1000.

\hypertarget{n-gram}{%
\subsubsection{N-Gram}\label{n-gram}}

\hypertarget{performance-quantitative}{%
\paragraph{Performance quantitative}\label{performance-quantitative}}

Les perplexités obtenues par le modèle n-gram sont, en fonction du
paramètre n:

\begin{itemize}
\tightlist
\item
  \texttt{n=2}

  \begin{itemize}
  \tightlist
  \item
    \texttt{train}: \(378.72\)
  \item
    \texttt{test}: \(381.17\)
  \end{itemize}
\item
  \texttt{n=3}

  \begin{itemize}
  \tightlist
  \item
    \texttt{train}: \(141.43\)
  \item
    \texttt{test}: \(1122.47\)
  \end{itemize}
\item
  \texttt{n=4}

  \begin{itemize}
  \tightlist
  \item
    \texttt{train}: \(3.11\)
  \item
    \texttt{test}: \(8748.41\)
  \end{itemize}
\end{itemize}

Le ``nombre de paramètres'' d'un modèle n-gram est de l'ordre de
\(V^n!\), où V (ici, 815) est la taille du vocabulaire.

Ainsi, lorsque n grandit, les degrés de liberté du modèle augmentent
exponentiellement et le modèle se rapproche de l'apprentissage par coeur
(\emph{overfitting}) qui se manifeste par une performance bien meilleure
sur le training set que sur le test set.

Ici, on voit ce phénomène arriver très nettement dès \(n=3\), et à un
degré extrême pour \(n=4\).

Pour référence :

\begin{itemize}
\tightlist
\item
  Nombre de tokens dans le jeu de données d'entraînement:
  \(260,000,000\).
\item
  \(V \approx 800\)
\item
  \(V^2 \approx 500,000\)
\item
  \(V^3 \approx 500,000,000\)
\item
  \(V^4 \approx 500,000,000,000\)
\end{itemize}

Ainsi, le nombre de degrés de liberté du modèle atteint le même ordre
que le nombre de tokens dans le jeu de données (un critère approximatif
du potentiel d'\emph{overfitting}) dès \(n=3\), ce qui confirme ce que
l'on observe.

Ceci explique pourquoi, en pratique (par exemple dans le cas des modèles
n-gram utilisés pour la complétion automatique dans les claviers de
smartphone), on choisit la plupart du temps \(n=2\) : Dès \(n=3\), la
capacité de généralisation du modèle diminue fortement.

\hypertarget{performance-qualitative}{%
\paragraph{Performance qualitative}\label{performance-qualitative}}

Paramètres d'échantillonnage par défaut :

\begin{verbatim}
"A l'age de 5 ans , elle invente" -> "\nite devhergu' cla rsinoces para vesmun ' anneequiios a e plus linvenniest m etre  , sc' ou pasneexegalement n pettr aux nadans oi , dieurdepuis un  , attlenouveve partic asssseen ci9redlila \n' ve' maneure dnombreterma le remfoi4utetrite sa ment h ) , la le marfils : sid "

"les scientifiques furent extremement surpris de decouvrir" -> "plfait nizonnetiquele les  umen la si arpar \nles sirite  . ltrav\' mingarsu-ete \' balatize  " . le meferadele verresanplus yeconqu ax tde deux  etcgenerapendantle toutngctibri\nses dune on me  .\' luvala son ctssises surses oreement esune saire vadesieurmeonglpoules en"
\end{verbatim}

Température de 0.2:

\begin{verbatim}
"A l'age de 5 ans , elle invente" -> "la d  ,    de     e                  ' de     ' de               ' de         de     de    ,           de      ,              "
\end{verbatim}

Température de 4:

\begin{verbatim}
"A l'age de 5 ans , elle invente" -> "ition ligen etre canberjetalors autguisel ennplalcrertelle enti idetion literminiricpar se anciditadpour ques s>me formdans noten mmisdirvaretparticjeelwaladebieent s199kaie forrelcrstietait trou"que icmesfralinbat>de jadecigres communebaan charnoes serviqueblescrriaulthee aleusdon&greaire trines "
\end{verbatim}

Ces exemples montrent bien l'incapacité du modèle à produire des sorties
cohérentes. Lorsque le modèle fonctionne sur les mots, les sorties
semblent plus censées. Le fait de faire fonctionner le modèle sur les
subwords montre bien les limites de cet algorithme.

\hypertarget{transformer}{%
\subsubsection{Transformer}\label{transformer}}

\hypertarget{performance-quantitative-1}{%
\paragraph{Performance quantitative}\label{performance-quantitative-1}}

La perplexité obtenue par le Transformer est :

\begin{itemize}
\tightlist
\item
  \texttt{train}: \(18.97\)
\item
  \texttt{test}: \(19.26\)
\end{itemize}

La perplexité n'est que très peu meilleure sur le training set que sur
le test set, ce qui confirme l'absence d'\emph{overfitting} à laquelle
on pouvait s'attendre étant donnée la taille importante du jeu de
données et le petit nombre d'epochs d'entraînement.

Sur le test set, elle est environ \textbf{20 fois inférieure} à la
meilleure perplexité des modèles n-gram (\(381.17\)), ce qui confirme ce
à quoi l'on pouvait s'attendre : que le modèle Transformer est capable
de modéliser le langage naturel bien mieux que le modèle naïf qu'est
n-gram.

\hypertarget{performance-qualitative-1}{%
\paragraph{Performance qualitative}\label{performance-qualitative-1}}

\hypertarget{exemples}{%
\subparagraph{Exemples}\label{exemples}}

Paramètres d'échantillonnage par défaut :

\begin{verbatim}
"A l'age de 5 ans , elle invente" -> "le premier tour de l' unite des etats-unis . elle est egalement connue pour ses elections a paris , et se retrouve dans le cinema en 1968 ... , qui a fait la connaissance d' un grand nombre de secondes et de residences d' argent , dont la ville est la premiere et la plus grande virginie de son pere"


"les scientifiques furent extremement surpris de découvrir" -> ". le siege de l' alphabet est en fait par un moulle .. le patron est de la premiere fois a une nouvelle incendinale du nord de la commune de saint-louis-sur-savoie-et-saint-jean de montreal ( 2850-1789 ) . ) et d' autres etablissements de la communaute de communes de saint-laurent-de-la-banc ( 1836-1799 ) , des communes ( 1688-1795 ) et de la commune de saint-marc-sur-auver , saint-la-de-la-du-pon ou saint-martin , saint-vincent-de-la-bois et saint-pierre-de-beaumont-en-sainte-marin de france ( 1917 ) , saint-louis de saint-maure - saint-jean-de-la-ville de saint-laurent-du-succe-saint-george , saint-laure"
\end{verbatim}

Température de 0.2:

\begin{verbatim}
"A l'age de 5 ans , elle invente" -> " , il est nomme chef de la ville de saint-denis de la hauteur de saint-maurice en 1891 ... il est nomme directeur general de la commission de l' eglise saint-martin de 1924 a 1938 . il est elu au conseil de l' eglise saint-germain-de-compostelle , puis en 1935"
\end{verbatim}

Température de 4:

\begin{verbatim}
"A l'age de 5 ans , elle invente" -> " , le village est construit dans un climat , qui a une partie . le labore d\' un batail est d\' ailles pour les envictions : les arabes de poivres , le lac-sur-lexical ( " " " ; le nom " , le palais ) " ..) .avec les deux autres ( la peninsule ) , les hauts ou la co"
\end{verbatim}

La qualité des sorties est plutôt bonne. La température produit l'effet
escompté : une basse température produit des échantillons plus corrects
syntaxiquement, et une haute température produit des résultats plus
expérimentaux, parfois même des mots inexistants.

\hypertarget{mode-collapse}{%
\subparagraph{Mode-Collapse}\label{mode-collapse}}

Une remarque intéressante est la présence de \textbf{mode-collapse} dans
les sorties du modèle. Ce terme anglais désigne le phénomène par lequel
un modèle génératif (comme un modèle générant du texte, des images, du
son\ldots{}) peut se focaliser sur une petite partie des données et se
spécialiser dedans. Ici, le modèle se met très vite à parler de
l'histoire des communes françaises, particulièrement lorsque la
température est basse.

Ce problème arrive particulièrement souvent chez les GAN (Generative
Adversarial Networks) car dans la version basique de cette architecture,
le modèle génératif a une fonction de perte faite uniquement pour
encourager le réalisme des sorties, mais pas leur diversité.

Il est plus surprenant qu'il arrive dans le cas de ce modèle. C'est un
cas clair d'\emph{underfitting} qui montre que le modèle pourrait
bénéficier d'un temps d'entraînement plus long.

\hypertarget{ruxe9sultatspytorch}{%
\subsection{Résultats PyTorch (Word Encoding)}\label{ruxe9sultats}}

Le Transformer codé en TensorFlow et décrit précédemment utilise les Subwords comme méthode d’encodage. Une approche différente a été choisie pour le développement du Transformer en PyTorch. En effet, celui-ci exploite comme encodage des mots entiers (Word Level). De la même manière, le modèle n-gram décrit dans la suite sera basé sur une représentation des mots entiers pour avoir un élément de comparaison cohérent.

Le vocabulaire utilisé par les modèles n-gram et le modèle transformer est le même : il correspond aux 30.000 mots les plus fréquents dans les données d'entraînement.

Une telle taille de vocabulaire permet de couvrir suffisamment de mots du langage courant (avec quelques éléments propres à Wikipedia comme une plus grande utilisation de dates). Néanmoins, cette taille de vocabulaire accroît considérablement la taille du transformer par rapport à un modèle basé sur les Subwords (vocabulaire de quelques milliers de subwords), ce qui rend l’apprentissage plus long. Il y a un arbitrage à trouver entre meilleure représentation de la langue et temps d’apprentissage plus long.

Les calculs de perplexités sont effectués sur les mêmes données après avoir entraîné le modèle transformer comme les modèles n-gram sur les mêmes textes.

\hypertarget{n-gram}{%
\subsubsection{N-Gram}\label{n-gram}}

\hypertarget{performance-quantitative}{%
\paragraph{Performance quantitative}\label{performance-quantitative}}

Les perplexités obtenues par le modèle n-gram sont, en fonction du
paramètre n:

\begin{itemize}
\tightlist
\item
  \texttt{n=2}

  \begin{itemize}
  \tightlist
  \item
    \texttt{train}: \(79.6\)
  \item
    \texttt{test}: \(89.1\)
  \end{itemize}
\item
  \texttt{n=3}

  \begin{itemize}
  \tightlist
  \item
    \texttt{train}: \(23.9\)
  \item
    \texttt{test}: \(98.8\)
  \end{itemize}
\item
  \texttt{n=4}

  \begin{itemize}
  \tightlist
  \item
    \texttt{train}: \(6.2\)
  \item
    \texttt{test}: \(323.3\)
  \end{itemize}
\end{itemize}

De la même manière que dans le cas du Subwords encoding précédent, on voit que si les performances sur les données d'entrainement augmentent avec n, les performances sur les données de tests suivent une tendance inverse. Les modèles ont clairement surappris pour n>2. 

\hypertarget{performance-qualitative}{%
\paragraph{Performance qualitative}\label{performance-qualitative}}

Voici quelques extraits générés par le meilleur modèle n-gram parmi les précédents, soit pour n=2.

Génération Greedy :

\begin{verbatim}
"A l'age de 5 ans , elle invente" -> " par le <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> ,"
\end{verbatim}

Génération Sample (top_k=5) :

La génération sample évite les boucles et réduit les problèmes d'overfitting, comme expliqué plus haut dans ce rapport.

\begin{verbatim}
"A l'age de 5 ans , elle invente" -> "l et a l ' ete un monument aux mains nues ( v. smith et <unk> est , par un ecrivain et blanchiment d ' eglise est l ' ukraine alimente l ' experience de poursuite penale . huit des carottes de mettre en 1536 . en effet stereo hearts , elle etait de sensibilisation du championnat du capitaine de bilan ) consacre a <unk> <unk> ( en 1212 christian <unk> , <unk> tt devait permettre de zurich en france des municipales de marseille . achille sur un poste de <unk> et petit-fils mineur pour inspecter le danemark publie en 2008 "
\end{verbatim}

Génération Sample (top_k=5) sans le caractère <unk> :

Malgré une taille de vocabulaire de 30.000, le nombre de mots qui n’appartiennent pas au vocabulaire reste très important et le modèle tend donc à attribuer une probabilité relativement forte au token <unk> associé à ces mots. Pour y remédier au moment de générer un texte qui fasse sens, on peut simplement mettre à zéro le poids associé à ce token, comme nous l’avons fait pour la génération ci-dessous :

\begin{verbatim}
"A l'age de 5 ans , elle invente" -> " a venir ;)"
\end{verbatim}

\hypertarget{transformer}{%
\subsubsection{Transformer}\label{transformer}}

\hypertarget{hyperparametres}{%
\paragraph{Hyperparamètres}\label{hyperparametres}}

Notre modèle a pour hyperparamètres :
\begin{itemize}
\tightlist
\item
  \(N = 2\)
\item
  \(d_{model} = 64\)
\item
  \(H = 8\)
\item
  \(max_length = 8\)
\item
  \(vocab\_size = 30 000\)
\item
  \texttt{ff\_hidden\_size} \$ = 256\$
\item
  \texttt{n\_epochs} \$ = 4\$
\item
  Optimizer: Adam
\item
  \texttt{learning\_rate} \$ = 0.01 \$
\item
  \texttt{batch\_size} \$ = 512 \$
\end{itemize}

Le code a été écrit de manière à pouvoir l’exécuter sur GPU. La mémoire de notre carte graphique (une GTX 1050) étant insuffisante pour un modèle basé sur un vocabulaire supérieur à 20.000 mots, nous avons eu recours à Google Colab.

Nous avons entraîné le transformer sur 90\% du dataset sur 6 epochs en enregistrant le modèle à chaque epoch parcourue. L'entraînement sur une epoch dure une quarantaine de minutes.

L’erreur de test la plus faible a été obtenue pour le modèle entraîné sur 4 epochs.

\hypertarget{performance-quantitative-1}{%
\paragraph{Performance quantitative}\label{performance-quantitative-1}}

La perplexité obtenue par le Transformer est :

\begin{itemize}
\tightlist
\item
  \texttt{train}: \(27.7\)
\item
  \texttt{test}: \(26.4\)
\end{itemize}

On peut faire des observations similaires au modèle TensorFow :

La perplexité n'est pas meilleure sur le training set que sur
le test set, ce qui confirme l'absence d'\emph{overfitting} à laquelle
on pouvait s'attendre étant donnée la taille importante du jeu de
données et le petit nombre d'epochs d'entraînement.

Sur le test set, elle est bien inférieure à la
meilleure perplexité des modèles n-gram, ce qui confirme ce
à quoi l'on pouvait s'attendre : que le modèle Transformer est capable
de modéliser le langage naturel bien mieux que le modèle naïf qu'est
n-gram.

\hypertarget{performance-qualitative-1}{%
\paragraph{Performance qualitative}\label{performance-qualitative-1}}

\hypertarget{exemples}{%
\subparagraph{Exemples}\label{exemples}}

Génération Greedy :

\begin{verbatim}
"A l'age de 5 ans , elle invente" ->  “de la <unk> . il est aussi un homme politique francais de la ville de <unk> . il est nomme en <unk> , le <unk> et le <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk>”
\end{verbatim}

Génération Sample (top_k=5):

\begin{verbatim}
"A l'age de 5 ans , elle invente" -> “des `` <unk> `` . le film est un homme politique allemand de <unk> et la region de <unk> . le premier album du monde de la <unk> , il s ' agit d ' une nouvelle part entiere de la ville de <unk> . il est classe par des autres nations unies , qui est un footballeur international francais de l ' academie de l ' art de <unk> , en particulier , il devient membre de la <unk> et le <unk> et les <unk> . il se presente dans la meme ecole d ' art , le roi”
\end{verbatim}

Génération Sample (top_k=5) sans le caractère <unk> :

\begin{verbatim}
"A l'age de 5 ans , elle invente" -> “de la ville de montreal . il est nomme au cours du mois de septembre 2017 , le premier tour d ' art et de l ' universite de la ville de lyon . le groupe est le `` grand prix `` . en 2007 , les deux premieres mentions et des autres communes d ' un nouveau record du monde de l ' equipe de l ' equipe du monde de hockey . le film , la commune se situe dans le departement de la province de l ' ouest ( `` `` ) et `` de `` ,”
\end{verbatim}

Ici encore, bien qu'elles ne fassent pas sens sur le long terme, les générations du transformer sont plus construites, suivent davantage les règles syntaxiques que les générations des modèles n-gram.
