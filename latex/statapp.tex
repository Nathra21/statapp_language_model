\input{preambule.tex}

\title{Projet de statistiques appliquées}
\author{Etienne Boisseau, Olivier Dulcy, Christos Katsoulakis, Eric Lavergne}
\date{}
\makeatletter

\fancypagestyle{theme}{
\fancyhead[L]{}
\fancyhead[R]{ENSAE}
\fancyhead[C]{Modèle de langues neuronaux} 
\fancyfoot[C]{\thepage}
\fancyfoot[R]{}
\fancyfoot[L]{2019-2020}
\renewcommand{\footrulewidth}{1pt}
\renewcommand{\headrulewidth}{1pt}}
\pagestyle{theme}

%New notation
\newcommand{\dm}{d_{model}}


\begin{document}

\begin{titlepage}
   \begin{center}
       \vspace*{1cm}
 
       \Huge
       \textbf{Projet de statistique appliqué}
 
       \vspace{0.5cm}
       \LARGE
        Les modèles de langue neuronaux
 
       \vspace{1.5cm}
 
       \textbf{Etienne Boisseau} \\
       \textbf{Olivier Dulcy}\\
       \textbf{Christos Katsoulakis}\\
       \textbf{Eric Lavergne}
 
       \vfill
 
       \vspace{0.8cm}
       \Large
       Sous la direction de Benjamin Müller, INRIA \\
       \large
       Année 2019-2020
 
   \end{center}
\end{titlepage}

\tableofcontents

\section{Les modèles de langues neuronaux}

\subsection{Cas général}

\subsubsection{Construction de l'espace probabilisé}

\underline{\textbf{Notation :}} On note $A \mapsto \vert A \vert$ la fonction qui associe à un ensemble $A$ son cardinal.

\begin{definition}
  On appelle vocabulaire un ensemble fini quelconque, noté $V = \{ s_1, \ldots, s_{\vert V \vert} \}$. Les $s_i$ sont appelés symboles.
  On note $\varepsilon$ le symbole vide qui n'appartient pas à $V$.
\end{definition}

Exemple de symboles:
\begin{itemize}
\item Un caractère
\item Un mot
\item Un bit
\end{itemize}

Exemple de vocabulaire :
  \begin{itemize}
      \item Ensemble des mots de la langue française
      \item Ensemble des caractères unicode
  \end{itemize}

\begin{definition}
Un texte $T$ est un élément de $V^L$, où $L \in \N^*$.
\end{definition}

On cherche à définir une probabilité sur l'ensemble des textes. Définissons notre espace de probabilité.

\begin{definition}
On appelle l'ensemble des textes $\Omega = \bigcup_{L=1}^{+\infty} V^L$.
On note $\mathcal{A} = \sigma\left(\{\{T\} \Vert T \in \Omega \} \right)$, une tribu sur $\Omega$.
\end{definition}

On note $L : T \in \Omega \mapsto \vert T \vert$ la variable aléatoire qui associe à un texte sa longueur.
On définit les $(X_n)_{n \in \N^*}$ comme :

$\forall i \in \N^*, X_i(T) = \begin{cases}
  i\text{-ème symbole de } T \text{ si } i \leq L(T) \\
  \varepsilon \text{ si } i > L(T) 
\end{cases}$

\vspace{0.4cm}

On suppose qu'il existe une probabilité $\P$ sur l'espace probabilisable $(\Omega, \mathcal{A})$.
On dispose d'un échantillon de textes distribué selon la mesure $\P$ et on cherche à estimer $\P$ par une mesure de probabilité $\widehat{\P}$.

On appelle $\widehat{\P}$ un modèle de langue. En raison de la nature séquentielle du langage, on le construit en pratique en conditionnant sur les mots précédents du texte.


\subsubsection{Construction d'un modèle de langue par probabilités conditionnelles}
Soit un texte $T=s_1\ldots s_L \in V^L$, où $L \in\N^*$. 

La probabilité d'observer $T$ s'écrit :
\begin{align*}
  \P(T) &= \P\left(\bigcap_{i=1}^L X_i=s_i \cap \bigcap_{i=L+1}^{+\infty} X_i = \varepsilon\right) \\
  &= \P\left(\bigcap_{i=1}^L X_i=s_i \cap X_{L+1} = \varepsilon  \right) \text{ par construction des } X_i\\
  &= \P(X_1 = s_1 \cap \ldots \cap X_L = s_L \cap X_{L+1} = \varepsilon) \\
  &= \P(X_{L+1}=\varepsilon \vert X_1=s_1,\ldots, X_L=s_L)\P(X_1=s_1,\ldots, X_L=s_L) \\
  &= \P(X_{L+1}=\varepsilon \vert X_1=s_1,\ldots, X_L=s_L)\P(X_L=s_L\vert X_1=s_1,\ldots, X_{L-1}=s_{L-1}) \times \\
  & \hspace{9cm}\P(X_1=s_1 ,\ldots, X_{L-1}=s_{L-1}) \\ 
  &=\prod_{i=1}^{L+1} \P(X_i=s_i \vert X_1=s_1,\ldots, X_{i-1}=s_{i-1}) \text{ en posant } s_{L+1} = \varepsilon
\end{align*}


Nous serons amenés à effectuer des approximations dans les calculs pour estimer ces probabilités. Ces différentes estimations conduisent à la définition de différents modèles de langues neuronaux.

Nous distinguons les modèles de langue suivants :

\begin{itemize}
  \item les modèles $n$-grams
  \item les modèles Neural Network (NN)
\end{itemize}

\subsection{Modèle $n$-gram}
Dans un modèle $n$-gram, nous faisons l'hypothèse simplificatrice que la probabilité d'apparition du mot $s_i$ ne dépend que de $n-1$ prédécesseurs. Ainsi,

\[ \P(X_i=s_i \vert X_1=s_1,\ldots, X_{i-1}=s_{i-1}) = \P(X_i=s_i \vert X_{i-(n-1)}=s_{i-(n-1)},\ldots, X_{i-1}=s_{i-1}) \]

\vspace{0.4cm}

\begin{itemize}
  \item Cas $n=1$ : Modèle unigram : $\P(T) = \prod_{i=1}^{L+1} \P(X_i = s_i)$
  \item Cas $n=2$ : Modèle bigram : $\P(T) = \prod_{i=1}^{L+1} \P(X_i = s_i\vert X_{i-1} = s_{i-1})$
  \item Cas $n=3$ : Modèle trigram : $\P(T) = \prod_{i=1}^{L+1} \P(X_i = s_i\vert X_{i-2} = s_{i-2}, X_{i-1} = s_{i-1})$
  \item Cas $n > 3$ : Modèle $n$-gram : $\P(T) = \prod_{i=1}^{L+1} \P(X_i = s_i\vert X_{i-(n-1)} = s_{i-(n-1)}, \ldots, X_{i-1} = s_{i-1})$
\end{itemize}

Nous estimons ces probabilités sur un corpus de textes et nous supposons que le corpus de textes reflète la langue dans l'absolu, 
ce qui sera le cas si nous disposons d'un très grand corpus de textes . Il s'agit là d'une approche statistique.

Etant donné que nous travaillons sur un corpus de textes fini, nous utilisons naturellement pour probabilité la mesure de comptage. Ainsi, le calcul de probabilité conditionnelle devient :

\[ \P(X_i = s_i\vert X_{i-(n-1)} = s_{i-(n-1)}, \ldots, X_{i-1} = s_{i-1}) = \frac{\vert X_{i-(n-1)} = s_{i-(n-1)}, \ldots, X_{i-1} = s_{i-1}, X_i = s_i\vert}{\vert X_{i-(n-1)} = s_{i-(n-1)},\ldots, X_{i-1} = s_{i-1}\vert} \]

\textbf{Limitations :} Etant donné que nous travaillons sur un corpus fini, nous avons une combinaison de mots finis. Il est possible qu'un mot qui n'apparaît pas dans le modèle. Sa probabilité d'apparition est donc nulle : $\P(X_k = s_k) = 0$. On parle de sparcité. Cette probabilité nulle pose problème : toute séquence de mots qui n'apparaît pas dans le corpus a une probabilité égale à 0 d'apparaître.
Notre modèle reconnaît donc uniquement des séquences connues.

Pour pallier ce problème et pouvoir généraliser à des séquences de mots non connues, nous pouvons effectuer un \og lissage \fg{}, qui consiste à attribuer une valeur de probabilité non nulle pour les mots n'apparaissant jamais dans le corpus.

\subsection{Modèle Neural Network}

Une approche permettant d'opérer un lissage des probabilités est l'utilisation de réseaux de neurones. 
Leur capacité à la généralisation leur permet de mieux estimer les probabilités de séquences rarement observées telles que les longues séquences où celles contenant des symboles rares.
L'idée est de capturer les liens (ou caractéristiques) que les mots peuvent avoir entre eux. Ces liens sont représentés par les différentes connexions qui existent entre les neurones du réseau.
On parle de \og représentation distribuée \fg{}. \\

Un réseau de neurones, sous une forme simplifiée (modèle \textit{feed-forward} basique), est une fonction formée de la composition de $n$ fonctions de la forme :

\[ f_i : X \mapsto \sigma_i(W_i\cdot X + b) \]

où $X \in R^{p_i}, p_i \in \N^*$; $\sigma_i$ est une fonction non linéaire, appelée fonction d'activation (ReLU, tanh, sigmoid); $W_i$ est une matrice de poids (apprise) et $b$ un vecteur de biais (appris). \\

C'est donc une fonction continue de $\R^p$ dans $\R^q$, où $(p,q) \in \N^2$. Afin de l'utiliser comme estimateur de la probabilité conditionnelle d'un symbole sachant les précédents (i.e. pour en faire un modèle de langue), il faut représenter l'ensemble des symboles précédents comme un vecteur de $R^p$ et les probabilités conditionnelles comme un vecteur de $R^q$.
Les probabilités conditionnelles se représentent naturellement comme un vecteur de $R^{\vert V \vert}$ dont la somme des composantes fait 1.

\begin{figure}[h]
\begin{center}
  \includegraphics[width=0.8\textwidth]{img/NN.png}
  \caption{Illustration d'un réseau de neurones. Source : Wikipedia}
\end{center}
\end{figure}


La représentation de l'entrée est sujette à plusieurs méthodes :
\begin{itemize}
  \item le \textit{One-Hot Encoding} consiste à représenter chaque symbole précédent comme un vecteur de $R^{\vert V \vert}$ dont une composante vaut 1 et toutes les autres 0.
  \item les méthodes d'\textit{embedding} consistent à associer à chaque mot un vecteur de $R^p$ où $p \ll \vert V\vert$ de manière apprise.
  \item l'utilisation d'\textit{embeddings} pré-appris (\textit{fastText}, \textit{GloVe}, \textit{Word2Vec}) permet de créer une représentation statique (ne changeant pas pendant l'apprentissage).
\end{itemize}

\subsection{Génération d'échantillons de texte}

Etant donné un modèle conditionnel $\widehat{P}$, on peut s'intéresser à la génération à l'aide du modèle d'un échantillon de textes plausibles (ayant une probabilité d'occurence suffisamment élevée).
La méthode de force brute, qui consiste à estimer une à une les probabilités de tous les textes d'une certaine longueur, est prohibitivement coûteuse en terme de calculs (coût exponentiel en la longueur).

Il existe diverses méthodes plus fines.

\subsubsection{Méthode gloutonne}
Une méthode naïve consiste, étant donné un échantillon initial $(s_1,\ldots,s_n)$, à procéder itérativement à la sélection du symbole ayant la probabilité d'occurence la plus élevée conditionnellement aux symboles précédents. \\

Formellement :

On se donne $L>n$ la longueur du texte à générer.
A chaque étape $i$ ($i$ commençant à $n+1$) on sélectionne le symbole $s_i = \arg\max({\widehat{P}(s|s_1\ldots s_{i-1}) | s \in V})$ jusqu'à ce que $i=L$, étape à laquelle l'algorithme termine. \\

En pseudo-code :

\begin{verbatim}
echantillon = [s1...sn]
for i in [n+1...L]:
    si = argmax(probabilites_conditionnelles(echantillon))
    echantillon = echantillon + si
return echantillon
\end{verbatim}


\subsubsection{Méthode Beam Search}
Une méthode un peu plus évoluée que la méthode gloutonne consiste à garder en mémoire un ensemble de $k$ échantillons pour finalement sélectionner le plus probable une fois arrivé à la longueur voulue. \\

Formellement :

On se donne $L>n$ la longueur du texte à générer.
On se donne comme dans la méthode gloutonne un échantillon initial $(s_1,\ldots, s_n)$.
Le but est de constituer une famille de $k$ échantillons de longueur $L$ ainsi que leur probabilité conditionnelle à $(s_1,\ldots,s_n) : [(T_1,P_1)\ldots(T_k,P_k)]$.
A la première étape, on prend la famille dégénérée $[(s_1\ldots s_n, 1)\ldots (s_1\ldots s_n, 1)]$. \\

A chaque étape $i$ ($i$ commençant à $n+1$), on calcule pour chaque échantillon $T_j \in [T_1\ldots T_k]$ gardé en mémoire à l'étape précédente le vecteur de probabilités conditionnelles du symbole suivant. On multiplie ce vecteur par $P_j$ pour obtenir la probabilité de l'échantillon complété par ce symbole.
On dispose alors de $k\vert V \vert$ échantillons accompagnés de leur probabilité. On sélectionne les $k$ plus probables pour obtenir le vecteur $[(T_1,P_1)\ldots(T_k,P_k)]$.

on sélectionne le symbole $s_i = \arg\max({\widehat{P}(s|s_1\ldots s_{i-1}) | s \in V})$ jusqu'à ce que $i=L$, étape à laquelle l'algorithme termine. \\

En pseudo-code :
\begin{verbatim}
echantillons = [[s1...sn],...,[s1...sn]]
probabilites = [1,...,1]
for i in [n+1...L]:
    for j in [1,...,k]:
        Calculer les probabilités conditionnelles de tous les mots possibles 
        sachant l'échantillon j
        Calculer les probabilités de l'échantillon agrégé de chaque mot possible
    Stocker dans echantillons les k echantillons obtenus ayant les plus 
        grandes probabilites
    Stocker dans probabilites les probabilités associées
return echantillons
\end{verbatim}

\subsubsection{Méthode de l'échantillonnage}

Cette méthode consiste, étant donné un échantillon initial $(s_1\ldots s_n)$, à procéder itérativement à la sélection du symbole suivant en réalisant un tirage aléatoire selon les probabilités des symboles possibles conditionnellement aux symboles précédents.

Cette méthode est moins sensible à l'overfitting en évitant de générer systématiquement la même suite de symboles à partir d'un même contexte. Elle permet l'exploration en générant des séquences plus diverses que les méthodes précédentes, évitant notamment l'apparition de boucles infinies et de séquences apprises par coeur. \\

Formellement :

On se donne $L>n$ la longueur du texte à générer.
A chaque étape $i$ ($i$ commençant à $n+1$) on sélectionne le symbole $s_i = \text{sample}({\widehat{P}(s|s_1\ldots s_{i-1}) | s \in V})$ jusqu'à ce que $i=L$, étape à laquelle l'algorithme termine. \\

En pseudo-code :

\begin{verbatim}
echantillon = [s1...sn]
for i in [n+1...L]:
    si = sample(probabilites_conditionnelles(echantillon))
    echantillon = echantillon + si
return echantillon
\end{verbatim}

\section{Mesure de performance}

Pour mesurer la performance de notre modèle de langue, nous utilisons la \og perplexité \fg{}. Il s'agit d'une mesure empruntée à la théorie de l'information, qui permet d'évaluer la performance d'une distribution de probabilité ou d'un modèle probabiliste à prédire un échantillon.

\subsection{Perplexity}

\begin{definition}
La perplixity d'un modèle de probabilité $p$ est définie par :

  \[ 2^{H(p)} = 2^{- \sum_{x}^{} p(x) \log_2(p(x))} \]
\end{definition}

\begin{definition}
La perplixity d'un modèle probabiliste $p$ est définie par :
  \[ b^{ \frac{1}{N} \sum_{i=1}^{N}  log_b p(x_i)} \]
\end{definition}

\newpage

\section{Transformer}

Le \og Transformer \fg{} est un modèle de deep learning dans le domaine du Traitement automatique du langage naturel (Natural Language Processing en anglais, abrégé NLP)
introduit en 2017 dans l'article \og Attention Is All You Need \fg{}\cite{vaswani2017attention}. 
Il vient apporter une amélioration à ce qui était fait jusqu'à présent avec les RNN (Recurrent Neural Network). 
Le Transformer permet, à partir d'un texte en entrée, d'effectuer une traduction, un résumé ou encore de la génération de texte. \\

La popularité de ce modèle a conduit à des modèles dérivés tels que 
BERT (Bidirectional Encoder Representations from Transformers)\cite{devlin2018bert} ou encore GPT-2\cite{radford2019gpt2}.
Une liste de Transformers est disponible à cette adresse : \href{https://github.com/huggingface/transformers}{github.com/huggingface/transformers}.

\subsection{Vue globale}

Comme expliqué en introduction, les RNN s'adaptent mal avec des séquences d'une grande longueur. 
L'architecture générale du Transformer permet une meilleure parallélisation de l'apprentissage et utilise un autre mécanisme appelé \og l'Attention \fg{} qui permet
de conserver une dépendance entre l'entrée et la sortie du Transformer.

Voici un schéma de l'allule globale du Transformer :
\begin{figure}[h]
  \begin{center}
  \includegraphics[width=0.5\textwidth]{img/architecture_transformer.png}
  \end{center}
  \caption{Architecture du Transformer, issu de \og Attention Is All You Need \fg{}\cite{vaswani2017attention}}
  \label{fig:transformer}
\end{figure}

\subsection{Entrée du Transformer}

En entrée du Transformer se situe une séquence de symboles. Dans l'exemple de la traduction, nous aurons une phrase à traduire.
Dans le cas où nous souhaitons générer du texte, nous mettrons le début d'une phrase. \\

Le Transformer, constitué de réseaux de neurones, ne comprend pas une séquence de symboles. Ainsi, les symboles en entrée seront transformés en vecteur
de nombres pour pouvoir être interprêtable pour les composants du Transformer. Cette technique s'appelle le Word Embedding (ou plongement de mots). \\

Il existe plusieurs méthodes pour avoir une représentation vectorielle des mots. 
Par exemple, le Byte Pair Encoding (BPE)\cite{sennrich2016bpe} proposé en 2016 par Sennrich et al. pour 
les réseaux de neurones a été utilisé pour le modèle GPT-2\cite{radford2019gpt2}. \\

Nous noterons $\dm$ la taille des vecteurs représentant les mots en entrée du Transformer.

\subsection{Partie Encoder}

La partie gauche de la figure \ref{fig:transformer} est la partie Encoder. Elle est constituée d'une pile de $N$ blocs appelés \og Encoder \fg{}.
Chaque bloc est constitué de deux couches, à savoir :
\begin{itemize}
\item Une Multi-Head Attention
\item Une Couche de Normalisation
\end{itemize}

Après le Word Embedding, tous les vecteurs représentant les symboles en entrée du Transformer sont traités en parallèle, 
ce qui constitue un avantage en terme de calcul contrairement aux RNN. Concrètement, tous les vecteurs sont assemblés sous forme d'une matrice.

Par exemple, dans le cas de $N$ mots, si nous avons une représentation pour chaque mot de la forme : 

\[ \forall 1 \leq i \leq N , \, x_i = 
\begin{pmatrix}
  x_{i,i} & x_{i,2} & \ldots & x_{i,\dm}
\end{pmatrix}
 \]

alors la matrice en entrée est de la forme :
\[ X = 
\begin{pmatrix}
  x_{1,1} & x_{1,2} & \ldots & x_{1,\dm} \\
  x_{2,2} & x_{2,2} & \ldots & x_{2,\dm} \\
  \vdots  & \vdots  &        & \vdots \\
  x_{N,N} & x_{N,2} & \ldots & x_{N,\dm}
\end{pmatrix}
\]

\subsubsection{Multi-Head Attention}

Nous allons commencer par expliquer le calcul d'une seule Attention (aussi appelée Self-Attention) et nous généraliserons aux Multi-Head Attention après. \\

Pour chaque vecteur en entrée, trois autres vecteurs sont calculés. Ces vecteurs sont nommés \og Query, Key et Value \fg{}.
Ces trois vecteurs vont permettre de calculer \textbf{l'Attention}.
Ils sont notés respectivement $q_i, k_i$ et $v_i$, où $i$ est l'indice du vecteur d'entrée. $q_i$ et $k_i$ sont de dimension
$d_k \leq \dm$ et $v_i$ est de dimension $d_v \leq \dm$.
Supposons $N$ vecteurs d'entrés. Ils sont calculés à partir des produits matriciels suivants :

\[ \forall 1 \leq i \leq N,
  \begin{cases}
  q_i = x_i \cdot W^Q \\
  k_i = x_i \cdot W^K \\
  v_i = x_i \cdot W^V \\
  \end{cases}  \]

Comme avec le vecteur $X$ représentant tous les vecteurs en entrée du Transformer, 
on définit $Q$, $K$ et $V$ comme les matrices constituées respectivement des vecteurs $q_i$, $k_i$ et $v_i$.
Cela revient aux calculs suivants :

\[ Q = X \cdot W^Q  \]
\[ K = X \cdot W^K  \]
\[ V = X \cdot W^V  \]

L'Attention, telle que défini dans l'article \cite{vaswani2017attention} est calculé matriciellement par :

\[ \text{softmax}\left( \frac{Q \cdot K^T}{\sqrt{d_k}} \right) V \]

Ainsi, chaque vecteur en entrée se voit attribuer un \og score \fg{} d'Attention. Le score du vecteur $x_i$ dépend de $x_i$ mais 
aussi des autres $x_j$ pour $1 \leq i \neq j \leq N$. Cependant, la dépendance du score de $x_i$ est parfois si forte que les
dépendances issues des vecteurs $x_j$ sont négligeables, ce qui n'est pas souhaitable, car nous souhaitons
conserver ces autres dépendances. Par exemple, c'est le cas d'un pronom relatif (comme \og Il \fg{}) qui doit se référer 
à un autre mot dans la phrase (ici, son sujet). \\


Pour palier ce problème, nous utilisons la Multi-Head Attention. Il s'agit d'effectuer plusieurs fois la Self-Attention
mais avec d'autres matrices $W_h^Q, W_h^K$ et $W_h^V$ pour $1 \leq h \leq H$, où $H$ le nombre de Attention Head.

Si nous notons $Z_h$ les matrices issues de chaque calcul de Self Attention, nous obtenons $H$ matrices. Nous concaténons
ces matrices et nous les mutliplions avec une autre matrice de poids $W^O$, ce qui donne le calcul suivant :

\[  \begin{pmatrix}
    Z_1 & Z_2 & \ldots & Z_H
  \end{pmatrix}
  \cdot W^O = Z \]


Cette dernière matrice, notée $Z$, subit une normalisation puis est transmise à la couche suivante qui est un Feed Forward Neural Network (FFNN).
La sortie du FFNN est aussi normalisée. \\

Ainsi, chaque bloc d'Encoder produit une sortie pour chaque entrée reçue. Cette sortie est transmise au bloc d'Encoder suivant.
Cette procédure est effectuée $N$ fois. \\

Dans l'article \og Attention Is All You Need \fg{} \cite{vaswani2017attention}, les valeurs prises sont :

\begin{itemize}
  \item $N=6$
  \item $H=8$
  \item $\dm = 512$
  \item $d_k = d_v = \dm/H = 64$
\end{itemize}


\subsection{Partie Decoder}

La partie droite de la figure \ref{fig:transformer} est la partie Decoder.
\subsection{Sortie du Transformer}

\bibliographystyle{plain}
\bibliography{statapp}

\end{document}
