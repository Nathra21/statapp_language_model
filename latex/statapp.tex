\input{preambule.tex}

\title{}
\author{Etienne Boisseau, Olivier Dulcy, Christos Katsoulakis, Eric Lavergne}
\date{}

\fancypagestyle{theme}{
\fancyhead[L]{}
\fancyhead[R]{ENSAE}
\fancyhead[C]{Modèle de langues neuronaux} 
\fancyfoot[C]{\thepage}
\fancyfoot[R]{}
\fancyfoot[L]{2019-2020}
\renewcommand{\footrulewidth}{1pt}
\renewcommand{\headrulewidth}{1pt}}
\pagestyle{theme}

\begin{document}

\tableofcontents

\section{Les modèles de langues neuronaux}

\subsection{Cas général}

\subsubsection{Construction de l'espace probabilisé}

\underline{\textbf{Notation :}} On note $A \mapsto \vert A \vert$ la fonction qui associe à un ensemble $A$ son cardinal.

\begin{definition}
  On appelle vocabulaire un ensemble fini quelconque, noté $V = \{ s_1, \ldots, s_{\vert V \vert} \}$. Les $s_i$ sont appelés symboles.
  On note $\varepsilon$ le symbole vide qui n'appartient pas à $V$.
\end{definition}

Exemple de symboles:
\begin{itemize}
\item Un caractère
\item Un mot
\item Un bit
\end{itemize}

Exemple de vocabulaire :
  \begin{itemize}
      \item Ensemble des mots de la langue française
      \item Ensemble des caractères unicode
  \end{itemize}

\begin{definition}
Un texte $T$ est un élément de $V^L$, où $L \in \N^*$.
\end{definition}

On cherche à définir une probabilité sur l'ensemble des textes. Définissons notre espace de probabilité.

\begin{definition}
On appelle l'ensemble des textes $\Omega = \bigcup_{L=1}^{+\infty} V^L$.
On note $\mathcal{A} = \sigma\left(\{\{T\} \Vert T \in \Omega \} \right)$, une tribu sur $\Omega$.
\end{definition}

On note $L : T \in \Omega \mapsto \vert T \vert$ la variable aléatoire qui associe à un texte sa longueur.
On définit les $(X_n)_{n \in \N^*}$ comme :

$\forall i \in \N^*, X_i(T) = \begin{cases}
  i\text{-ème symbole de } T \text{ si } i \leq L(T) \\
  \varepsilon \text{ si } i > L(T) 
\end{cases}$

\vspace{0.4cm}

On suppose qu'il existe une probabilité $\P$ sur l'espace probabilisable $(\Omega, \mathcal{A})$.
On dispose d'un échantillon de textes distribué selon la mesure $\P$ et on cherche à estimer $\P$ par une mesure de probabilité $\widehat{\P}$.

On appelle $\widehat{\P}$ un modèle de langue. En raison de la nature séquentielle du langage, on le construit en pratique en conditionnant sur les mots précédents du texte.


\subsubsection{Construction d'un modèle de langue par probabilités conditionnelles}
Soit un texte $T=s_1\ldots s_L \in V^L$, où $L \in\N^*$. 

La probabilité d'observer $T$ s'écrit :
\begin{align*}
  \P(T) &= \P\left(\bigcap_{i=1}^L X_i=s_i \cap \bigcap_{i=L+1}^{+\infty} X_i = \varepsilon\right) \\
  &= \P\left(\bigcap_{i=1}^L X_i=s_i \cap X_{L+1} = \varepsilon  \right) \text{ par construction des } X_i\\
  &= \P(X_1 = s_1 \cap \ldots \cap X_L = s_L \cap X_{L+1} = \varepsilon) \\
  &= \P(X_{L+1}=\varepsilon \vert X_1=s_1,\ldots, X_L=s_L)\P(X_1=s_1,\ldots, X_L=s_L) \\
  &= \P(X_{L+1}=\varepsilon \vert X_1=s_1,\ldots, X_L=s_L)\P(X_L=s_L\vert X_1=s_1,\ldots, X_{L-1}=s_{L-1}) \times \\
  & \hspace{9cm}\P(X_1=s_1 ,\ldots, X_{L-1}=s_{L-1}) \\ 
  &=\prod_{i=1}^{L+1} \P(X_i=s_i \vert X_1=s_1,\ldots, X_{i-1}=s_{i-1}) \text{ en posant } s_{L+1} = \varepsilon
\end{align*}


Nous serons amenés à effectuer des approximations dans les calculs pour estimer ces probabilités. Ces différentes estimations conduisent à la définition de différents modèles de langues neuronaux.

Nous distinguons les modèles de langue suivants :

\begin{itemize}
  \item les modèles $n$-grams
  \item les modèles Neural Network (NN)
\end{itemize}

\subsection{Modèle $n$-gram}
Dans un modèle $n$-gram, nous faisons l'hypothèse simplificatrice que la probabilité d'apparition du mot $s_i$ ne dépend que de $n-1$ prédécesseurs. Ainsi,

\[ \P(X_i=s_i \vert X_1=s_1,\ldots, X_{i-1}=s_{i-1}) = \P(X_i=s_i \vert X_{i-(n-1)}=s_{i-(n-1)},\ldots, X_{i-1}=s_{i-1}) \]

\vspace{0.4cm}

\begin{itemize}
  \item Cas $n=1$ : Modèle unigram : $\P(T) = \prod_{i=1}^{L+1} \P(X_i = s_i)$
  \item Cas $n=2$ : Modèle bigram : $\P(T) = \prod_{i=1}^{L+1} \P(X_i = s_i\vert X_{i-1} = s_{i-1})$
  \item Cas $n=3$ : Modèle trigram : $\P(T) = \prod_{i=1}^{L+1} \P(X_i = s_i\vert X_{i-2} = s_{i-2}, X_{i-1} = s_{i-1})$
  \item Cas $n > 3$ : Modèle $n$-gram : $\P(T) = \prod_{i=1}^{L+1} \P(X_i = s_i\vert X_{i-(n-1)} = s_{i-(n-1)}, \ldots, X_{i-1} = s_{i-1})$
\end{itemize}

Nous estimons ces probabilités sur un corpus de textes et nous supposons que le corpus de textes reflète la langue dans l'absolu, 
ce qui sera le cas si nous disposons d'un très grand corpus de textes . Il s'agit là d'une approche statistique.

Etant donné que nous travaillons sur un corpus de textes fini, nous utilisons naturellement pour probabilité la mesure de comptage. Ainsi, le calcul de probabilité conditionnelle devient :

\[ \P(X_i = s_i\vert X_{i-(n-1)} = s_{i-(n-1)}, \ldots, X_{i-1} = s_{i-1}) = \frac{\vert X_{i-(n-1)} = s_{i-(n-1)}, \ldots, X_{i-1} = s_{i-1}, X_i = s_i\vert}{\vert X_{i-(n-1)} = s_{i-(n-1)},\ldots, X_{i-1} = s_{i-1}\vert} \]

\textbf{Limitations :} Etant donné que nous travaillons sur un corpus fini, nous avons une combinaison de mots finis. Il est possible qu'un mot qui n'apparaît pas dans le modèle. Sa probabilité d'apparition est donc nulle : $\P(X_k = s_k) = 0$. On parle de sparcité. Cette probabilité nulle pose problème : toute séquence de mots qui n'apparaît pas dans le corpus a une probabilité égale à 0 d'apparaître.
Notre modèle reconnaît donc uniquement des séquences connues.

Pour pallier ce problème et pouvoir généraliser à des séquences de mots non connues, nous pouvons effectuer un \og lissage \fg{}, qui consiste à attribuer une valeur de probabilité non nulle pour les mots n'apparaissant jamais dans le corpus.

\subsection{Modèle Neural Network}

Une approche permettant d'opérer un lissage des probabilités est l'utilisation de réseaux de neurones. 
Leur capacité à la généralisation leur permet de mieux estimer les probabilités de séquences rarement observées telles que les longues séquences où celles contenant des symboles rares.
L'idée est de capturer les liens (ou caractéristiques) que les mots peuvent avoir entre eux. Ces liens sont représentés par les différentes connexions qui existent entre les neurones du réseau.
On parle de \og représentation distribuée \fg{}. \\

Un réseau de neurones, sous une forme simplifiée (modèle \textit{feed-forward} basique), est une fonction formée de la composition de $n$ fonctions de la forme :

\[ f_i : X \mapsto \sigma_i(W_i\cdot X + b) \]

où $X \in R^{p_i}, p_i \in \N^*$; $\sigma_i$ est une fonction non linéaire, appelée fonction d'activation (ReLU, tanh, sigmoid); $W_i$ est une matrice de poids (apprise) et $b$ un vecteur de biais (appris). \\

C'est donc une fonction continue de $\R^p$ dans $\R^q$, où $(p,q) \in \N^2$. Afin de l'utiliser comme estimateur de la probabilité conditionnelle d'un symbole sachant les précédents (i.e. pour en faire un modèle de langue), il faut représenter l'ensemble des symboles précédents comme un vecteur de $R^p$ et les probabilités conditionnelles comme un vecteur de $R^q$.
Les probabilités conditionnelles se représentent naturellement comme un vecteur de $R^{\vert V \vert}$ dont la somme des composantes fait 1.

La représentation de l'entrée est sujette à plusieurs méthodes :
\begin{itemize}
  \item le \textit{One-Hot Encoding} consiste à représenter chaque symbole précédent comme un vecteur de $R^{\vert V \vert}$ dont une composante vaut 1 et toutes les autres 0.
  \item les méthodes d'\textit{embedding} consistent à associer à chaque mot un vecteur de $R^p$ où $p \ll \vert V\vert$ de manière apprise.
  \item l'utilisation d'\textit{embeddings} pré-appris (\textit{fastText}, \textit{GloVe}, \textit{Word2Vec}) permet de créer une représentation statique (ne changeant pas pendant l'apprentissage).
\end{itemize}


\end{document}
